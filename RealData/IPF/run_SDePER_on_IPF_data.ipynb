{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02f03238",
   "metadata": {},
   "source": [
    "# Run *SDePER* on Real data: IPF\n",
    "\n",
    "In this Notebook we run SDePER on one Real Dataset -- human **Idiopathic pulmonary fibrosis (IPF)** dataset. \n",
    "\n",
    "For downloading and preprocessing original spatial and reference scRNA-seq data for cell type deconvolution please refer [IPF_preprocess.nb.html](https://rawcdn.githack.com/az7jh2/SDePER_Analysis/c9b4698ecd9d0b1b0d2794df963127efe01ec231/RealData/IPF/IPF_preprocess.nb.html).\n",
    "\n",
    "==================================================================================================================\n",
    "\n",
    "So here we use **5 input files** as shown below:\n",
    "\n",
    "1. raw nUMI counts of spatial transcriptomic data (spots × genes): `IPF_spatial_spot_nUMI.csv`, just decompress the gzipped file [IPF_spatial_spot_nUMI.csv.gz](https://github.com/az7jh2/SDePER_Analysis/blob/main/RealData/IPF/IPF_spatial_spot_nUMI.csv.gz)\n",
    "2. raw nUMI counts of reference scRNA-seq data (cells × genes): `IPF_ref_scRNA_cell_nUMI.csv`, just decompress the gzipped file [IPF_ref_scRNA_cell_nUMI.csv.gz](https://github.com/az7jh2/SDePER_Analysis/blob/main/RealData/IPF/IPF_ref_scRNA_cell_nUMI.csv.gz)\n",
    "3. cell type annotations for cells of selected 26 cell types in reference scRNA-seq data (cells × 1): [IPF_ref_scRNA_cell_celltype.csv](https://github.com/az7jh2/SDePER_Analysis/blob/main/RealData/IPF/IPF_ref_scRNA_cell_celltype.csv)\n",
    "4. adjacency matrix of spots in spatial transcriptomic data (spots × spots): [IPF_spatial_spot_adjacency_matrix.csv](https://github.com/az7jh2/SDePER_Analysis/blob/main/RealData/IPF/IPF_spatial_spot_adjacency_matrix.csv)\n",
    "5. manually selected 2,534 cell type specific marker genes from scRNA-seq data (26 cell types × 2,534 genes): [IPF_selected_2534_celltype_markers.csv](https://github.com/az7jh2/SDePER_Analysis/blob/main/RealData/IPF/IPF_selected_2534_celltype_markers.csv)\n",
    "\n",
    "==================================================================================================================\n",
    "\n",
    "SDePER settings are:\n",
    "\n",
    "* number of included highly variable genes `n_hv_gene`: 2000\n",
    "* number of selected TOP marker genes for each comparison in Differential `n_marker_per_cmp`: 20\n",
    "* maximum value of cells in one pseudo-spot for building CVAE `pseudo_spot_max_cell`: 10\n",
    "* hyper-parameter for Adaptive Lasso `lambda_r`: 0.72\n",
    "* hyper-parameter for Graph Laplacian Constrain `lambda_g`: 13.895\n",
    "* seed for random values `seed`: 1\n",
    "* number of used CPU cores `n_core`: 64\n",
    "\n",
    "ALL other options are left as default.\n",
    "\n",
    "==================================================================================================================\n",
    "\n",
    "the `bash` command to start cell type deconvolution is\n",
    "\n",
    "`runDeconvolution -q IPF_spatial_spot_nUMI.csv -r IPF_ref_scRNA_cell_nUMI.csv -c IPF_ref_scRNA_cell_celltype.csv -a IPF_spatial_spot_adjacency_matrix.csv -m IPF_selected_2534_celltype_markers.csv --n_hv_gene 2000 --n_marker_per_cmp 20 --pseudo_spot_max_cell 10 --lambda_r 0.72 --lambda_g 13.895 --seed 1 -n 64`\n",
    "\n",
    "Note this Notebook uses **SDePER v1.0.1**. Cell type deconvolution result is renamed as [IPF_SDePER_celltype_proportions.csv](https://github.com/az7jh2/SDePER_Analysis/tree/main/RealData/IPF/IPF_SDePER_celltype_proportions.csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f43766d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T02:54:41.700164Z",
     "iopub.status.busy": "2023-04-26T02:54:41.699973Z",
     "iopub.status.idle": "2023-04-30T16:11:05.399108Z",
     "shell.execute_reply": "2023-04-30T16:11:05.398017Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SDePER (Spatial Deconvolution method with Platform Effect Removal) v1.0.1\n",
      "\n",
      "\n",
      "running options:\n",
      "spatial_file: /home/exouser/Spatial/IPF_spatial_spot_nUMI.csv\n",
      "ref_file: /home/exouser/Spatial/IPF_ref_scRNA_cell_nUMI.csv\n",
      "ref_celltype_file: /home/exouser/Spatial/IPF_ref_scRNA_cell_celltype.csv\n",
      "marker_file: /home/exouser/Spatial/IPF_selected_2534_celltype_markers.csv\n",
      "loc_file: None\n",
      "A_file: /home/exouser/Spatial/IPF_spatial_spot_adjacency_matrix.csv\n",
      "n_cores: 64\n",
      "lambda_r: 0.72\n",
      "lambda_g: 13.895\n",
      "use_cvae: True\n",
      "threshold: 0\n",
      "n_hv_gene: 2000\n",
      "n_marker_per_cmp: 20\n",
      "pseudo_spot_min_cell: 2\n",
      "pseudo_spot_max_cell: 10\n",
      "seq_depth_scaler: 10000\n",
      "cvae_input_scaler: 10\n",
      "cvae_init_lr: 0.003\n",
      "redo_de: True\n",
      "seed: 1\n",
      "diagnosis: False\n",
      "verbose: True\n",
      "use_imputation: False\n",
      "diameter: 200\n",
      "impute_diameter: [160, 114, 80]\n",
      "\n",
      "\n",
      "######### Preprocessing... #########\n",
      "\n",
      "######### First build CVAE... #########\n",
      "\n",
      "read spatial data from file /home/exouser/Spatial/IPF_spatial_spot_nUMI.csv\n",
      "total 3532 spots; 32078 genes\n",
      "\n",
      "read scRNA-seq data from file /home/exouser/Spatial/IPF_ref_scRNA_cell_nUMI.csv\n",
      "total 11227 cells; 35483 genes\n",
      "read scRNA-seq cell-type annotation from file /home/exouser/Spatial/IPF_ref_scRNA_cell_celltype.csv\n",
      "total 26 cell-types\n",
      "subset cells with cell-type annotation, finally keep 11227 cells; 35483 genes\n",
      "\n",
      "total 30222 overlapped genes\n",
      "\n",
      "identify 2000 highly variable genes from scRNA-seq data...\n",
      "\n",
      "identify cell-type marker genes...\n",
      "user provided marker gene profile, DE will be skipped...\n",
      "\n",
      "read 2534 marker genes from user specified marker gene file\n",
      "from user specified marker gene expression use 2534 marker genes overlapped with spatial + scRNA-seq data\n",
      "\n",
      "use union of highly variable gene list and cell-type marker gene list derived from scRNA-seq data, finally get 3101 genes for downstream analysis\n",
      "\n",
      "start CVAE building...\n",
      "\n",
      "generate pseudo-spots containing 2 to 10 cells from scRNA-seq cells...\n",
      "generate 400000 pseudo-spots for training and 100000 pseudo-spots for validation\n",
      "scaling inputs to range 0 to 10\n",
      "in training -- spatial spots : pseudo-spots = 3532 : 411227\n",
      "\n",
      "Start training...\n",
      "\n",
      "Train on 414759 samples, validate on 100000 samples\n",
      "Epoch 1/1000\n",
      "414759/414759 [==============================] - 16s 38us/sample - loss: 648.5680 - val_loss: 1293.9209 - lr: 0.0030\n",
      "Epoch 2/1000\n",
      "414759/414759 [==============================] - 19s 45us/sample - loss: 1457.5292 - val_loss: 597.7031 - lr: 0.0030\n",
      "Epoch 3/1000\n",
      "414759/414759 [==============================] - 17s 42us/sample - loss: 734.6433 - val_loss: 454.0854 - lr: 0.0030\n",
      "Epoch 4/1000\n",
      "414759/414759 [==============================] - 18s 43us/sample - loss: 583.3225 - val_loss: 452.9033 - lr: 0.0030\n",
      "Epoch 5/1000\n",
      "414759/414759 [==============================] - 20s 47us/sample - loss: 578.4910 - val_loss: 525.8589 - lr: 0.0030\n",
      "Epoch 6/1000\n",
      "414759/414759 [==============================] - 23s 56us/sample - loss: 656.6307 - val_loss: 424.2505 - lr: 0.0030\n",
      "Epoch 7/1000\n",
      "414759/414759 [==============================] - 22s 53us/sample - loss: 549.5275 - val_loss: 384.0400 - lr: 0.0030\n",
      "Epoch 8/1000\n",
      "414759/414759 [==============================] - 22s 52us/sample - loss: 507.2419 - val_loss: 360.4219 - lr: 0.0030\n",
      "Epoch 9/1000\n",
      "414759/414759 [==============================] - 22s 52us/sample - loss: 481.1720 - val_loss: 366.1832 - lr: 0.0030\n",
      "Epoch 10/1000\n",
      "414759/414759 [==============================] - 22s 52us/sample - loss: 486.1134 - val_loss: 342.6895 - lr: 0.0030\n",
      "Epoch 11/1000\n",
      "414759/414759 [==============================] - 21s 51us/sample - loss: 461.4505 - val_loss: 330.4469 - lr: 0.0030\n",
      "Epoch 12/1000\n",
      "414759/414759 [==============================] - 21s 51us/sample - loss: 447.9386 - val_loss: 330.0387 - lr: 0.0030\n",
      "Epoch 13/1000\n",
      "414759/414759 [==============================] - 22s 52us/sample - loss: 447.0276 - val_loss: 310.7932 - lr: 0.0030\n",
      "Epoch 14/1000\n",
      "414759/414759 [==============================] - 16s 39us/sample - loss: 426.7744 - val_loss: 323.7586 - lr: 0.0030\n",
      "Epoch 15/1000\n",
      "414759/414759 [==============================] - 22s 54us/sample - loss: 439.6057 - val_loss: 299.2744 - lr: 0.0030\n",
      "Epoch 16/1000\n",
      "414759/414759 [==============================] - 16s 39us/sample - loss: 414.0753 - val_loss: 339.6587 - lr: 0.0030\n",
      "Epoch 17/1000\n",
      "414759/414759 [==============================] - 24s 58us/sample - loss: 456.7122 - val_loss: 320.1053 - lr: 0.0030\n",
      "Epoch 18/1000\n",
      "414759/414759 [==============================] - 21s 50us/sample - loss: 435.9984 - val_loss: 324.7607 - lr: 0.0030\n",
      "Epoch 19/1000\n",
      "414759/414759 [==============================] - 16s 39us/sample - loss: 439.9869 - val_loss: 306.4714 - lr: 0.0030\n",
      "Epoch 20/1000\n",
      "414759/414759 [==============================] - 16s 40us/sample - loss: 421.5034 - val_loss: 309.3784 - lr: 0.0030\n",
      "Epoch 21/1000\n",
      "414759/414759 [==============================] - 23s 54us/sample - loss: 424.2193 - val_loss: 301.4593 - lr: 0.0030\n",
      "Epoch 22/1000\n",
      "414759/414759 [==============================] - 27s 66us/sample - loss: 415.7262 - val_loss: 293.1478 - lr: 0.0030\n",
      "Epoch 23/1000\n",
      "414759/414759 [==============================] - 16s 39us/sample - loss: 407.0669 - val_loss: 287.0498 - lr: 0.0030\n",
      "Epoch 24/1000\n",
      "414759/414759 [==============================] - 15s 37us/sample - loss: 400.4649 - val_loss: 283.5584 - lr: 0.0030\n",
      "Epoch 25/1000\n",
      "414759/414759 [==============================] - 22s 52us/sample - loss: 396.6936 - val_loss: 283.7668 - lr: 0.0030\n",
      "Epoch 26/1000\n",
      "414759/414759 [==============================] - 17s 40us/sample - loss: 396.7234 - val_loss: 272.6840 - lr: 0.0030\n",
      "Epoch 27/1000\n",
      "414759/414759 [==============================] - 21s 51us/sample - loss: 384.8537 - val_loss: 277.9244 - lr: 0.0030\n",
      "Epoch 28/1000\n",
      "414759/414759 [==============================] - 16s 38us/sample - loss: 390.3474 - val_loss: 274.3375 - lr: 0.0030\n",
      "Epoch 29/1000\n",
      "414759/414759 [==============================] - 22s 52us/sample - loss: 386.3604 - val_loss: 267.5186 - lr: 0.0030\n",
      "Epoch 30/1000\n",
      "414759/414759 [==============================] - 16s 38us/sample - loss: 379.2374 - val_loss: 269.4325 - lr: 0.0030\n",
      "Epoch 31/1000\n",
      "414759/414759 [==============================] - 16s 38us/sample - loss: 381.0916 - val_loss: 262.9824 - lr: 0.0030\n",
      "Epoch 32/1000\n",
      "414759/414759 [==============================] - 17s 41us/sample - loss: 374.3363 - val_loss: 262.0836 - lr: 0.0030\n",
      "Epoch 33/1000\n",
      "414759/414759 [==============================] - 17s 41us/sample - loss: 373.2256 - val_loss: 259.3769 - lr: 0.0030\n",
      "Epoch 34/1000\n",
      "414759/414759 [==============================] - 17s 40us/sample - loss: 370.4036 - val_loss: 257.2745 - lr: 0.0030\n",
      "Epoch 35/1000\n",
      "414759/414759 [==============================] - 19s 45us/sample - loss: 368.1839 - val_loss: 254.5431 - lr: 0.0030\n",
      "Epoch 36/1000\n",
      "414759/414759 [==============================] - 23s 56us/sample - loss: 365.3756 - val_loss: 251.3896 - lr: 0.0030\n",
      "Epoch 37/1000\n",
      "414759/414759 [==============================] - 17s 41us/sample - loss: 362.0453 - val_loss: 250.8127 - lr: 0.0030\n",
      "Epoch 38/1000\n",
      "414759/414759 [==============================] - 16s 38us/sample - loss: 361.3837 - val_loss: 248.4783 - lr: 0.0030\n",
      "Epoch 39/1000\n",
      "414759/414759 [==============================] - 16s 39us/sample - loss: 359.0107 - val_loss: 245.6262 - lr: 0.0030\n",
      "Epoch 40/1000\n",
      "414759/414759 [==============================] - 16s 39us/sample - loss: 355.9567 - val_loss: 245.8757 - lr: 0.0030\n",
      "Epoch 41/1000\n",
      "414759/414759 [==============================] - 16s 39us/sample - loss: 356.0713 - val_loss: 243.2602 - lr: 0.0030\n",
      "Epoch 42/1000\n",
      "414759/414759 [==============================] - 17s 40us/sample - loss: 353.4207 - val_loss: 241.1079 - lr: 0.0030\n",
      "Epoch 43/1000\n",
      "414759/414759 [==============================] - 16s 40us/sample - loss: 351.1310 - val_loss: 242.4467 - lr: 0.0030\n",
      "Epoch 44/1000\n",
      "414759/414759 [==============================] - 21s 50us/sample - loss: 352.4249 - val_loss: 238.7335 - lr: 0.0030\n",
      "Epoch 45/1000\n",
      "414759/414759 [==============================] - 17s 41us/sample - loss: 348.4770 - val_loss: 240.6717 - lr: 0.0030\n",
      "Epoch 46/1000\n",
      "414759/414759 [==============================] - 16s 39us/sample - loss: 350.3277 - val_loss: 238.1238 - lr: 0.0030\n",
      "Epoch 47/1000\n",
      "414759/414759 [==============================] - 16s 37us/sample - loss: 347.7517 - val_loss: 237.8909 - lr: 0.0030\n",
      "Epoch 48/1000\n",
      "414759/414759 [==============================] - 15s 37us/sample - loss: 347.5306 - val_loss: 236.1668 - lr: 0.0030\n",
      "Epoch 49/1000\n",
      "414759/414759 [==============================] - 16s 38us/sample - loss: 345.6038 - val_loss: 235.0673 - lr: 0.0030\n",
      "Epoch 50/1000\n",
      "414759/414759 [==============================] - 16s 38us/sample - loss: 344.4259 - val_loss: 234.8892 - lr: 0.0030\n",
      "Epoch 51/1000\n",
      "414759/414759 [==============================] - 25s 61us/sample - loss: 344.2619 - val_loss: 233.9225 - lr: 0.0030\n",
      "Epoch 52/1000\n",
      "414759/414759 [==============================] - 19s 46us/sample - loss: 343.0659 - val_loss: 233.2072 - lr: 0.0030\n",
      "Epoch 53/1000\n",
      "414759/414759 [==============================] - 17s 41us/sample - loss: 342.4565 - val_loss: 232.0637 - lr: 0.0030\n",
      "Epoch 54/1000\n",
      "414759/414759 [==============================] - 16s 38us/sample - loss: 341.2328 - val_loss: 231.7242 - lr: 0.0030\n",
      "Epoch 55/1000\n",
      "414759/414759 [==============================] - 22s 53us/sample - loss: 340.8555 - val_loss: 230.5707 - lr: 0.0030\n",
      "Epoch 56/1000\n",
      "414759/414759 [==============================] - 16s 40us/sample - loss: 339.6389 - val_loss: 230.6150 - lr: 0.0030\n",
      "Epoch 57/1000\n",
      "414759/414759 [==============================] - 22s 53us/sample - loss: 339.6648 - val_loss: 228.5686 - lr: 0.0030\n",
      "Epoch 58/1000\n",
      "414759/414759 [==============================] - 16s 39us/sample - loss: 337.4529 - val_loss: 229.5694 - lr: 0.0030\n",
      "Epoch 59/1000\n",
      "414759/414759 [==============================] - 16s 39us/sample - loss: 338.6064 - val_loss: 227.4262 - lr: 0.0030\n",
      "Epoch 60/1000\n",
      "414759/414759 [==============================] - 20s 49us/sample - loss: 336.2287 - val_loss: 230.6090 - lr: 0.0030\n",
      "Epoch 61/1000\n",
      "414759/414759 [==============================] - 23s 55us/sample - loss: 339.5154 - val_loss: 227.8680 - lr: 0.0030\n",
      "Epoch 62/1000\n",
      "414759/414759 [==============================] - 16s 39us/sample - loss: 336.7230 - val_loss: 228.2659 - lr: 0.0030\n",
      "Epoch 63/1000\n",
      "414759/414759 [==============================] - 21s 51us/sample - loss: 337.1382 - val_loss: 228.5122 - lr: 0.0030\n",
      "Epoch 64/1000\n",
      "414759/414759 [==============================] - 16s 40us/sample - loss: 337.4538 - val_loss: 226.6844 - lr: 0.0030\n",
      "Epoch 65/1000\n",
      "414759/414759 [==============================] - 17s 41us/sample - loss: 335.3749 - val_loss: 225.6415 - lr: 0.0030\n",
      "Epoch 66/1000\n",
      "414759/414759 [==============================] - 16s 38us/sample - loss: 334.3190 - val_loss: 226.7076 - lr: 0.0030\n",
      "Epoch 67/1000\n",
      "414759/414759 [==============================] - 24s 58us/sample - loss: 335.4520 - val_loss: 225.4579 - lr: 0.0030\n",
      "Epoch 68/1000\n",
      "414759/414759 [==============================] - 16s 39us/sample - loss: 334.0158 - val_loss: 224.9940 - lr: 0.0030\n",
      "Epoch 69/1000\n",
      "414759/414759 [==============================] - 19s 46us/sample - loss: 333.5565 - val_loss: 223.5209 - lr: 0.0030\n",
      "Epoch 70/1000\n",
      "414759/414759 [==============================] - 17s 42us/sample - loss: 332.0089 - val_loss: 223.8051 - lr: 0.0030\n",
      "Epoch 71/1000\n",
      "414759/414759 [==============================] - 15s 37us/sample - loss: 332.3039 - val_loss: 222.8316 - lr: 0.0030\n",
      "Epoch 72/1000\n",
      "414759/414759 [==============================] - 22s 52us/sample - loss: 331.2968 - val_loss: 222.7692 - lr: 0.0030\n",
      "Epoch 73/1000\n",
      "414759/414759 [==============================] - 18s 44us/sample - loss: 331.0799 - val_loss: 220.7873 - lr: 0.0030\n",
      "Epoch 74/1000\n",
      "414759/414759 [==============================] - 18s 44us/sample - loss: 329.0883 - val_loss: 222.4627 - lr: 0.0030\n",
      "Epoch 75/1000\n",
      "414759/414759 [==============================] - 17s 40us/sample - loss: 330.8449 - val_loss: 220.8216 - lr: 0.0030\n",
      "Epoch 76/1000\n",
      "414759/414759 [==============================] - 22s 52us/sample - loss: 329.1396 - val_loss: 222.1315 - lr: 0.0030\n",
      "Epoch 77/1000\n",
      "414759/414759 [==============================] - 21s 50us/sample - loss: 330.5030 - val_loss: 220.4043 - lr: 0.0030\n",
      "Epoch 78/1000\n",
      "414759/414759 [==============================] - 15s 37us/sample - loss: 328.6520 - val_loss: 221.4580 - lr: 0.0030\n",
      "Epoch 79/1000\n",
      "414759/414759 [==============================] - 25s 61us/sample - loss: 329.8214 - val_loss: 219.1070 - lr: 0.0030\n",
      "Epoch 80/1000\n",
      "414759/414759 [==============================] - 18s 44us/sample - loss: 327.2764 - val_loss: 219.9153 - lr: 0.0030\n",
      "Epoch 81/1000\n",
      "414759/414759 [==============================] - 22s 52us/sample - loss: 328.1933 - val_loss: 218.5355 - lr: 0.0030\n",
      "Epoch 82/1000\n",
      "414759/414759 [==============================] - 23s 56us/sample - loss: 326.7455 - val_loss: 220.6229 - lr: 0.0030\n",
      "Epoch 83/1000\n",
      "414759/414759 [==============================] - 17s 41us/sample - loss: 328.8216 - val_loss: 219.5131 - lr: 0.0030\n",
      "Epoch 84/1000\n",
      "414759/414759 [==============================] - 22s 54us/sample - loss: 327.6686 - val_loss: 219.9193 - lr: 0.0030\n",
      "Epoch 85/1000\n",
      "414759/414759 [==============================] - 17s 41us/sample - loss: 328.1243 - val_loss: 219.5540 - lr: 0.0030\n",
      "Epoch 86/1000\n",
      "414759/414759 [==============================] - 22s 54us/sample - loss: 327.6057 - val_loss: 217.5542 - lr: 0.0030\n",
      "Epoch 87/1000\n",
      "414759/414759 [==============================] - 24s 57us/sample - loss: 325.5376 - val_loss: 218.7131 - lr: 0.0030\n",
      "Epoch 88/1000\n",
      "414759/414759 [==============================] - 21s 51us/sample - loss: 326.7348 - val_loss: 218.6098 - lr: 0.0030\n",
      "Epoch 89/1000\n",
      "414759/414759 [==============================] - 22s 53us/sample - loss: 326.7243 - val_loss: 218.1593 - lr: 0.0030\n",
      "Epoch 90/1000\n",
      "414759/414759 [==============================] - 32s 76us/sample - loss: 326.1289 - val_loss: 216.4568 - lr: 0.0030\n",
      "Epoch 91/1000\n",
      "414759/414759 [==============================] - 28s 67us/sample - loss: 324.3759 - val_loss: 216.8772 - lr: 0.0030\n",
      "Epoch 92/1000\n",
      "414759/414759 [==============================] - 20s 49us/sample - loss: 324.7365 - val_loss: 216.3615 - lr: 0.0030\n",
      "Epoch 93/1000\n",
      "414759/414759 [==============================] - 17s 41us/sample - loss: 324.1965 - val_loss: 214.8169 - lr: 0.0030\n",
      "Epoch 94/1000\n",
      "414759/414759 [==============================] - 16s 39us/sample - loss: 322.6259 - val_loss: 216.3138 - lr: 0.0030\n",
      "Epoch 95/1000\n",
      "414759/414759 [==============================] - 16s 40us/sample - loss: 324.1687 - val_loss: 214.4962 - lr: 0.0030\n",
      "Epoch 96/1000\n",
      "414759/414759 [==============================] - 22s 53us/sample - loss: 322.2328 - val_loss: 216.4496 - lr: 0.0030\n",
      "Epoch 97/1000\n",
      "414759/414759 [==============================] - 16s 39us/sample - loss: 324.3125 - val_loss: 213.8311 - lr: 0.0030\n",
      "Epoch 98/1000\n",
      "414759/414759 [==============================] - 23s 55us/sample - loss: 321.4998 - val_loss: 219.3905 - lr: 0.0030\n",
      "Epoch 99/1000\n",
      "414759/414759 [==============================] - 15s 37us/sample - loss: 327.3218 - val_loss: 214.7741 - lr: 0.0030\n",
      "Epoch 100/1000\n",
      "414759/414759 [==============================] - 21s 52us/sample - loss: 322.5995 - val_loss: 216.9138 - lr: 0.0030\n",
      "Epoch 101/1000\n",
      "414759/414759 [==============================] - 22s 52us/sample - loss: 324.7352 - val_loss: 218.6582 - lr: 0.0030\n",
      "Epoch 102/1000\n",
      "414759/414759 [==============================] - 16s 39us/sample - loss: 326.6383 - val_loss: 217.3678 - lr: 0.0030\n",
      "Epoch 103/1000\n",
      "414759/414759 [==============================] - 24s 57us/sample - loss: 325.1809 - val_loss: 216.8213 - lr: 0.0030\n",
      "Epoch 104/1000\n",
      "414759/414759 [==============================] - 15s 37us/sample - loss: 324.5267 - val_loss: 215.5010 - lr: 0.0030\n",
      "Epoch 105/1000\n",
      "414759/414759 [==============================] - 21s 51us/sample - loss: 323.2235 - val_loss: 215.5089 - lr: 0.0030\n",
      "Epoch 106/1000\n",
      "414759/414759 [==============================] - 22s 52us/sample - loss: 323.2272 - val_loss: 216.7116 - lr: 0.0030\n",
      "Epoch 107/1000\n",
      "414759/414759 [==============================] - 19s 45us/sample - loss: 324.4341 - val_loss: 213.4260 - lr: 0.0030\n",
      "Epoch 108/1000\n",
      "414759/414759 [==============================] - 16s 38us/sample - loss: 321.0227 - val_loss: 216.2950 - lr: 0.0030\n",
      "Epoch 109/1000\n",
      "414759/414759 [==============================] - 21s 51us/sample - loss: 324.0779 - val_loss: 215.2012 - lr: 0.0030\n",
      "Epoch 110/1000\n",
      "414759/414759 [==============================] - 16s 38us/sample - loss: 322.9791 - val_loss: 214.7274 - lr: 0.0030\n",
      "Epoch 111/1000\n",
      "414759/414759 [==============================] - 23s 56us/sample - loss: 322.2538 - val_loss: 214.7320 - lr: 0.0030\n",
      "Epoch 112/1000\n",
      "414759/414759 [==============================] - 24s 57us/sample - loss: 322.4207 - val_loss: 212.2380 - lr: 0.0030\n",
      "Epoch 113/1000\n",
      "414759/414759 [==============================] - 16s 38us/sample - loss: 319.7744 - val_loss: 214.3232 - lr: 0.0030\n",
      "Epoch 114/1000\n",
      "414759/414759 [==============================] - 16s 38us/sample - loss: 321.7850 - val_loss: 213.0237 - lr: 0.0030\n",
      "Epoch 115/1000\n",
      "414759/414759 [==============================] - 20s 48us/sample - loss: 320.5952 - val_loss: 213.0385 - lr: 0.0030\n",
      "Epoch 116/1000\n",
      "414759/414759 [==============================] - 20s 49us/sample - loss: 320.4777 - val_loss: 212.4142 - lr: 0.0030\n",
      "Epoch 117/1000\n",
      "414759/414759 [==============================] - 16s 38us/sample - loss: 319.8449 - val_loss: 212.8172 - lr: 0.0030\n",
      "Epoch 118/1000\n",
      "414759/414759 [==============================] - 21s 50us/sample - loss: 320.1738 - val_loss: 210.7660 - lr: 0.0030\n",
      "Epoch 119/1000\n",
      "414759/414759 [==============================] - 16s 37us/sample - loss: 318.0634 - val_loss: 213.0771 - lr: 0.0030\n",
      "Epoch 120/1000\n",
      "414759/414759 [==============================] - 16s 38us/sample - loss: 320.4081 - val_loss: 210.2889 - lr: 0.0030\n",
      "Epoch 121/1000\n",
      "414759/414759 [==============================] - 23s 56us/sample - loss: 317.5080 - val_loss: 214.7384 - lr: 0.0030\n",
      "Epoch 122/1000\n",
      "414759/414759 [==============================] - 23s 55us/sample - loss: 322.2085 - val_loss: 212.6232 - lr: 0.0030\n",
      "Epoch 123/1000\n",
      "414759/414759 [==============================] - 20s 47us/sample - loss: 319.9041 - val_loss: 212.8939 - lr: 0.0030\n",
      "Epoch 124/1000\n",
      "414759/414759 [==============================] - 20s 48us/sample - loss: 320.2425 - val_loss: 212.2366 - lr: 0.0030\n",
      "Epoch 125/1000\n",
      "414759/414759 [==============================] - 15s 37us/sample - loss: 319.5091 - val_loss: 211.9032 - lr: 0.0030\n",
      "Epoch 126/1000\n",
      "414759/414759 [==============================] - 21s 51us/sample - loss: 319.1307 - val_loss: 211.0021 - lr: 0.0030\n",
      "Epoch 127/1000\n",
      "414759/414759 [==============================] - 15s 37us/sample - loss: 318.2110 - val_loss: 209.8941 - lr: 0.0030\n",
      "Epoch 128/1000\n",
      "414759/414759 [==============================] - 16s 38us/sample - loss: 317.0206 - val_loss: 210.6795 - lr: 0.0030\n",
      "Epoch 129/1000\n",
      "414759/414759 [==============================] - 18s 43us/sample - loss: 317.8330 - val_loss: 209.0143 - lr: 0.0030\n",
      "Epoch 130/1000\n",
      "414759/414759 [==============================] - 16s 39us/sample - loss: 316.0431 - val_loss: 208.9185 - lr: 0.0030\n",
      "Epoch 131/1000\n",
      "414759/414759 [==============================] - 16s 39us/sample - loss: 315.9691 - val_loss: 208.8872 - lr: 0.0030\n",
      "Epoch 132/1000\n",
      "414759/414759 [==============================] - 29s 70us/sample - loss: 315.9917 - val_loss: 209.1970 - lr: 0.0030\n",
      "Epoch 133/1000\n",
      "414759/414759 [==============================] - 17s 41us/sample - loss: 316.2087 - val_loss: 207.9344 - lr: 0.0030\n",
      "Epoch 134/1000\n",
      "414759/414759 [==============================] - 22s 54us/sample - loss: 314.8958 - val_loss: 209.6058 - lr: 0.0030\n",
      "Epoch 135/1000\n",
      "414759/414759 [==============================] - 17s 40us/sample - loss: 316.6082 - val_loss: 207.3794 - lr: 0.0030\n",
      "Epoch 136/1000\n",
      "414759/414759 [==============================] - 20s 47us/sample - loss: 314.3927 - val_loss: 210.5890 - lr: 0.0030\n",
      "Epoch 137/1000\n",
      "414759/414759 [==============================] - 19s 47us/sample - loss: 317.7617 - val_loss: 208.1864 - lr: 0.0030\n",
      "Epoch 138/1000\n",
      "414759/414759 [==============================] - 16s 38us/sample - loss: 315.2315 - val_loss: 207.6382 - lr: 0.0030\n",
      "Epoch 139/1000\n",
      "414759/414759 [==============================] - 23s 54us/sample - loss: 314.5715 - val_loss: 208.8519 - lr: 0.0030\n",
      "Epoch 140/1000\n",
      "414759/414759 [==============================] - 21s 51us/sample - loss: 315.8601 - val_loss: 207.3798 - lr: 0.0030\n",
      "Epoch 141/1000\n",
      "414759/414759 [==============================] - 15s 37us/sample - loss: 314.2494 - val_loss: 207.4462 - lr: 0.0030\n",
      "Epoch 142/1000\n",
      "414759/414759 [==============================] - 20s 49us/sample - loss: 314.3139 - val_loss: 207.1834 - lr: 0.0030\n",
      "Epoch 143/1000\n",
      "414759/414759 [==============================] - 15s 37us/sample - loss: 314.0481 - val_loss: 207.4677 - lr: 0.0030\n",
      "Epoch 144/1000\n",
      "414759/414759 [==============================] - 15s 36us/sample - loss: 314.2674 - val_loss: 206.8164 - lr: 0.0030\n",
      "Epoch 145/1000\n",
      "414759/414759 [==============================] - 16s 38us/sample - loss: 313.5744 - val_loss: 207.2394 - lr: 0.0030\n",
      "Epoch 146/1000\n",
      "414759/414759 [==============================] - 15s 35us/sample - loss: 314.0294 - val_loss: 205.6437 - lr: 0.0030\n",
      "Epoch 147/1000\n",
      "414759/414759 [==============================] - 16s 37us/sample - loss: 312.3534 - val_loss: 206.3721 - lr: 0.0030\n",
      "Epoch 148/1000\n",
      "414759/414759 [==============================] - 15s 35us/sample - loss: 313.0363 - val_loss: 205.6100 - lr: 0.0030\n",
      "Epoch 149/1000\n",
      "414759/414759 [==============================] - 14s 35us/sample - loss: 312.3669 - val_loss: 205.7310 - lr: 0.0030\n",
      "Epoch 150/1000\n",
      "414759/414759 [==============================] - 15s 37us/sample - loss: 312.3871 - val_loss: 204.5559 - lr: 0.0030\n",
      "Epoch 151/1000\n",
      "414759/414759 [==============================] - 20s 49us/sample - loss: 311.0984 - val_loss: 206.1017 - lr: 0.0030\n",
      "Epoch 152/1000\n",
      "414759/414759 [==============================] - 20s 48us/sample - loss: 312.7458 - val_loss: 204.5823 - lr: 0.0030\n",
      "Epoch 153/1000\n",
      "414759/414759 [==============================] - 16s 38us/sample - loss: 311.1167 - val_loss: 205.2541 - lr: 0.0030\n",
      "Epoch 154/1000\n",
      "414759/414759 [==============================] - 25s 60us/sample - loss: 311.8017 - val_loss: 205.9122 - lr: 0.0030\n",
      "Epoch 155/1000\n",
      "414759/414759 [==============================] - 25s 59us/sample - loss: 312.5919 - val_loss: 204.0160 - lr: 0.0030\n",
      "Epoch 156/1000\n",
      "414759/414759 [==============================] - 20s 47us/sample - loss: 310.5679 - val_loss: 205.3586 - lr: 0.0030\n",
      "Epoch 157/1000\n",
      "414759/414759 [==============================] - 20s 48us/sample - loss: 311.8694 - val_loss: 204.4803 - lr: 0.0030\n",
      "Epoch 158/1000\n",
      "414759/414759 [==============================] - 20s 48us/sample - loss: 311.0010 - val_loss: 203.8216 - lr: 0.0030\n",
      "Epoch 159/1000\n",
      "414759/414759 [==============================] - 20s 48us/sample - loss: 310.3429 - val_loss: 203.4612 - lr: 0.0030\n",
      "Epoch 160/1000\n",
      "414759/414759 [==============================] - 19s 46us/sample - loss: 309.8881 - val_loss: 203.5547 - lr: 0.0030\n",
      "Epoch 161/1000\n",
      "414759/414759 [==============================] - 24s 59us/sample - loss: 310.1254 - val_loss: 203.1551 - lr: 0.0030\n",
      "Epoch 162/1000\n",
      "414759/414759 [==============================] - 16s 38us/sample - loss: 309.5937 - val_loss: 203.2995 - lr: 0.0030\n",
      "Epoch 163/1000\n",
      "414759/414759 [==============================] - 16s 40us/sample - loss: 309.7508 - val_loss: 202.2620 - lr: 0.0030\n",
      "Epoch 164/1000\n",
      "414759/414759 [==============================] - 22s 53us/sample - loss: 308.6335 - val_loss: 202.4870 - lr: 0.0030\n",
      "Epoch 165/1000\n",
      "414759/414759 [==============================] - 15s 37us/sample - loss: 308.8001 - val_loss: 201.5322 - lr: 0.0030\n",
      "Epoch 166/1000\n",
      "414759/414759 [==============================] - 16s 39us/sample - loss: 307.9264 - val_loss: 203.4926 - lr: 0.0030\n",
      "Epoch 167/1000\n",
      "414759/414759 [==============================] - 15s 37us/sample - loss: 309.9588 - val_loss: 202.0867 - lr: 0.0030\n",
      "Epoch 168/1000\n",
      "414759/414759 [==============================] - 25s 60us/sample - loss: 308.4214 - val_loss: 202.2969 - lr: 0.0030\n",
      "Epoch 169/1000\n",
      "414759/414759 [==============================] - 17s 41us/sample - loss: 308.5934 - val_loss: 201.3858 - lr: 0.0030\n",
      "Epoch 170/1000\n",
      "414759/414759 [==============================] - 22s 54us/sample - loss: 307.7753 - val_loss: 202.2645 - lr: 0.0030\n",
      "Epoch 171/1000\n",
      "414759/414759 [==============================] - 16s 39us/sample - loss: 308.6089 - val_loss: 200.6782 - lr: 0.0030\n",
      "Epoch 172/1000\n",
      "414759/414759 [==============================] - 17s 40us/sample - loss: 306.9406 - val_loss: 201.1310 - lr: 0.0030\n",
      "Epoch 173/1000\n",
      "414759/414759 [==============================] - 15s 37us/sample - loss: 307.4789 - val_loss: 201.3202 - lr: 0.0030\n",
      "Epoch 174/1000\n",
      "414759/414759 [==============================] - 22s 53us/sample - loss: 307.5284 - val_loss: 200.1400 - lr: 0.0030\n",
      "Epoch 175/1000\n",
      "414759/414759 [==============================] - 16s 40us/sample - loss: 306.2705 - val_loss: 201.7400 - lr: 0.0030\n",
      "Epoch 176/1000\n",
      "414759/414759 [==============================] - 16s 38us/sample - loss: 308.1859 - val_loss: 200.2019 - lr: 0.0030\n",
      "Epoch 177/1000\n",
      "414759/414759 [==============================] - 28s 67us/sample - loss: 306.4477 - val_loss: 201.3140 - lr: 0.0030\n",
      "Epoch 178/1000\n",
      "414759/414759 [==============================] - 15s 37us/sample - loss: 307.6123 - val_loss: 201.2020 - lr: 0.0030\n",
      "Epoch 179/1000\n",
      "414759/414759 [==============================] - 29s 69us/sample - loss: 307.5294 - val_loss: 199.8897 - lr: 0.0030\n",
      "Epoch 180/1000\n",
      "414759/414759 [==============================] - 22s 52us/sample - loss: 306.1563 - val_loss: 200.3532 - lr: 0.0030\n",
      "Epoch 181/1000\n",
      "414759/414759 [==============================] - 15s 36us/sample - loss: 306.5742 - val_loss: 200.4036 - lr: 0.0030\n",
      "Epoch 182/1000\n",
      "414759/414759 [==============================] - 20s 47us/sample - loss: 306.6145 - val_loss: 199.4462 - lr: 0.0030\n",
      "Epoch 183/1000\n",
      "414759/414759 [==============================] - 16s 39us/sample - loss: 305.4920 - val_loss: 199.2472 - lr: 0.0030\n",
      "Epoch 184/1000\n",
      "414759/414759 [==============================] - 19s 47us/sample - loss: 305.3548 - val_loss: 199.3164 - lr: 0.0030\n",
      "Epoch 185/1000\n",
      "414759/414759 [==============================] - 16s 38us/sample - loss: 305.4736 - val_loss: 198.2586 - lr: 0.0030\n",
      "Epoch 186/1000\n",
      "414759/414759 [==============================] - 24s 57us/sample - loss: 304.3151 - val_loss: 198.6669 - lr: 0.0030\n",
      "Epoch 187/1000\n",
      "414759/414759 [==============================] - 16s 38us/sample - loss: 304.7341 - val_loss: 198.2402 - lr: 0.0030\n",
      "Epoch 188/1000\n",
      "414759/414759 [==============================] - 15s 36us/sample - loss: 304.2391 - val_loss: 198.8899 - lr: 0.0030\n",
      "Epoch 189/1000\n",
      "414759/414759 [==============================] - 15s 36us/sample - loss: 304.9043 - val_loss: 197.6171 - lr: 0.0030\n",
      "Epoch 190/1000\n",
      "414759/414759 [==============================] - 24s 57us/sample - loss: 303.6306 - val_loss: 198.5348 - lr: 0.0030\n",
      "Epoch 191/1000\n",
      "414759/414759 [==============================] - 24s 58us/sample - loss: 304.5766 - val_loss: 197.3760 - lr: 0.0030\n",
      "Epoch 192/1000\n",
      "414759/414759 [==============================] - 21s 50us/sample - loss: 303.3373 - val_loss: 198.4820 - lr: 0.0030\n",
      "Epoch 193/1000\n",
      "414759/414759 [==============================] - 18s 42us/sample - loss: 304.4471 - val_loss: 196.8170 - lr: 0.0030\n",
      "Epoch 194/1000\n",
      "414759/414759 [==============================] - 27s 66us/sample - loss: 302.6719 - val_loss: 198.2706 - lr: 0.0030\n",
      "Epoch 195/1000\n",
      "414759/414759 [==============================] - 17s 40us/sample - loss: 304.2196 - val_loss: 198.1020 - lr: 0.0030\n",
      "Epoch 196/1000\n",
      "414759/414759 [==============================] - 22s 53us/sample - loss: 304.0226 - val_loss: 196.8355 - lr: 0.0030\n",
      "Epoch 197/1000\n",
      "414759/414759 [==============================] - 19s 46us/sample - loss: 302.6059 - val_loss: 199.0066 - lr: 0.0030\n",
      "Epoch 198/1000\n",
      "414759/414759 [==============================] - 20s 47us/sample - loss: 304.8933 - val_loss: 197.4478 - lr: 0.0030\n",
      "Epoch 199/1000\n",
      "414759/414759 [==============================] - 21s 52us/sample - loss: 303.2626 - val_loss: 199.2628 - lr: 0.0030\n",
      "Epoch 200/1000\n",
      "414759/414759 [==============================] - 23s 56us/sample - loss: 305.1662 - val_loss: 197.6009 - lr: 0.0030\n",
      "Epoch 201/1000\n",
      "414759/414759 [==============================] - 25s 61us/sample - loss: 303.4298 - val_loss: 197.9323 - lr: 0.0030\n",
      "Epoch 202/1000\n",
      "414759/414759 [==============================] - 19s 46us/sample - loss: 303.7689 - val_loss: 199.1970 - lr: 0.0030\n",
      "Epoch 203/1000\n",
      "414759/414759 [==============================] - 19s 46us/sample - loss: 305.1408 - val_loss: 198.1595 - lr: 0.0030\n",
      "Epoch 204/1000\n",
      "414759/414759 [==============================] - 21s 50us/sample - loss: 303.9474 - val_loss: 197.2260 - lr: 0.0030\n",
      "Epoch 205/1000\n",
      "414759/414759 [==============================] - 14s 35us/sample - loss: 303.0269 - val_loss: 196.9560 - lr: 0.0030\n",
      "Epoch 206/1000\n",
      "414759/414759 [==============================] - 19s 46us/sample - loss: 302.7080 - val_loss: 197.1657 - lr: 0.0030\n",
      "Epoch 207/1000\n",
      "414759/414759 [==============================] - 19s 45us/sample - loss: 302.9745 - val_loss: 196.6344 - lr: 0.0030\n",
      "Epoch 208/1000\n",
      "414759/414759 [==============================] - 15s 37us/sample - loss: 302.2818 - val_loss: 196.8986 - lr: 0.0030\n",
      "Epoch 209/1000\n",
      "414759/414759 [==============================] - 16s 39us/sample - loss: 302.5541 - val_loss: 195.4331 - lr: 0.0030\n",
      "Epoch 210/1000\n",
      "414759/414759 [==============================] - 22s 54us/sample - loss: 301.0500 - val_loss: 196.5216 - lr: 0.0030\n",
      "Epoch 211/1000\n",
      "414759/414759 [==============================] - 16s 38us/sample - loss: 302.1737 - val_loss: 195.1207 - lr: 0.0030\n",
      "Epoch 212/1000\n",
      "414759/414759 [==============================] - 22s 54us/sample - loss: 300.6273 - val_loss: 196.0052 - lr: 0.0030\n",
      "Epoch 213/1000\n",
      "414759/414759 [==============================] - 17s 40us/sample - loss: 301.6655 - val_loss: 195.3026 - lr: 0.0030\n",
      "Epoch 214/1000\n",
      "414759/414759 [==============================] - 21s 52us/sample - loss: 300.9136 - val_loss: 195.4961 - lr: 0.0030\n",
      "Epoch 215/1000\n",
      "414759/414759 [==============================] - 19s 46us/sample - loss: 301.0194 - val_loss: 195.2138 - lr: 0.0030\n",
      "Epoch 216/1000\n",
      "414759/414759 [==============================] - 15s 36us/sample - loss: 300.8026 - val_loss: 194.9633 - lr: 0.0030\n",
      "Epoch 217/1000\n",
      "414759/414759 [==============================] - 19s 47us/sample - loss: 300.4699 - val_loss: 195.1861 - lr: 0.0030\n",
      "Epoch 218/1000\n",
      "414759/414759 [==============================] - 15s 36us/sample - loss: 300.6526 - val_loss: 195.4532 - lr: 0.0030\n",
      "Epoch 219/1000\n",
      "414759/414759 [==============================] - 19s 47us/sample - loss: 300.9718 - val_loss: 194.3559 - lr: 0.0030\n",
      "Epoch 220/1000\n",
      "414759/414759 [==============================] - 21s 50us/sample - loss: 299.8198 - val_loss: 195.2286 - lr: 0.0030\n",
      "Epoch 221/1000\n",
      "414759/414759 [==============================] - 19s 47us/sample - loss: 300.7627 - val_loss: 194.1413 - lr: 0.0030\n",
      "Epoch 222/1000\n",
      "414759/414759 [==============================] - 21s 52us/sample - loss: 299.5467 - val_loss: 194.7370 - lr: 0.0030\n",
      "Epoch 223/1000\n",
      "414759/414759 [==============================] - 20s 48us/sample - loss: 300.0915 - val_loss: 194.2438 - lr: 0.0030\n",
      "Epoch 224/1000\n",
      "414759/414759 [==============================] - 15s 36us/sample - loss: 299.5992 - val_loss: 194.0052 - lr: 0.0030\n",
      "Epoch 225/1000\n",
      "414759/414759 [==============================] - 20s 47us/sample - loss: 299.3776 - val_loss: 194.2595 - lr: 0.0030\n",
      "Epoch 226/1000\n",
      "414759/414759 [==============================] - 20s 47us/sample - loss: 299.6327 - val_loss: 193.2384 - lr: 0.0030\n",
      "Epoch 227/1000\n",
      "414759/414759 [==============================] - 19s 47us/sample - loss: 298.5134 - val_loss: 193.3701 - lr: 0.0030\n",
      "Epoch 228/1000\n",
      "414759/414759 [==============================] - 15s 36us/sample - loss: 298.6590 - val_loss: 194.0155 - lr: 0.0030\n",
      "Epoch 229/1000\n",
      "414759/414759 [==============================] - 20s 48us/sample - loss: 299.4338 - val_loss: 192.8648 - lr: 0.0030\n",
      "Epoch 230/1000\n",
      "414759/414759 [==============================] - 15s 37us/sample - loss: 298.1148 - val_loss: 193.0721 - lr: 0.0030\n",
      "Epoch 231/1000\n",
      "414759/414759 [==============================] - 16s 39us/sample - loss: 298.2650 - val_loss: 192.6238 - lr: 0.0030\n",
      "Epoch 232/1000\n",
      "414759/414759 [==============================] - 22s 53us/sample - loss: 297.8412 - val_loss: 193.0593 - lr: 0.0030\n",
      "Epoch 233/1000\n",
      "414759/414759 [==============================] - 15s 37us/sample - loss: 298.3107 - val_loss: 192.4203 - lr: 0.0030\n",
      "Epoch 234/1000\n",
      "414759/414759 [==============================] - 22s 52us/sample - loss: 297.5568 - val_loss: 192.7696 - lr: 0.0030\n",
      "Epoch 235/1000\n",
      "414759/414759 [==============================] - 16s 39us/sample - loss: 297.8865 - val_loss: 192.7732 - lr: 0.0030\n",
      "Epoch 236/1000\n",
      "414759/414759 [==============================] - 21s 51us/sample - loss: 297.9700 - val_loss: 192.6850 - lr: 0.0030\n",
      "Epoch 237/1000\n",
      "414759/414759 [==============================] - 16s 39us/sample - loss: 297.8357 - val_loss: 192.1293 - lr: 0.0030\n",
      "Epoch 238/1000\n",
      "414759/414759 [==============================] - 15s 36us/sample - loss: 297.2044 - val_loss: 192.2720 - lr: 0.0030\n",
      "Epoch 239/1000\n",
      "414759/414759 [==============================] - 15s 36us/sample - loss: 297.3968 - val_loss: 191.6248 - lr: 0.0030\n",
      "Epoch 240/1000\n",
      "414759/414759 [==============================] - 18s 44us/sample - loss: 296.6891 - val_loss: 193.4624 - lr: 0.0030\n",
      "Epoch 241/1000\n",
      "414759/414759 [==============================] - 15s 36us/sample - loss: 298.5114 - val_loss: 191.2818 - lr: 0.0030\n",
      "Epoch 242/1000\n",
      "414759/414759 [==============================] - 20s 48us/sample - loss: 296.2374 - val_loss: 194.6582 - lr: 0.0030\n",
      "Epoch 243/1000\n",
      "414759/414759 [==============================] - 14s 35us/sample - loss: 299.8469 - val_loss: 193.3177 - lr: 0.0030\n",
      "Epoch 244/1000\n",
      "414759/414759 [==============================] - 20s 48us/sample - loss: 298.4605 - val_loss: 191.9761 - lr: 0.0030\n",
      "Epoch 245/1000\n",
      "414759/414759 [==============================] - 25s 61us/sample - loss: 296.9913 - val_loss: 193.4266 - lr: 0.0030\n",
      "Epoch 246/1000\n",
      "414759/414759 [==============================] - 15s 36us/sample - loss: 298.4342 - val_loss: 193.2726 - lr: 0.0030\n",
      "Epoch 247/1000\n",
      "414759/414759 [==============================] - 22s 54us/sample - loss: 298.3921 - val_loss: 192.9523 - lr: 0.0030\n",
      "Epoch 248/1000\n",
      "414759/414759 [==============================] - 16s 38us/sample - loss: 297.9659 - val_loss: 191.6908 - lr: 0.0030\n",
      "Epoch 249/1000\n",
      "414759/414759 [==============================] - 20s 49us/sample - loss: 296.6129 - val_loss: 193.1700 - lr: 0.0030\n",
      "Epoch 250/1000\n",
      "414759/414759 [==============================] - 15s 35us/sample - loss: 298.2579 - val_loss: 191.9013 - lr: 0.0030\n",
      "Epoch 251/1000\n",
      "414759/414759 [==============================] - 19s 46us/sample - loss: 296.8716 - val_loss: 192.9354 - lr: 0.0030\n",
      "Epoch 252/1000\n",
      "414759/414759 [==============================] - 16s 38us/sample - loss: 298.0261 - val_loss: 191.4407 - lr: 0.0030\n",
      "Epoch 253/1000\n",
      "414759/414759 [==============================] - 24s 58us/sample - loss: 296.3605 - val_loss: 192.1419 - lr: 0.0030\n",
      "Epoch 254/1000\n",
      "414759/414759 [==============================] - 15s 37us/sample - loss: 297.0771 - val_loss: 191.7196 - lr: 0.0030\n",
      "Epoch 255/1000\n",
      "414759/414759 [==============================] - 20s 49us/sample - loss: 296.6180 - val_loss: 191.6660 - lr: 0.0030\n",
      "Epoch 256/1000\n",
      "414759/414759 [==============================] - 23s 55us/sample - loss: 296.4470 - val_loss: 191.9735 - lr: 0.0030\n",
      "Epoch 257/1000\n",
      "414759/414759 [==============================] - 25s 60us/sample - loss: 296.8557 - val_loss: 189.8806 - lr: 0.0022\n",
      "Epoch 258/1000\n",
      "414759/414759 [==============================] - 21s 52us/sample - loss: 294.5478 - val_loss: 191.1987 - lr: 0.0022\n",
      "Epoch 259/1000\n",
      "414759/414759 [==============================] - 16s 38us/sample - loss: 295.9370 - val_loss: 190.3494 - lr: 0.0022\n",
      "Epoch 260/1000\n",
      "414759/414759 [==============================] - 21s 51us/sample - loss: 295.0094 - val_loss: 189.8826 - lr: 0.0022\n",
      "Epoch 261/1000\n",
      "414759/414759 [==============================] - 21s 50us/sample - loss: 294.5389 - val_loss: 190.3232 - lr: 0.0022\n",
      "Epoch 262/1000\n",
      "414759/414759 [==============================] - 25s 59us/sample - loss: 294.9713 - val_loss: 189.3629 - lr: 0.0022\n",
      "Epoch 263/1000\n",
      "414759/414759 [==============================] - 16s 38us/sample - loss: 293.9173 - val_loss: 190.5308 - lr: 0.0022\n",
      "Epoch 264/1000\n",
      "414759/414759 [==============================] - 20s 47us/sample - loss: 295.1875 - val_loss: 189.5859 - lr: 0.0022\n",
      "Epoch 265/1000\n",
      "414759/414759 [==============================] - 15s 36us/sample - loss: 294.1549 - val_loss: 190.0527 - lr: 0.0022\n",
      "Epoch 266/1000\n",
      "414759/414759 [==============================] - 21s 51us/sample - loss: 294.6342 - val_loss: 189.8076 - lr: 0.0022\n",
      "Epoch 267/1000\n",
      "414759/414759 [==============================] - 15s 36us/sample - loss: 294.3369 - val_loss: 189.2307 - lr: 0.0022\n",
      "Epoch 268/1000\n",
      "414759/414759 [==============================] - 20s 49us/sample - loss: 293.7415 - val_loss: 189.2537 - lr: 0.0022\n",
      "Epoch 269/1000\n",
      "414759/414759 [==============================] - 16s 37us/sample - loss: 293.7273 - val_loss: 189.2527 - lr: 0.0022\n",
      "Epoch 270/1000\n",
      "414759/414759 [==============================] - 21s 50us/sample - loss: 293.7402 - val_loss: 188.2650 - lr: 0.0022\n",
      "Epoch 271/1000\n",
      "414759/414759 [==============================] - 15s 37us/sample - loss: 292.6580 - val_loss: 189.3012 - lr: 0.0022\n",
      "Epoch 272/1000\n",
      "414759/414759 [==============================] - 22s 53us/sample - loss: 293.7466 - val_loss: 188.3310 - lr: 0.0022\n",
      "Epoch 273/1000\n",
      "414759/414759 [==============================] - 21s 49us/sample - loss: 292.7332 - val_loss: 188.8802 - lr: 0.0022\n",
      "Epoch 274/1000\n",
      "414759/414759 [==============================] - 21s 50us/sample - loss: 293.1935 - val_loss: 188.5112 - lr: 0.0022\n",
      "Epoch 275/1000\n",
      "414759/414759 [==============================] - 15s 36us/sample - loss: 292.8722 - val_loss: 188.4763 - lr: 0.0022\n",
      "Epoch 276/1000\n",
      "414759/414759 [==============================] - 21s 50us/sample - loss: 292.7894 - val_loss: 187.8614 - lr: 0.0022\n",
      "Epoch 277/1000\n",
      "414759/414759 [==============================] - 15s 37us/sample - loss: 292.1595 - val_loss: 187.9838 - lr: 0.0022\n",
      "Epoch 278/1000\n",
      "414759/414759 [==============================] - 16s 38us/sample - loss: 292.2496 - val_loss: 187.7430 - lr: 0.0022\n",
      "Epoch 279/1000\n",
      "414759/414759 [==============================] - 15s 36us/sample - loss: 291.9913 - val_loss: 187.4811 - lr: 0.0022\n",
      "Epoch 280/1000\n",
      "414759/414759 [==============================] - 15s 37us/sample - loss: 291.6728 - val_loss: 187.5619 - lr: 0.0022\n",
      "Epoch 281/1000\n",
      "414759/414759 [==============================] - 19s 46us/sample - loss: 291.7461 - val_loss: 186.8804 - lr: 0.0022\n",
      "Epoch 282/1000\n",
      "414759/414759 [==============================] - 15s 36us/sample - loss: 290.9758 - val_loss: 187.5425 - lr: 0.0022\n",
      "Epoch 283/1000\n",
      "414759/414759 [==============================] - 23s 56us/sample - loss: 291.6972 - val_loss: 186.7366 - lr: 0.0022\n",
      "Epoch 284/1000\n",
      "414759/414759 [==============================] - 14s 34us/sample - loss: 290.8038 - val_loss: 188.0961 - lr: 0.0022\n",
      "Epoch 285/1000\n",
      "414759/414759 [==============================] - 22s 53us/sample - loss: 292.2021 - val_loss: 186.7195 - lr: 0.0022\n",
      "Epoch 286/1000\n",
      "414759/414759 [==============================] - 14s 35us/sample - loss: 290.7666 - val_loss: 189.1513 - lr: 0.0022\n",
      "Epoch 287/1000\n",
      "414759/414759 [==============================] - 21s 50us/sample - loss: 293.3542 - val_loss: 188.1495 - lr: 0.0022\n",
      "Epoch 288/1000\n",
      "414759/414759 [==============================] - 20s 48us/sample - loss: 292.3053 - val_loss: 187.5891 - lr: 0.0022\n",
      "Epoch 289/1000\n",
      "414759/414759 [==============================] - 21s 51us/sample - loss: 291.6601 - val_loss: 188.9470 - lr: 0.0022\n",
      "Epoch 290/1000\n",
      "414759/414759 [==============================] - 27s 64us/sample - loss: 293.0909 - val_loss: 187.8726 - lr: 0.0022\n",
      "Epoch 291/1000\n",
      "414759/414759 [==============================] - 26s 63us/sample - loss: 291.9456 - val_loss: 187.5695 - lr: 0.0022\n",
      "Epoch 292/1000\n",
      "414759/414759 [==============================] - 15s 36us/sample - loss: 291.5879 - val_loss: 188.3690 - lr: 0.0022\n",
      "Epoch 293/1000\n",
      "414759/414759 [==============================] - 24s 57us/sample - loss: 292.4688 - val_loss: 187.5515 - lr: 0.0022\n",
      "Epoch 294/1000\n",
      "414759/414759 [==============================] - 16s 38us/sample - loss: 291.5576 - val_loss: 186.7850 - lr: 0.0022\n",
      "Epoch 295/1000\n",
      "414759/414759 [==============================] - 21s 50us/sample - loss: 290.7426 - val_loss: 186.8025 - lr: 0.0022\n",
      "Epoch 296/1000\n",
      "414759/414759 [==============================] - 15s 37us/sample - loss: 290.8045 - val_loss: 186.8218 - lr: 0.0022\n",
      "Epoch 297/1000\n",
      "414759/414759 [==============================] - 19s 47us/sample - loss: 290.7305 - val_loss: 186.2129 - lr: 0.0022\n",
      "Epoch 298/1000\n",
      "414759/414759 [==============================] - 15s 36us/sample - loss: 290.0961 - val_loss: 186.3897 - lr: 0.0022\n",
      "Epoch 299/1000\n",
      "414759/414759 [==============================] - 15s 35us/sample - loss: 290.2368 - val_loss: 186.1060 - lr: 0.0022\n",
      "Epoch 300/1000\n",
      "414759/414759 [==============================] - 20s 49us/sample - loss: 289.9413 - val_loss: 186.0109 - lr: 0.0022\n",
      "Epoch 301/1000\n",
      "414759/414759 [==============================] - 20s 48us/sample - loss: 289.7502 - val_loss: 185.7424 - lr: 0.0022\n",
      "Epoch 302/1000\n",
      "414759/414759 [==============================] - 16s 38us/sample - loss: 289.5342 - val_loss: 185.6580 - lr: 0.0022\n",
      "Epoch 303/1000\n",
      "414759/414759 [==============================] - 14s 34us/sample - loss: 289.3434 - val_loss: 185.1510 - lr: 0.0022\n",
      "Epoch 304/1000\n",
      "414759/414759 [==============================] - 15s 36us/sample - loss: 288.7494 - val_loss: 185.3116 - lr: 0.0022\n",
      "Epoch 305/1000\n",
      "414759/414759 [==============================] - 15s 36us/sample - loss: 288.9167 - val_loss: 185.0564 - lr: 0.0022\n",
      "Epoch 306/1000\n",
      "414759/414759 [==============================] - 17s 41us/sample - loss: 288.6126 - val_loss: 185.3122 - lr: 0.0022\n",
      "Epoch 307/1000\n",
      "414759/414759 [==============================] - 15s 37us/sample - loss: 288.8535 - val_loss: 184.8048 - lr: 0.0022\n",
      "Epoch 308/1000\n",
      "414759/414759 [==============================] - 29s 70us/sample - loss: 288.3077 - val_loss: 185.4139 - lr: 0.0022\n",
      "Epoch 309/1000\n",
      "414759/414759 [==============================] - 25s 60us/sample - loss: 289.0218 - val_loss: 184.3769 - lr: 0.0022\n",
      "Epoch 310/1000\n",
      "414759/414759 [==============================] - 21s 51us/sample - loss: 287.8273 - val_loss: 186.2896 - lr: 0.0022\n",
      "Epoch 311/1000\n",
      "414759/414759 [==============================] - 20s 49us/sample - loss: 289.8392 - val_loss: 184.7093 - lr: 0.0022\n",
      "Epoch 312/1000\n",
      "414759/414759 [==============================] - 19s 47us/sample - loss: 288.1570 - val_loss: 186.4117 - lr: 0.0022\n",
      "Epoch 313/1000\n",
      "414759/414759 [==============================] - 21s 50us/sample - loss: 289.9395 - val_loss: 186.1214 - lr: 0.0022\n",
      "Epoch 314/1000\n",
      "414759/414759 [==============================] - 21s 50us/sample - loss: 289.6411 - val_loss: 184.6615 - lr: 0.0022\n",
      "Epoch 315/1000\n",
      "414759/414759 [==============================] - 20s 49us/sample - loss: 288.0488 - val_loss: 187.1180 - lr: 0.0022\n",
      "Epoch 316/1000\n",
      "414759/414759 [==============================] - 17s 42us/sample - loss: 290.7678 - val_loss: 185.7205 - lr: 0.0022\n",
      "Epoch 317/1000\n",
      "414759/414759 [==============================] - 20s 48us/sample - loss: 289.1751 - val_loss: 185.9252 - lr: 0.0022\n",
      "Epoch 318/1000\n",
      "414759/414759 [==============================] - 19s 46us/sample - loss: 289.4795 - val_loss: 186.7925 - lr: 0.0022\n",
      "Epoch 319/1000\n",
      "414759/414759 [==============================] - 22s 53us/sample - loss: 290.4286 - val_loss: 186.2892 - lr: 0.0022\n",
      "Epoch 320/1000\n",
      "414759/414759 [==============================] - 22s 52us/sample - loss: 289.7102 - val_loss: 184.2926 - lr: 0.0022\n",
      "Epoch 321/1000\n",
      "414759/414759 [==============================] - 15s 36us/sample - loss: 287.5974 - val_loss: 187.0919 - lr: 0.0022\n",
      "Epoch 322/1000\n",
      "414759/414759 [==============================] - 22s 54us/sample - loss: 290.6432 - val_loss: 185.9330 - lr: 0.0022\n",
      "Epoch 323/1000\n",
      "414759/414759 [==============================] - 16s 38us/sample - loss: 289.3933 - val_loss: 185.1098 - lr: 0.0022\n",
      "Epoch 324/1000\n",
      "414759/414759 [==============================] - 20s 49us/sample - loss: 288.4684 - val_loss: 186.2006 - lr: 0.0022\n",
      "Epoch 325/1000\n",
      "414759/414759 [==============================] - 16s 38us/sample - loss: 289.6513 - val_loss: 185.2332 - lr: 0.0022\n",
      "Epoch 326/1000\n",
      "414759/414759 [==============================] - 23s 55us/sample - loss: 288.5540 - val_loss: 185.4419 - lr: 0.0022\n",
      "Epoch 327/1000\n",
      "414759/414759 [==============================] - 15s 37us/sample - loss: 288.7897 - val_loss: 184.8225 - lr: 0.0022\n",
      "Epoch 328/1000\n",
      "414759/414759 [==============================] - 15s 36us/sample - loss: 288.0946 - val_loss: 185.0226 - lr: 0.0022\n",
      "Epoch 329/1000\n",
      "414759/414759 [==============================] - 19s 46us/sample - loss: 288.1928 - val_loss: 184.3486 - lr: 0.0022\n",
      "Epoch 330/1000\n",
      "414759/414759 [==============================] - 15s 37us/sample - loss: 287.4961 - val_loss: 184.5332 - lr: 0.0022\n",
      "Epoch 331/1000\n",
      "414759/414759 [==============================] - 19s 46us/sample - loss: 287.6319 - val_loss: 184.0368 - lr: 0.0022\n",
      "Epoch 332/1000\n",
      "414759/414759 [==============================] - 15s 36us/sample - loss: 287.0476 - val_loss: 183.9536 - lr: 0.0022\n",
      "Epoch 333/1000\n",
      "414759/414759 [==============================] - 15s 36us/sample - loss: 286.9612 - val_loss: 184.0401 - lr: 0.0022\n",
      "Epoch 334/1000\n",
      "414759/414759 [==============================] - 15s 37us/sample - loss: 287.0350 - val_loss: 183.7550 - lr: 0.0022\n",
      "Epoch 335/1000\n",
      "414759/414759 [==============================] - 15s 36us/sample - loss: 286.6432 - val_loss: 183.4258 - lr: 0.0022\n",
      "Epoch 336/1000\n",
      "414759/414759 [==============================] - 15s 37us/sample - loss: 286.3380 - val_loss: 183.5016 - lr: 0.0022\n",
      "Epoch 337/1000\n",
      "414759/414759 [==============================] - 24s 59us/sample - loss: 286.4014 - val_loss: 183.2363 - lr: 0.0022\n",
      "Epoch 338/1000\n",
      "414759/414759 [==============================] - 14s 35us/sample - loss: 286.0915 - val_loss: 183.2394 - lr: 0.0022\n",
      "Epoch 339/1000\n",
      "414759/414759 [==============================] - 15s 37us/sample - loss: 286.0347 - val_loss: 183.3492 - lr: 0.0022\n",
      "Epoch 340/1000\n",
      "414759/414759 [==============================] - 20s 48us/sample - loss: 286.1333 - val_loss: 182.7333 - lr: 0.0022\n",
      "Epoch 341/1000\n",
      "414759/414759 [==============================] - 25s 60us/sample - loss: 285.4686 - val_loss: 183.1923 - lr: 0.0022\n",
      "Epoch 342/1000\n",
      "414759/414759 [==============================] - 15s 37us/sample - loss: 285.9315 - val_loss: 182.6514 - lr: 0.0022\n",
      "Epoch 343/1000\n",
      "414759/414759 [==============================] - 20s 47us/sample - loss: 285.3188 - val_loss: 183.1648 - lr: 0.0022\n",
      "Epoch 344/1000\n",
      "414759/414759 [==============================] - 16s 37us/sample - loss: 285.7846 - val_loss: 182.5204 - lr: 0.0022\n",
      "Epoch 345/1000\n",
      "414759/414759 [==============================] - 20s 48us/sample - loss: 285.0835 - val_loss: 183.4522 - lr: 0.0022\n",
      "Epoch 346/1000\n",
      "414759/414759 [==============================] - 13s 31us/sample - loss: 286.1372 - val_loss: 182.5010 - lr: 0.0022\n",
      "Epoch 347/1000\n",
      "414759/414759 [==============================] - 22s 54us/sample - loss: 285.0036 - val_loss: 183.4428 - lr: 0.0022\n",
      "Epoch 348/1000\n",
      "414759/414759 [==============================] - 14s 34us/sample - loss: 286.0215 - val_loss: 182.8638 - lr: 0.0022\n",
      "Epoch 349/1000\n",
      "414759/414759 [==============================] - 18s 44us/sample - loss: 285.4355 - val_loss: 182.8189 - lr: 0.0022\n",
      "Epoch 350/1000\n",
      "414759/414759 [==============================] - 20s 47us/sample - loss: 285.3262 - val_loss: 183.4273 - lr: 0.0022\n",
      "Epoch 351/1000\n",
      "414759/414759 [==============================] - 18s 43us/sample - loss: 285.9242 - val_loss: 182.0684 - lr: 0.0022\n",
      "Epoch 352/1000\n",
      "414759/414759 [==============================] - 14s 35us/sample - loss: 284.4817 - val_loss: 183.8296 - lr: 0.0022\n",
      "Epoch 353/1000\n",
      "414759/414759 [==============================] - 14s 35us/sample - loss: 286.3584 - val_loss: 183.3091 - lr: 0.0022\n",
      "Epoch 354/1000\n",
      "414759/414759 [==============================] - 19s 46us/sample - loss: 285.7885 - val_loss: 182.6292 - lr: 0.0022\n",
      "Epoch 355/1000\n",
      "414759/414759 [==============================] - 15s 36us/sample - loss: 284.9634 - val_loss: 183.8529 - lr: 0.0022\n",
      "Epoch 356/1000\n",
      "414759/414759 [==============================] - 19s 45us/sample - loss: 286.3293 - val_loss: 183.0282 - lr: 0.0022\n",
      "Epoch 357/1000\n",
      "414759/414759 [==============================] - 19s 47us/sample - loss: 285.3829 - val_loss: 182.5061 - lr: 0.0022\n",
      "Epoch 358/1000\n",
      "414759/414759 [==============================] - 20s 48us/sample - loss: 284.7862 - val_loss: 183.1593 - lr: 0.0022\n",
      "Epoch 359/1000\n",
      "414759/414759 [==============================] - 19s 46us/sample - loss: 285.5292 - val_loss: 182.3925 - lr: 0.0022\n",
      "Epoch 360/1000\n",
      "414759/414759 [==============================] - 22s 52us/sample - loss: 284.5948 - val_loss: 182.4946 - lr: 0.0022\n",
      "Epoch 361/1000\n",
      "414759/414759 [==============================] - 20s 49us/sample - loss: 284.7097 - val_loss: 182.5940 - lr: 0.0022\n",
      "Epoch 362/1000\n",
      "414759/414759 [==============================] - 20s 48us/sample - loss: 284.9122 - val_loss: 182.1027 - lr: 0.0022\n",
      "Epoch 363/1000\n",
      "414759/414759 [==============================] - 20s 47us/sample - loss: 284.3076 - val_loss: 181.7619 - lr: 0.0022\n",
      "Epoch 364/1000\n",
      "414759/414759 [==============================] - 17s 42us/sample - loss: 283.9096 - val_loss: 182.2195 - lr: 0.0022\n",
      "Epoch 365/1000\n",
      "414759/414759 [==============================] - 20s 48us/sample - loss: 284.4463 - val_loss: 181.6970 - lr: 0.0022\n",
      "Epoch 366/1000\n",
      "414759/414759 [==============================] - 20s 47us/sample - loss: 283.6937 - val_loss: 181.8358 - lr: 0.0022\n",
      "Epoch 367/1000\n",
      "414759/414759 [==============================] - 19s 46us/sample - loss: 283.8756 - val_loss: 181.5962 - lr: 0.0022\n",
      "Epoch 368/1000\n",
      "414759/414759 [==============================] - 20s 47us/sample - loss: 283.6441 - val_loss: 181.6834 - lr: 0.0022\n",
      "Epoch 369/1000\n",
      "414759/414759 [==============================] - 19s 46us/sample - loss: 283.5698 - val_loss: 181.1424 - lr: 0.0022\n",
      "Epoch 370/1000\n",
      "414759/414759 [==============================] - 20s 47us/sample - loss: 282.9974 - val_loss: 181.7986 - lr: 0.0022\n",
      "Epoch 371/1000\n",
      "414759/414759 [==============================] - 19s 46us/sample - loss: 283.6476 - val_loss: 181.0266 - lr: 0.0022\n",
      "Epoch 372/1000\n",
      "414759/414759 [==============================] - 23s 56us/sample - loss: 282.8426 - val_loss: 182.0010 - lr: 0.0022\n",
      "Epoch 373/1000\n",
      "414759/414759 [==============================] - 19s 47us/sample - loss: 283.8640 - val_loss: 180.9367 - lr: 0.0022\n",
      "Epoch 374/1000\n",
      "414759/414759 [==============================] - 20s 49us/sample - loss: 282.6249 - val_loss: 182.9975 - lr: 0.0022\n",
      "Epoch 375/1000\n",
      "414759/414759 [==============================] - 20s 47us/sample - loss: 284.9146 - val_loss: 181.6080 - lr: 0.0022\n",
      "Epoch 376/1000\n",
      "414759/414759 [==============================] - 20s 47us/sample - loss: 283.3728 - val_loss: 182.5253 - lr: 0.0022\n",
      "Epoch 377/1000\n",
      "414759/414759 [==============================] - 21s 51us/sample - loss: 284.4433 - val_loss: 183.1516 - lr: 0.0022\n",
      "Epoch 378/1000\n",
      "414759/414759 [==============================] - 15s 35us/sample - loss: 285.0714 - val_loss: 181.1994 - lr: 0.0022\n",
      "Epoch 379/1000\n",
      "414759/414759 [==============================] - 22s 53us/sample - loss: 282.8751 - val_loss: 182.8275 - lr: 0.0022\n",
      "Epoch 380/1000\n",
      "414759/414759 [==============================] - 20s 47us/sample - loss: 284.6050 - val_loss: 182.3528 - lr: 0.0022\n",
      "Epoch 381/1000\n",
      "414759/414759 [==============================] - 14s 35us/sample - loss: 284.0998 - val_loss: 181.3970 - lr: 0.0022\n",
      "Epoch 382/1000\n",
      "414759/414759 [==============================] - 19s 45us/sample - loss: 282.9695 - val_loss: 183.7074 - lr: 0.0022\n",
      "Epoch 383/1000\n",
      "414759/414759 [==============================] - 18s 43us/sample - loss: 285.5243 - val_loss: 182.2997 - lr: 0.0022\n",
      "Epoch 384/1000\n",
      "414759/414759 [==============================] - 19s 47us/sample - loss: 283.9427 - val_loss: 182.3589 - lr: 0.0022\n",
      "Epoch 385/1000\n",
      "414759/414759 [==============================] - 15s 36us/sample - loss: 284.0708 - val_loss: 182.5596 - lr: 0.0022\n",
      "Epoch 386/1000\n",
      "414759/414759 [==============================] - 21s 51us/sample - loss: 284.2292 - val_loss: 182.5266 - lr: 0.0022\n",
      "Epoch 387/1000\n",
      "414759/414759 [==============================] - 21s 50us/sample - loss: 284.0483 - val_loss: 181.2892 - lr: 0.0022\n",
      "Epoch 388/1000\n",
      "414759/414759 [==============================] - 15s 35us/sample - loss: 282.8011 - val_loss: 182.8887 - lr: 0.0022\n",
      "Epoch 389/1000\n",
      "414759/414759 [==============================] - 20s 48us/sample - loss: 284.4840 - val_loss: 181.8622 - lr: 0.0017\n",
      "Epoch 390/1000\n",
      "414759/414759 [==============================] - 15s 37us/sample - loss: 283.2799 - val_loss: 180.9848 - lr: 0.0017\n",
      "Epoch 391/1000\n",
      "414759/414759 [==============================] - 20s 49us/sample - loss: 282.1880 - val_loss: 181.4689 - lr: 0.0017\n",
      "Epoch 392/1000\n",
      "414759/414759 [==============================] - 15s 35us/sample - loss: 282.8277 - val_loss: 180.8464 - lr: 0.0017\n",
      "Epoch 393/1000\n",
      "414759/414759 [==============================] - 20s 49us/sample - loss: 282.0956 - val_loss: 181.1000 - lr: 0.0017\n",
      "Epoch 394/1000\n",
      "414759/414759 [==============================] - 15s 37us/sample - loss: 282.3440 - val_loss: 180.5697 - lr: 0.0017\n",
      "Epoch 395/1000\n",
      "414759/414759 [==============================] - 20s 48us/sample - loss: 281.7784 - val_loss: 180.6750 - lr: 0.0017\n",
      "Epoch 396/1000\n",
      "414759/414759 [==============================] - 20s 49us/sample - loss: 281.8417 - val_loss: 180.4150 - lr: 0.0017\n",
      "Epoch 397/1000\n",
      "414759/414759 [==============================] - 22s 54us/sample - loss: 281.4695 - val_loss: 180.2813 - lr: 0.0017\n",
      "Epoch 398/1000\n",
      "414759/414759 [==============================] - 19s 47us/sample - loss: 281.3683 - val_loss: 180.3806 - lr: 0.0017\n",
      "Epoch 399/1000\n",
      "414759/414759 [==============================] - 22s 53us/sample - loss: 281.4662 - val_loss: 179.9401 - lr: 0.0017\n",
      "Epoch 400/1000\n",
      "414759/414759 [==============================] - 21s 50us/sample - loss: 280.9516 - val_loss: 180.0026 - lr: 0.0017\n",
      "Epoch 401/1000\n",
      "414759/414759 [==============================] - 14s 35us/sample - loss: 280.9731 - val_loss: 179.7680 - lr: 0.0017\n",
      "Epoch 402/1000\n",
      "414759/414759 [==============================] - 15s 37us/sample - loss: 280.6895 - val_loss: 179.6201 - lr: 0.0017\n",
      "Epoch 403/1000\n",
      "414759/414759 [==============================] - 15s 35us/sample - loss: 280.4886 - val_loss: 179.5569 - lr: 0.0017\n",
      "Epoch 404/1000\n",
      "414759/414759 [==============================] - 16s 39us/sample - loss: 280.3781 - val_loss: 179.4981 - lr: 0.0017\n",
      "Epoch 405/1000\n",
      "414759/414759 [==============================] - 17s 42us/sample - loss: 280.2443 - val_loss: 179.3702 - lr: 0.0017\n",
      "Epoch 406/1000\n",
      "414759/414759 [==============================] - 16s 38us/sample - loss: 280.1476 - val_loss: 179.3465 - lr: 0.0017\n",
      "Epoch 407/1000\n",
      "414759/414759 [==============================] - 15s 36us/sample - loss: 280.0795 - val_loss: 179.2136 - lr: 0.0017\n",
      "Epoch 408/1000\n",
      "414759/414759 [==============================] - 15s 35us/sample - loss: 279.8045 - val_loss: 179.2601 - lr: 0.0017\n",
      "Epoch 409/1000\n",
      "414759/414759 [==============================] - 21s 51us/sample - loss: 279.9206 - val_loss: 179.0025 - lr: 0.0017\n",
      "Epoch 410/1000\n",
      "414759/414759 [==============================] - 20s 48us/sample - loss: 279.5888 - val_loss: 179.1665 - lr: 0.0017\n",
      "Epoch 411/1000\n",
      "414759/414759 [==============================] - 14s 34us/sample - loss: 279.7096 - val_loss: 178.8507 - lr: 0.0017\n",
      "Epoch 412/1000\n",
      "414759/414759 [==============================] - 15s 35us/sample - loss: 279.3683 - val_loss: 179.1526 - lr: 0.0017\n",
      "Epoch 413/1000\n",
      "414759/414759 [==============================] - 15s 36us/sample - loss: 279.6342 - val_loss: 178.6482 - lr: 0.0017\n",
      "Epoch 414/1000\n",
      "414759/414759 [==============================] - 19s 47us/sample - loss: 279.1613 - val_loss: 179.2857 - lr: 0.0017\n",
      "Epoch 415/1000\n",
      "414759/414759 [==============================] - 18s 44us/sample - loss: 279.8056 - val_loss: 178.6765 - lr: 0.0017\n",
      "Epoch 416/1000\n",
      "414759/414759 [==============================] - 22s 53us/sample - loss: 279.0050 - val_loss: 179.4524 - lr: 0.0017\n",
      "Epoch 417/1000\n",
      "414759/414759 [==============================] - 22s 52us/sample - loss: 279.8247 - val_loss: 178.8355 - lr: 0.0017\n",
      "Epoch 418/1000\n",
      "414759/414759 [==============================] - 21s 51us/sample - loss: 279.1401 - val_loss: 179.0901 - lr: 0.0017\n",
      "Epoch 419/1000\n",
      "414759/414759 [==============================] - 20s 48us/sample - loss: 279.3508 - val_loss: 179.3331 - lr: 0.0017\n",
      "Epoch 420/1000\n",
      "414759/414759 [==============================] - 16s 38us/sample - loss: 279.6588 - val_loss: 178.4933 - lr: 0.0017\n",
      "Epoch 421/1000\n",
      "414759/414759 [==============================] - 21s 51us/sample - loss: 278.6608 - val_loss: 179.3714 - lr: 0.0017\n",
      "Epoch 422/1000\n",
      "414759/414759 [==============================] - 16s 38us/sample - loss: 279.6336 - val_loss: 178.7851 - lr: 0.0017\n",
      "Epoch 423/1000\n",
      "414759/414759 [==============================] - 22s 52us/sample - loss: 278.9796 - val_loss: 178.8558 - lr: 0.0017\n",
      "Epoch 424/1000\n",
      "414759/414759 [==============================] - 22s 52us/sample - loss: 279.0081 - val_loss: 179.0658 - lr: 0.0017\n",
      "Epoch 425/1000\n",
      "414759/414759 [==============================] - 14s 34us/sample - loss: 279.2300 - val_loss: 178.3491 - lr: 0.0017\n",
      "Epoch 426/1000\n",
      "414759/414759 [==============================] - 20s 49us/sample - loss: 278.3536 - val_loss: 179.4994 - lr: 0.0017\n",
      "Epoch 427/1000\n",
      "414759/414759 [==============================] - 14s 33us/sample - loss: 279.6409 - val_loss: 179.1822 - lr: 0.0017\n",
      "Epoch 428/1000\n",
      "414759/414759 [==============================] - 19s 45us/sample - loss: 279.2314 - val_loss: 178.7541 - lr: 0.0017\n",
      "Epoch 429/1000\n",
      "414759/414759 [==============================] - 15s 36us/sample - loss: 278.6609 - val_loss: 179.0465 - lr: 0.0017\n",
      "Epoch 430/1000\n",
      "414759/414759 [==============================] - 22s 54us/sample - loss: 279.0190 - val_loss: 179.0211 - lr: 0.0017\n",
      "Epoch 431/1000\n",
      "414759/414759 [==============================] - 21s 50us/sample - loss: 278.9829 - val_loss: 178.0540 - lr: 0.0017\n",
      "Epoch 432/1000\n",
      "414759/414759 [==============================] - 15s 36us/sample - loss: 277.8875 - val_loss: 179.3615 - lr: 0.0017\n",
      "Epoch 433/1000\n",
      "414759/414759 [==============================] - 15s 36us/sample - loss: 279.2248 - val_loss: 178.7101 - lr: 0.0017\n",
      "Epoch 434/1000\n",
      "414759/414759 [==============================] - 20s 48us/sample - loss: 278.5197 - val_loss: 178.9582 - lr: 0.0017\n",
      "Epoch 435/1000\n",
      "414759/414759 [==============================] - 21s 50us/sample - loss: 278.7358 - val_loss: 178.5988 - lr: 0.0017\n",
      "Epoch 436/1000\n",
      "414759/414759 [==============================] - 14s 34us/sample - loss: 278.3463 - val_loss: 179.2271 - lr: 0.0017\n",
      "Epoch 437/1000\n",
      "414759/414759 [==============================] - 21s 51us/sample - loss: 278.9740 - val_loss: 178.8103 - lr: 0.0017\n",
      "Epoch 438/1000\n",
      "414759/414759 [==============================] - 20s 47us/sample - loss: 278.5087 - val_loss: 178.4212 - lr: 0.0017\n",
      "Epoch 439/1000\n",
      "414759/414759 [==============================] - 15s 35us/sample - loss: 278.0798 - val_loss: 178.4788 - lr: 0.0017\n",
      "Epoch 440/1000\n",
      "414759/414759 [==============================] - 23s 55us/sample - loss: 278.0564 - val_loss: 178.6130 - lr: 0.0017\n",
      "Epoch 441/1000\n",
      "414759/414759 [==============================] - 14s 34us/sample - loss: 278.1817 - val_loss: 178.1574 - lr: 0.0017\n",
      "Epoch 442/1000\n",
      "414759/414759 [==============================] - 20s 48us/sample - loss: 277.7020 - val_loss: 178.4026 - lr: 0.0017\n",
      "Epoch 443/1000\n",
      "414759/414759 [==============================] - 20s 49us/sample - loss: 277.8860 - val_loss: 178.4269 - lr: 0.0017\n",
      "Epoch 444/1000\n",
      "414759/414759 [==============================] - 16s 38us/sample - loss: 277.8694 - val_loss: 177.9604 - lr: 0.0017\n",
      "Epoch 445/1000\n",
      "414759/414759 [==============================] - 16s 38us/sample - loss: 277.2999 - val_loss: 178.3638 - lr: 0.0017\n",
      "Epoch 446/1000\n",
      "414759/414759 [==============================] - 14s 34us/sample - loss: 277.7312 - val_loss: 177.9151 - lr: 0.0017\n",
      "Epoch 447/1000\n",
      "414759/414759 [==============================] - 17s 41us/sample - loss: 277.2405 - val_loss: 178.4644 - lr: 0.0017\n",
      "Epoch 448/1000\n",
      "414759/414759 [==============================] - 15s 35us/sample - loss: 277.7419 - val_loss: 178.0066 - lr: 0.0017\n",
      "Epoch 449/1000\n",
      "414759/414759 [==============================] - 22s 52us/sample - loss: 277.2597 - val_loss: 178.3585 - lr: 0.0017\n",
      "Epoch 450/1000\n",
      "414759/414759 [==============================] - 22s 53us/sample - loss: 277.6487 - val_loss: 177.6702 - lr: 0.0017\n",
      "Epoch 451/1000\n",
      "414759/414759 [==============================] - 15s 36us/sample - loss: 276.8392 - val_loss: 178.7972 - lr: 0.0017\n",
      "Epoch 452/1000\n",
      "414759/414759 [==============================] - 19s 46us/sample - loss: 278.0154 - val_loss: 178.0139 - lr: 0.0017\n",
      "Epoch 453/1000\n",
      "414759/414759 [==============================] - 17s 41us/sample - loss: 277.1877 - val_loss: 178.3382 - lr: 0.0017\n",
      "Epoch 454/1000\n",
      "414759/414759 [==============================] - 20s 48us/sample - loss: 277.4411 - val_loss: 178.4341 - lr: 0.0017\n",
      "Epoch 455/1000\n",
      "414759/414759 [==============================] - 15s 37us/sample - loss: 277.4772 - val_loss: 177.7654 - lr: 0.0017\n",
      "Epoch 456/1000\n",
      "414759/414759 [==============================] - 19s 47us/sample - loss: 276.7931 - val_loss: 178.0057 - lr: 0.0017\n",
      "Epoch 457/1000\n",
      "414759/414759 [==============================] - 15s 35us/sample - loss: 276.9950 - val_loss: 178.0826 - lr: 0.0017\n",
      "Epoch 458/1000\n",
      "414759/414759 [==============================] - 25s 61us/sample - loss: 277.0716 - val_loss: 177.3741 - lr: 0.0017\n",
      "Epoch 459/1000\n",
      "414759/414759 [==============================] - 21s 50us/sample - loss: 276.2108 - val_loss: 178.2451 - lr: 0.0017\n",
      "Epoch 460/1000\n",
      "414759/414759 [==============================] - 22s 53us/sample - loss: 277.1436 - val_loss: 177.5383 - lr: 0.0017\n",
      "Epoch 461/1000\n",
      "414759/414759 [==============================] - 24s 58us/sample - loss: 276.3683 - val_loss: 178.0546 - lr: 0.0017\n",
      "Epoch 462/1000\n",
      "414759/414759 [==============================] - 25s 59us/sample - loss: 276.8248 - val_loss: 178.0412 - lr: 0.0017\n",
      "Epoch 463/1000\n",
      "414759/414759 [==============================] - 23s 55us/sample - loss: 276.8230 - val_loss: 177.5369 - lr: 0.0017\n",
      "Epoch 464/1000\n",
      "414759/414759 [==============================] - 22s 54us/sample - loss: 276.2231 - val_loss: 177.8448 - lr: 0.0017\n",
      "Epoch 465/1000\n",
      "414759/414759 [==============================] - 26s 64us/sample - loss: 276.5254 - val_loss: 177.7422 - lr: 0.0017\n",
      "Epoch 466/1000\n",
      "414759/414759 [==============================] - 22s 53us/sample - loss: 276.4108 - val_loss: 177.4273 - lr: 0.0017\n",
      "Epoch 467/1000\n",
      "414759/414759 [==============================] - 26s 64us/sample - loss: 275.9970 - val_loss: 177.3929 - lr: 0.0017\n",
      "Epoch 468/1000\n",
      "414759/414759 [==============================] - 16s 40us/sample - loss: 275.8638 - val_loss: 177.3879 - lr: 0.0017\n",
      "Epoch 469/1000\n",
      "414759/414759 [==============================] - 26s 63us/sample - loss: 275.9482 - val_loss: 177.2506 - lr: 0.0017\n",
      "Epoch 470/1000\n",
      "414759/414759 [==============================] - 15s 35us/sample - loss: 275.7333 - val_loss: 177.0623 - lr: 0.0017\n",
      "Epoch 471/1000\n",
      "414759/414759 [==============================] - 14s 34us/sample - loss: 275.4067 - val_loss: 177.3170 - lr: 0.0017\n",
      "Epoch 472/1000\n",
      "414759/414759 [==============================] - 14s 35us/sample - loss: 275.7374 - val_loss: 176.8383 - lr: 0.0017\n",
      "Epoch 473/1000\n",
      "414759/414759 [==============================] - 26s 62us/sample - loss: 275.1733 - val_loss: 177.3281 - lr: 0.0017\n",
      "Epoch 474/1000\n",
      "414759/414759 [==============================] - 22s 52us/sample - loss: 275.6327 - val_loss: 176.9909 - lr: 0.0017\n",
      "Epoch 475/1000\n",
      "414759/414759 [==============================] - 19s 47us/sample - loss: 275.3373 - val_loss: 177.1402 - lr: 0.0017\n",
      "Epoch 476/1000\n",
      "414759/414759 [==============================] - 14s 34us/sample - loss: 275.3367 - val_loss: 176.9468 - lr: 0.0017\n",
      "Epoch 477/1000\n",
      "414759/414759 [==============================] - 24s 59us/sample - loss: 275.1454 - val_loss: 176.9575 - lr: 0.0017\n",
      "Epoch 478/1000\n",
      "414759/414759 [==============================] - 19s 46us/sample - loss: 275.1552 - val_loss: 176.8812 - lr: 0.0017\n",
      "Epoch 479/1000\n",
      "414759/414759 [==============================] - 20s 47us/sample - loss: 274.9980 - val_loss: 176.7840 - lr: 0.0017\n",
      "Epoch 480/1000\n",
      "414759/414759 [==============================] - 20s 49us/sample - loss: 274.8032 - val_loss: 176.8211 - lr: 0.0017\n",
      "Epoch 481/1000\n",
      "414759/414759 [==============================] - 22s 53us/sample - loss: 274.8554 - val_loss: 176.8240 - lr: 0.0017\n",
      "Epoch 482/1000\n",
      "414759/414759 [==============================] - 22s 52us/sample - loss: 274.8311 - val_loss: 176.6920 - lr: 0.0017\n",
      "Epoch 483/1000\n",
      "414759/414759 [==============================] - 14s 34us/sample - loss: 274.5777 - val_loss: 176.6586 - lr: 0.0017\n",
      "Epoch 484/1000\n",
      "414759/414759 [==============================] - 15s 36us/sample - loss: 274.5258 - val_loss: 176.5088 - lr: 0.0017\n",
      "Epoch 485/1000\n",
      "414759/414759 [==============================] - 16s 39us/sample - loss: 274.3725 - val_loss: 176.6080 - lr: 0.0017\n",
      "Epoch 486/1000\n",
      "414759/414759 [==============================] - 16s 38us/sample - loss: 274.4513 - val_loss: 176.4135 - lr: 0.0017\n",
      "Epoch 487/1000\n",
      "414759/414759 [==============================] - 23s 56us/sample - loss: 274.1049 - val_loss: 176.6199 - lr: 0.0017\n",
      "Epoch 488/1000\n",
      "414759/414759 [==============================] - 14s 33us/sample - loss: 274.3009 - val_loss: 176.2362 - lr: 0.0017\n",
      "Epoch 489/1000\n",
      "414759/414759 [==============================] - 14s 34us/sample - loss: 273.9281 - val_loss: 176.6539 - lr: 0.0017\n",
      "Epoch 490/1000\n",
      "414759/414759 [==============================] - 15s 36us/sample - loss: 274.2516 - val_loss: 176.2449 - lr: 0.0017\n",
      "Epoch 491/1000\n",
      "414759/414759 [==============================] - 20s 48us/sample - loss: 273.8051 - val_loss: 176.7794 - lr: 0.0017\n",
      "Epoch 492/1000\n",
      "414759/414759 [==============================] - 15s 36us/sample - loss: 274.3615 - val_loss: 176.1279 - lr: 0.0017\n",
      "Epoch 493/1000\n",
      "414759/414759 [==============================] - 20s 49us/sample - loss: 273.6420 - val_loss: 177.1136 - lr: 0.0017\n",
      "Epoch 494/1000\n",
      "414759/414759 [==============================] - 18s 43us/sample - loss: 274.6367 - val_loss: 176.4334 - lr: 0.0017\n",
      "Epoch 495/1000\n",
      "414759/414759 [==============================] - 20s 49us/sample - loss: 273.8398 - val_loss: 177.0723 - lr: 0.0017\n",
      "Epoch 496/1000\n",
      "414759/414759 [==============================] - 20s 47us/sample - loss: 274.5145 - val_loss: 176.9830 - lr: 0.0017\n",
      "Epoch 497/1000\n",
      "414759/414759 [==============================] - 24s 57us/sample - loss: 274.4153 - val_loss: 176.3211 - lr: 0.0017\n",
      "Epoch 498/1000\n",
      "414759/414759 [==============================] - 20s 48us/sample - loss: 273.6958 - val_loss: 177.0838 - lr: 0.0017\n",
      "Epoch 499/1000\n",
      "414759/414759 [==============================] - 20s 49us/sample - loss: 274.4174 - val_loss: 176.3578 - lr: 0.0017\n",
      "Epoch 500/1000\n",
      "414759/414759 [==============================] - 20s 48us/sample - loss: 273.6090 - val_loss: 176.5362 - lr: 0.0017\n",
      "Epoch 501/1000\n",
      "414759/414759 [==============================] - 20s 47us/sample - loss: 273.7780 - val_loss: 176.7202 - lr: 0.0017\n",
      "Epoch 502/1000\n",
      "414759/414759 [==============================] - 20s 48us/sample - loss: 273.9900 - val_loss: 176.1434 - lr: 0.0017\n",
      "Epoch 503/1000\n",
      "414759/414759 [==============================] - 15s 37us/sample - loss: 273.2694 - val_loss: 176.8529 - lr: 0.0017\n",
      "Epoch 504/1000\n",
      "414759/414759 [==============================] - 22s 54us/sample - loss: 274.0361 - val_loss: 176.3430 - lr: 0.0017\n",
      "Epoch 505/1000\n",
      "414759/414759 [==============================] - 19s 47us/sample - loss: 273.3611 - val_loss: 176.6706 - lr: 0.0017\n",
      "Epoch 506/1000\n",
      "414759/414759 [==============================] - 14s 35us/sample - loss: 273.6845 - val_loss: 176.6725 - lr: 0.0017\n",
      "Epoch 507/1000\n",
      "414759/414759 [==============================] - 21s 51us/sample - loss: 273.7512 - val_loss: 176.1098 - lr: 0.0017\n",
      "Epoch 508/1000\n",
      "414759/414759 [==============================] - 20s 48us/sample - loss: 272.9930 - val_loss: 176.8674 - lr: 0.0017\n",
      "Epoch 509/1000\n",
      "414759/414759 [==============================] - 20s 49us/sample - loss: 273.8478 - val_loss: 176.4206 - lr: 0.0017\n",
      "Epoch 510/1000\n",
      "414759/414759 [==============================] - 14s 34us/sample - loss: 273.3379 - val_loss: 176.3665 - lr: 0.0017\n",
      "Epoch 511/1000\n",
      "414759/414759 [==============================] - 23s 55us/sample - loss: 273.2066 - val_loss: 176.8956 - lr: 0.0017\n",
      "Epoch 512/1000\n",
      "414759/414759 [==============================] - 16s 38us/sample - loss: 273.7361 - val_loss: 176.4634 - lr: 0.0017\n",
      "Epoch 513/1000\n",
      "414759/414759 [==============================] - 20s 48us/sample - loss: 273.2170 - val_loss: 175.7908 - lr: 0.0017\n",
      "Epoch 514/1000\n",
      "414759/414759 [==============================] - 22s 53us/sample - loss: 272.4868 - val_loss: 177.0736 - lr: 0.0017\n",
      "Epoch 515/1000\n",
      "414759/414759 [==============================] - 20s 47us/sample - loss: 273.8179 - val_loss: 175.9930 - lr: 0.0017\n",
      "Epoch 516/1000\n",
      "414759/414759 [==============================] - 20s 49us/sample - loss: 272.5632 - val_loss: 177.6491 - lr: 0.0017\n",
      "Epoch 517/1000\n",
      "414759/414759 [==============================] - 15s 35us/sample - loss: 274.5121 - val_loss: 177.5235 - lr: 0.0017\n",
      "Epoch 518/1000\n",
      "414759/414759 [==============================] - 19s 46us/sample - loss: 274.3545 - val_loss: 176.8386 - lr: 0.0017\n",
      "Epoch 519/1000\n",
      "414759/414759 [==============================] - 15s 36us/sample - loss: 273.4483 - val_loss: 176.9072 - lr: 0.0017\n",
      "Epoch 520/1000\n",
      "414759/414759 [==============================] - 20s 47us/sample - loss: 273.4652 - val_loss: 177.4533 - lr: 0.0017\n",
      "Epoch 521/1000\n",
      "414759/414759 [==============================] - 20s 48us/sample - loss: 274.1517 - val_loss: 176.3540 - lr: 0.0017\n",
      "Epoch 522/1000\n",
      "414759/414759 [==============================] - 14s 34us/sample - loss: 272.8113 - val_loss: 176.8583 - lr: 0.0017\n",
      "Epoch 523/1000\n",
      "414759/414759 [==============================] - 21s 50us/sample - loss: 273.3290 - val_loss: 177.3491 - lr: 0.0017\n",
      "Epoch 524/1000\n",
      "414759/414759 [==============================] - 15s 37us/sample - loss: 273.8900 - val_loss: 176.0783 - lr: 0.0017\n",
      "Epoch 525/1000\n",
      "414759/414759 [==============================] - 23s 56us/sample - loss: 272.4692 - val_loss: 176.9261 - lr: 0.0017\n",
      "Epoch 526/1000\n",
      "414759/414759 [==============================] - 14s 34us/sample - loss: 273.2866 - val_loss: 177.0102 - lr: 0.0017\n",
      "Epoch 527/1000\n",
      "414759/414759 [==============================] - 21s 52us/sample - loss: 273.4488 - val_loss: 176.8351 - lr: 0.0017\n",
      "Epoch 528/1000\n",
      "414759/414759 [==============================] - 14s 34us/sample - loss: 273.1468 - val_loss: 175.7918 - lr: 0.0017\n",
      "Epoch 529/1000\n",
      "414759/414759 [==============================] - 20s 49us/sample - loss: 271.9048 - val_loss: 176.2376 - lr: 0.0013\n",
      "Epoch 530/1000\n",
      "414759/414759 [==============================] - 14s 34us/sample - loss: 272.4175 - val_loss: 175.8581 - lr: 0.0013\n",
      "Epoch 531/1000\n",
      "414759/414759 [==============================] - 19s 45us/sample - loss: 271.9531 - val_loss: 175.5432 - lr: 0.0013\n",
      "Epoch 532/1000\n",
      "414759/414759 [==============================] - 20s 48us/sample - loss: 271.5410 - val_loss: 175.8086 - lr: 0.0013\n",
      "Epoch 533/1000\n",
      "414759/414759 [==============================] - 20s 49us/sample - loss: 271.9248 - val_loss: 175.6380 - lr: 0.0013\n",
      "Epoch 534/1000\n",
      "414759/414759 [==============================] - 21s 50us/sample - loss: 271.6700 - val_loss: 175.4987 - lr: 0.0013\n",
      "Epoch 535/1000\n",
      "414759/414759 [==============================] - 20s 49us/sample - loss: 271.4046 - val_loss: 175.4125 - lr: 0.0013\n",
      "Epoch 536/1000\n",
      "414759/414759 [==============================] - 19s 45us/sample - loss: 271.2665 - val_loss: 175.5495 - lr: 0.0013\n",
      "Epoch 537/1000\n",
      "414759/414759 [==============================] - 20s 47us/sample - loss: 271.4315 - val_loss: 175.3124 - lr: 0.0013\n",
      "Epoch 538/1000\n",
      "414759/414759 [==============================] - 19s 45us/sample - loss: 271.1141 - val_loss: 175.2532 - lr: 0.0013\n",
      "Epoch 539/1000\n",
      "414759/414759 [==============================] - 20s 47us/sample - loss: 271.0319 - val_loss: 175.4820 - lr: 0.0013\n",
      "Epoch 540/1000\n",
      "414759/414759 [==============================] - 21s 50us/sample - loss: 271.2667 - val_loss: 175.0788 - lr: 0.0013\n",
      "Epoch 541/1000\n",
      "414759/414759 [==============================] - 20s 49us/sample - loss: 270.7598 - val_loss: 175.4777 - lr: 0.0013\n",
      "Epoch 542/1000\n",
      "414759/414759 [==============================] - 15s 36us/sample - loss: 271.2164 - val_loss: 175.1520 - lr: 0.0013\n",
      "Epoch 543/1000\n",
      "414759/414759 [==============================] - 22s 53us/sample - loss: 270.8616 - val_loss: 175.4680 - lr: 0.0013\n",
      "Epoch 544/1000\n",
      "414759/414759 [==============================] - 20s 47us/sample - loss: 271.0693 - val_loss: 175.1955 - lr: 0.0013\n",
      "Epoch 545/1000\n",
      "414759/414759 [==============================] - 14s 34us/sample - loss: 270.7881 - val_loss: 175.3147 - lr: 0.0013\n",
      "Epoch 546/1000\n",
      "414759/414759 [==============================] - 23s 56us/sample - loss: 270.8760 - val_loss: 175.2151 - lr: 0.0013\n",
      "Epoch 547/1000\n",
      "414759/414759 [==============================] - 24s 58us/sample - loss: 270.7719 - val_loss: 175.0251 - lr: 0.0013\n",
      "Epoch 548/1000\n",
      "414759/414759 [==============================] - 14s 35us/sample - loss: 270.5214 - val_loss: 175.1715 - lr: 0.0013\n",
      "Epoch 549/1000\n",
      "414759/414759 [==============================] - 18s 43us/sample - loss: 270.6480 - val_loss: 175.0148 - lr: 0.0013\n",
      "Epoch 550/1000\n",
      "414759/414759 [==============================] - 19s 46us/sample - loss: 270.3846 - val_loss: 175.0046 - lr: 0.0013\n",
      "Epoch 551/1000\n",
      "414759/414759 [==============================] - 20s 49us/sample - loss: 270.3474 - val_loss: 175.0661 - lr: 0.0013\n",
      "Epoch 552/1000\n",
      "414759/414759 [==============================] - 14s 34us/sample - loss: 270.4135 - val_loss: 174.8724 - lr: 0.0013\n",
      "Epoch 553/1000\n",
      "414759/414759 [==============================] - 26s 62us/sample - loss: 270.0444 - val_loss: 175.0480 - lr: 0.0013\n",
      "Epoch 554/1000\n",
      "414759/414759 [==============================] - 15s 37us/sample - loss: 270.3612 - val_loss: 174.7424 - lr: 0.0013\n",
      "Epoch 555/1000\n",
      "414759/414759 [==============================] - 15s 36us/sample - loss: 269.9037 - val_loss: 175.1195 - lr: 0.0013\n",
      "Epoch 556/1000\n",
      "414759/414759 [==============================] - 15s 35us/sample - loss: 270.3114 - val_loss: 174.9061 - lr: 0.0013\n",
      "Epoch 557/1000\n",
      "414759/414759 [==============================] - 19s 46us/sample - loss: 270.0413 - val_loss: 174.8918 - lr: 0.0013\n",
      "Epoch 558/1000\n",
      "414759/414759 [==============================] - 14s 34us/sample - loss: 269.9760 - val_loss: 174.9600 - lr: 0.0013\n",
      "Epoch 559/1000\n",
      "414759/414759 [==============================] - 19s 45us/sample - loss: 270.0200 - val_loss: 174.8466 - lr: 0.0013\n",
      "Epoch 560/1000\n",
      "414759/414759 [==============================] - 14s 35us/sample - loss: 269.7819 - val_loss: 175.0058 - lr: 0.0013\n",
      "Epoch 561/1000\n",
      "414759/414759 [==============================] - 22s 52us/sample - loss: 270.0072 - val_loss: 174.7190 - lr: 0.0013\n",
      "Epoch 562/1000\n",
      "414759/414759 [==============================] - 16s 40us/sample - loss: 269.6015 - val_loss: 174.8480 - lr: 0.0013\n",
      "Epoch 563/1000\n",
      "414759/414759 [==============================] - 16s 39us/sample - loss: 269.7549 - val_loss: 174.9310 - lr: 0.0013\n",
      "Epoch 564/1000\n",
      "414759/414759 [==============================] - 22s 54us/sample - loss: 269.7806 - val_loss: 174.5868 - lr: 0.0013\n",
      "Epoch 565/1000\n",
      "414759/414759 [==============================] - 15s 35us/sample - loss: 269.3242 - val_loss: 175.1324 - lr: 0.0013\n",
      "Epoch 566/1000\n",
      "414759/414759 [==============================] - 24s 57us/sample - loss: 269.9010 - val_loss: 174.6411 - lr: 0.0013\n",
      "Epoch 567/1000\n",
      "414759/414759 [==============================] - 14s 34us/sample - loss: 269.3505 - val_loss: 175.1140 - lr: 0.0013\n",
      "Epoch 568/1000\n",
      "414759/414759 [==============================] - 20s 47us/sample - loss: 269.8543 - val_loss: 174.9831 - lr: 0.0013\n",
      "Epoch 569/1000\n",
      "414759/414759 [==============================] - 14s 34us/sample - loss: 269.7950 - val_loss: 174.9109 - lr: 0.0013\n",
      "Epoch 570/1000\n",
      "414759/414759 [==============================] - 22s 53us/sample - loss: 269.4802 - val_loss: 174.9427 - lr: 0.0013\n",
      "Epoch 571/1000\n",
      "414759/414759 [==============================] - 16s 39us/sample - loss: 269.5557 - val_loss: 174.8568 - lr: 0.0013\n",
      "Epoch 572/1000\n",
      "414759/414759 [==============================] - 21s 51us/sample - loss: 269.4203 - val_loss: 174.5705 - lr: 0.0013\n",
      "Epoch 573/1000\n",
      "414759/414759 [==============================] - 20s 48us/sample - loss: 269.0771 - val_loss: 174.9849 - lr: 0.0013\n",
      "Epoch 574/1000\n",
      "414759/414759 [==============================] - 18s 44us/sample - loss: 269.4478 - val_loss: 174.3283 - lr: 0.0013\n",
      "Epoch 575/1000\n",
      "414759/414759 [==============================] - 21s 50us/sample - loss: 268.7154 - val_loss: 175.4003 - lr: 0.0013\n",
      "Epoch 576/1000\n",
      "414759/414759 [==============================] - 24s 57us/sample - loss: 269.9588 - val_loss: 175.1523 - lr: 0.0013\n",
      "Epoch 577/1000\n",
      "414759/414759 [==============================] - 16s 38us/sample - loss: 269.6542 - val_loss: 174.9274 - lr: 0.0013\n",
      "Epoch 578/1000\n",
      "414759/414759 [==============================] - 21s 51us/sample - loss: 269.2936 - val_loss: 175.2313 - lr: 0.0013\n",
      "Epoch 579/1000\n",
      "414759/414759 [==============================] - 15s 36us/sample - loss: 269.6587 - val_loss: 175.0114 - lr: 0.0013\n",
      "Epoch 580/1000\n",
      "414759/414759 [==============================] - 21s 51us/sample - loss: 269.3396 - val_loss: 174.5938 - lr: 0.0013\n",
      "Epoch 581/1000\n",
      "414759/414759 [==============================] - 14s 34us/sample - loss: 268.8302 - val_loss: 175.2847 - lr: 0.0013\n",
      "Epoch 582/1000\n",
      "414759/414759 [==============================] - 21s 52us/sample - loss: 269.5906 - val_loss: 174.6741 - lr: 0.0013\n",
      "Epoch 583/1000\n",
      "414759/414759 [==============================] - 15s 35us/sample - loss: 268.8975 - val_loss: 174.9295 - lr: 0.0013\n",
      "Epoch 584/1000\n",
      "414759/414759 [==============================] - 20s 49us/sample - loss: 269.0933 - val_loss: 175.3740 - lr: 0.0013\n",
      "Epoch 585/1000\n",
      "414759/414759 [==============================] - 20s 48us/sample - loss: 269.5831 - val_loss: 174.6962 - lr: 0.0013\n",
      "Epoch 586/1000\n",
      "414759/414759 [==============================] - 20s 48us/sample - loss: 268.7592 - val_loss: 174.7878 - lr: 0.0013\n",
      "Epoch 587/1000\n",
      "414759/414759 [==============================] - 20s 48us/sample - loss: 268.8435 - val_loss: 175.0850 - lr: 0.0013\n",
      "Epoch 588/1000\n",
      "414759/414759 [==============================] - 20s 47us/sample - loss: 269.1652 - val_loss: 174.5782 - lr: 0.0013\n",
      "Epoch 589/1000\n",
      "414759/414759 [==============================] - 26s 63us/sample - loss: 268.5147 - val_loss: 174.4796 - lr: 0.0013\n",
      "Epoch 590/1000\n",
      "414759/414759 [==============================] - 15s 37us/sample - loss: 268.4255 - val_loss: 174.5200 - lr: 9.4922e-04\n",
      "Epoch 591/1000\n",
      "414759/414759 [==============================] - 26s 62us/sample - loss: 268.4799 - val_loss: 174.1927 - lr: 9.4922e-04\n",
      "Epoch 592/1000\n",
      "414759/414759 [==============================] - 21s 52us/sample - loss: 268.0073 - val_loss: 174.4394 - lr: 9.4922e-04\n",
      "Epoch 593/1000\n",
      "414759/414759 [==============================] - 14s 34us/sample - loss: 268.2314 - val_loss: 174.3666 - lr: 9.4922e-04\n",
      "Epoch 594/1000\n",
      "414759/414759 [==============================] - 21s 51us/sample - loss: 268.2564 - val_loss: 174.1756 - lr: 9.4922e-04\n",
      "Epoch 595/1000\n",
      "414759/414759 [==============================] - 17s 41us/sample - loss: 267.9303 - val_loss: 174.2654 - lr: 9.4922e-04\n",
      "Epoch 596/1000\n",
      "414759/414759 [==============================] - 22s 54us/sample - loss: 267.9869 - val_loss: 174.2744 - lr: 9.4922e-04\n",
      "Epoch 597/1000\n",
      "414759/414759 [==============================] - 19s 47us/sample - loss: 268.0050 - val_loss: 174.0819 - lr: 9.4922e-04\n",
      "Epoch 598/1000\n",
      "414759/414759 [==============================] - 19s 46us/sample - loss: 267.7010 - val_loss: 174.2209 - lr: 9.4922e-04\n",
      "Epoch 599/1000\n",
      "414759/414759 [==============================] - 21s 51us/sample - loss: 267.9174 - val_loss: 174.1035 - lr: 9.4922e-04\n",
      "Epoch 600/1000\n",
      "414759/414759 [==============================] - 23s 56us/sample - loss: 267.7109 - val_loss: 174.0756 - lr: 9.4922e-04\n",
      "Epoch 601/1000\n",
      "414759/414759 [==============================] - 19s 45us/sample - loss: 267.6161 - val_loss: 174.0474 - lr: 9.4922e-04\n",
      "Epoch 602/1000\n",
      "414759/414759 [==============================] - 25s 61us/sample - loss: 267.5732 - val_loss: 174.1600 - lr: 9.4922e-04\n",
      "Epoch 603/1000\n",
      "414759/414759 [==============================] - 22s 53us/sample - loss: 267.6609 - val_loss: 173.7996 - lr: 9.4922e-04\n",
      "Epoch 604/1000\n",
      "414759/414759 [==============================] - 21s 50us/sample - loss: 267.2458 - val_loss: 174.4185 - lr: 9.4922e-04\n",
      "Epoch 605/1000\n",
      "414759/414759 [==============================] - 20s 48us/sample - loss: 267.8110 - val_loss: 174.0842 - lr: 9.4922e-04\n",
      "Epoch 606/1000\n",
      "414759/414759 [==============================] - 20s 49us/sample - loss: 267.5034 - val_loss: 174.5061 - lr: 9.4922e-04\n",
      "Epoch 607/1000\n",
      "414759/414759 [==============================] - 20s 49us/sample - loss: 267.9849 - val_loss: 174.5573 - lr: 9.4922e-04\n",
      "Epoch 608/1000\n",
      "414759/414759 [==============================] - 15s 37us/sample - loss: 268.0074 - val_loss: 174.1485 - lr: 9.4922e-04\n",
      "Epoch 609/1000\n",
      "414759/414759 [==============================] - 15s 36us/sample - loss: 267.4823 - val_loss: 174.3232 - lr: 9.4922e-04\n",
      "Epoch 610/1000\n",
      "414759/414759 [==============================] - 20s 49us/sample - loss: 267.6307 - val_loss: 174.0707 - lr: 9.4922e-04\n",
      "Epoch 611/1000\n",
      "414759/414759 [==============================] - 16s 39us/sample - loss: 267.3291 - val_loss: 174.4050 - lr: 9.4922e-04\n",
      "Epoch 612/1000\n",
      "414759/414759 [==============================] - 23s 57us/sample - loss: 267.6342 - val_loss: 173.9114 - lr: 9.4922e-04\n",
      "Epoch 613/1000\n",
      "414759/414759 [==============================] - 16s 38us/sample - loss: 267.1505 - val_loss: 174.4251 - lr: 9.4922e-04\n",
      "Epoch 614/1000\n",
      "414759/414759 [==============================] - 22s 54us/sample - loss: 267.6288 - val_loss: 174.3667 - lr: 9.4922e-04\n",
      "Epoch 615/1000\n",
      "414759/414759 [==============================] - 15s 36us/sample - loss: 267.5974 - val_loss: 173.9021 - lr: 9.4922e-04\n",
      "Epoch 616/1000\n",
      "414759/414759 [==============================] - 15s 35us/sample - loss: 267.0334 - val_loss: 174.4331 - lr: 9.4922e-04\n",
      "Epoch 617/1000\n",
      "414759/414759 [==============================] - 20s 49us/sample - loss: 267.5629 - val_loss: 174.0622 - lr: 9.4922e-04\n",
      "Epoch 618/1000\n",
      "414759/414759 [==============================] - 20s 48us/sample - loss: 267.1233 - val_loss: 174.2126 - lr: 9.4922e-04\n",
      "Epoch 619/1000\n",
      "414759/414759 [==============================] - 16s 40us/sample - loss: 267.2744 - val_loss: 173.9864 - lr: 7.1191e-04\n",
      "Epoch 620/1000\n",
      "414759/414759 [==============================] - 20s 49us/sample - loss: 267.0269 - val_loss: 173.9330 - lr: 7.1191e-04\n",
      "Epoch 621/1000\n",
      "414759/414759 [==============================] - 21s 52us/sample - loss: 266.8843 - val_loss: 174.0820 - lr: 7.1191e-04\n",
      "Epoch 622/1000\n",
      "414759/414759 [==============================] - 15s 37us/sample - loss: 267.0185 - val_loss: 173.7554 - lr: 7.1191e-04\n",
      "Epoch 623/1000\n",
      "414759/414759 [==============================] - 20s 48us/sample - loss: 266.6800 - val_loss: 173.8440 - lr: 7.1191e-04\n",
      "Epoch 624/1000\n",
      "414759/414759 [==============================] - 14s 35us/sample - loss: 266.7425 - val_loss: 173.8715 - lr: 7.1191e-04\n",
      "Epoch 625/1000\n",
      "414759/414759 [==============================] - 22s 52us/sample - loss: 266.7663 - val_loss: 173.6421 - lr: 7.1191e-04\n",
      "Epoch 626/1000\n",
      "414759/414759 [==============================] - 15s 37us/sample - loss: 266.4378 - val_loss: 173.7806 - lr: 7.1191e-04\n",
      "Epoch 627/1000\n",
      "414759/414759 [==============================] - 15s 36us/sample - loss: 266.6164 - val_loss: 173.7002 - lr: 7.1191e-04\n",
      "Epoch 628/1000\n",
      "414759/414759 [==============================] - 20s 49us/sample - loss: 266.4751 - val_loss: 173.6857 - lr: 7.1191e-04\n",
      "Epoch 629/1000\n",
      "414759/414759 [==============================] - 15s 37us/sample - loss: 266.3768 - val_loss: 173.7004 - lr: 7.1191e-04\n",
      "Epoch 630/1000\n",
      "414759/414759 [==============================] - 21s 50us/sample - loss: 266.4223 - val_loss: 173.5842 - lr: 7.1191e-04\n",
      "Epoch 631/1000\n",
      "414759/414759 [==============================] - 15s 37us/sample - loss: 266.2462 - val_loss: 173.7576 - lr: 7.1191e-04\n",
      "Epoch 632/1000\n",
      "414759/414759 [==============================] - 16s 38us/sample - loss: 266.4430 - val_loss: 173.5832 - lr: 7.1191e-04\n",
      "Epoch 633/1000\n",
      "414759/414759 [==============================] - 16s 40us/sample - loss: 266.1891 - val_loss: 173.6634 - lr: 7.1191e-04\n",
      "Epoch 634/1000\n",
      "414759/414759 [==============================] - 15s 37us/sample - loss: 266.3020 - val_loss: 173.5988 - lr: 7.1191e-04\n",
      "Epoch 635/1000\n",
      "414759/414759 [==============================] - 20s 49us/sample - loss: 266.1786 - val_loss: 173.6625 - lr: 7.1191e-04\n",
      "Epoch 636/1000\n",
      "414759/414759 [==============================] - 15s 35us/sample - loss: 266.2204 - val_loss: 173.6124 - lr: 7.1191e-04\n",
      "Epoch 637/1000\n",
      "414759/414759 [==============================] - 21s 50us/sample - loss: 266.1706 - val_loss: 173.5854 - lr: 7.1191e-04\n",
      "Epoch 638/1000\n",
      "414759/414759 [==============================] - 15s 35us/sample - loss: 266.0607 - val_loss: 173.6367 - lr: 7.1191e-04\n",
      "Epoch 639/1000\n",
      "414759/414759 [==============================] - 20s 48us/sample - loss: 266.0693 - val_loss: 173.5750 - lr: 7.1191e-04\n",
      "Epoch 640/1000\n",
      "414759/414759 [==============================] - 15s 37us/sample - loss: 266.0038 - val_loss: 173.4435 - lr: 7.1191e-04\n",
      "Epoch 641/1000\n",
      "414759/414759 [==============================] - 14s 34us/sample - loss: 265.8294 - val_loss: 173.5885 - lr: 7.1191e-04\n",
      "Epoch 642/1000\n",
      "414759/414759 [==============================] - 20s 47us/sample - loss: 265.9241 - val_loss: 173.3772 - lr: 7.1191e-04\n",
      "Epoch 643/1000\n",
      "414759/414759 [==============================] - 21s 50us/sample - loss: 265.7206 - val_loss: 173.6358 - lr: 7.1191e-04\n",
      "Epoch 644/1000\n",
      "414759/414759 [==============================] - 20s 48us/sample - loss: 265.9906 - val_loss: 173.5308 - lr: 7.1191e-04\n",
      "Epoch 645/1000\n",
      "414759/414759 [==============================] - 20s 48us/sample - loss: 265.8358 - val_loss: 173.5375 - lr: 7.1191e-04\n",
      "Epoch 646/1000\n",
      "414759/414759 [==============================] - 15s 36us/sample - loss: 265.8262 - val_loss: 173.5357 - lr: 7.1191e-04\n",
      "Epoch 647/1000\n",
      "414759/414759 [==============================] - 20s 48us/sample - loss: 265.7513 - val_loss: 173.4813 - lr: 7.1191e-04\n",
      "Epoch 648/1000\n",
      "414759/414759 [==============================] - 15s 36us/sample - loss: 265.6582 - val_loss: 173.4642 - lr: 7.1191e-04\n",
      "Epoch 649/1000\n",
      "414759/414759 [==============================] - 20s 48us/sample - loss: 265.6428 - val_loss: 173.4176 - lr: 7.1191e-04\n",
      "Epoch 650/1000\n",
      "414759/414759 [==============================] - 22s 52us/sample - loss: 265.5188 - val_loss: 173.3615 - lr: 7.1191e-04\n",
      "Epoch 651/1000\n",
      "414759/414759 [==============================] - 22s 53us/sample - loss: 265.5050 - val_loss: 173.3579 - lr: 7.1191e-04\n",
      "Epoch 652/1000\n",
      "414759/414759 [==============================] - 21s 51us/sample - loss: 265.4427 - val_loss: 173.3604 - lr: 7.1191e-04\n",
      "Epoch 653/1000\n",
      "414759/414759 [==============================] - 21s 51us/sample - loss: 265.3932 - val_loss: 173.3103 - lr: 7.1191e-04\n",
      "Epoch 654/1000\n",
      "414759/414759 [==============================] - 20s 49us/sample - loss: 265.2705 - val_loss: 173.3186 - lr: 7.1191e-04\n",
      "Epoch 655/1000\n",
      "414759/414759 [==============================] - 20s 48us/sample - loss: 265.3167 - val_loss: 173.3008 - lr: 7.1191e-04\n",
      "Epoch 656/1000\n",
      "414759/414759 [==============================] - 21s 50us/sample - loss: 265.1878 - val_loss: 173.3223 - lr: 7.1191e-04\n",
      "Epoch 657/1000\n",
      "414759/414759 [==============================] - 23s 57us/sample - loss: 265.2407 - val_loss: 173.2251 - lr: 7.1191e-04\n",
      "Epoch 658/1000\n",
      "414759/414759 [==============================] - 15s 36us/sample - loss: 265.1116 - val_loss: 173.2279 - lr: 7.1191e-04\n",
      "Epoch 659/1000\n",
      "414759/414759 [==============================] - 15s 36us/sample - loss: 265.0563 - val_loss: 173.2740 - lr: 7.1191e-04\n",
      "Epoch 660/1000\n",
      "414759/414759 [==============================] - 20s 49us/sample - loss: 265.0607 - val_loss: 173.1484 - lr: 7.1191e-04\n",
      "Epoch 661/1000\n",
      "414759/414759 [==============================] - 14s 35us/sample - loss: 264.9087 - val_loss: 173.1900 - lr: 7.1191e-04\n",
      "Epoch 662/1000\n",
      "414759/414759 [==============================] - 26s 63us/sample - loss: 264.9298 - val_loss: 173.1658 - lr: 7.1191e-04\n",
      "Epoch 663/1000\n",
      "414759/414759 [==============================] - 16s 38us/sample - loss: 264.9236 - val_loss: 173.1191 - lr: 7.1191e-04\n",
      "Epoch 664/1000\n",
      "414759/414759 [==============================] - 21s 50us/sample - loss: 264.8315 - val_loss: 173.1772 - lr: 7.1191e-04\n",
      "Epoch 665/1000\n",
      "414759/414759 [==============================] - 22s 54us/sample - loss: 264.7998 - val_loss: 173.1336 - lr: 7.1191e-04\n",
      "Epoch 666/1000\n",
      "414759/414759 [==============================] - 20s 47us/sample - loss: 264.7922 - val_loss: 173.1465 - lr: 7.1191e-04\n",
      "Epoch 667/1000\n",
      "414759/414759 [==============================] - 21s 51us/sample - loss: 264.6898 - val_loss: 173.1109 - lr: 7.1191e-04\n",
      "Epoch 668/1000\n",
      "414759/414759 [==============================] - 21s 51us/sample - loss: 264.6977 - val_loss: 173.0908 - lr: 7.1191e-04\n",
      "Epoch 669/1000\n",
      "414759/414759 [==============================] - 23s 55us/sample - loss: 264.6236 - val_loss: 173.1375 - lr: 7.1191e-04\n",
      "Epoch 670/1000\n",
      "414759/414759 [==============================] - 26s 62us/sample - loss: 264.6084 - val_loss: 173.0794 - lr: 7.1191e-04\n",
      "Epoch 671/1000\n",
      "414759/414759 [==============================] - 23s 56us/sample - loss: 264.5104 - val_loss: 173.0861 - lr: 7.1191e-04\n",
      "Epoch 672/1000\n",
      "414759/414759 [==============================] - 20s 48us/sample - loss: 264.5535 - val_loss: 173.0715 - lr: 7.1191e-04\n",
      "Epoch 673/1000\n",
      "414759/414759 [==============================] - 21s 50us/sample - loss: 264.4183 - val_loss: 173.0300 - lr: 7.1191e-04\n",
      "Epoch 674/1000\n",
      "414759/414759 [==============================] - 15s 37us/sample - loss: 264.3734 - val_loss: 173.0886 - lr: 7.1191e-04\n",
      "Epoch 675/1000\n",
      "414759/414759 [==============================] - 15s 36us/sample - loss: 264.4099 - val_loss: 172.9556 - lr: 7.1191e-04\n",
      "Epoch 676/1000\n",
      "414759/414759 [==============================] - 21s 50us/sample - loss: 264.2931 - val_loss: 173.0846 - lr: 7.1191e-04\n",
      "Epoch 677/1000\n",
      "414759/414759 [==============================] - 20s 49us/sample - loss: 264.3821 - val_loss: 173.0018 - lr: 7.1191e-04\n",
      "Epoch 678/1000\n",
      "414759/414759 [==============================] - 21s 50us/sample - loss: 264.2559 - val_loss: 173.0256 - lr: 7.1191e-04\n",
      "Epoch 679/1000\n",
      "414759/414759 [==============================] - 19s 46us/sample - loss: 264.2502 - val_loss: 173.0511 - lr: 7.1191e-04\n",
      "Epoch 680/1000\n",
      "414759/414759 [==============================] - 14s 34us/sample - loss: 264.1841 - val_loss: 172.9815 - lr: 7.1191e-04\n",
      "Epoch 681/1000\n",
      "414759/414759 [==============================] - 21s 50us/sample - loss: 264.0920 - val_loss: 173.0664 - lr: 7.1191e-04\n",
      "Epoch 682/1000\n",
      "414759/414759 [==============================] - 15s 37us/sample - loss: 264.1394 - val_loss: 172.8900 - lr: 7.1191e-04\n",
      "Epoch 683/1000\n",
      "414759/414759 [==============================] - 25s 60us/sample - loss: 264.0048 - val_loss: 173.0643 - lr: 7.1191e-04\n",
      "Epoch 684/1000\n",
      "414759/414759 [==============================] - 14s 33us/sample - loss: 264.0518 - val_loss: 172.9329 - lr: 7.1191e-04\n",
      "Epoch 685/1000\n",
      "414759/414759 [==============================] - 23s 56us/sample - loss: 263.9509 - val_loss: 172.9843 - lr: 7.1191e-04\n",
      "Epoch 686/1000\n",
      "414759/414759 [==============================] - 15s 37us/sample - loss: 263.9402 - val_loss: 172.9651 - lr: 7.1191e-04\n",
      "Epoch 687/1000\n",
      "414759/414759 [==============================] - 20s 49us/sample - loss: 263.9058 - val_loss: 172.9552 - lr: 7.1191e-04\n",
      "Epoch 688/1000\n",
      "414759/414759 [==============================] - 15s 36us/sample - loss: 263.8236 - val_loss: 172.9938 - lr: 7.1191e-04\n",
      "Epoch 689/1000\n",
      "414759/414759 [==============================] - 14s 35us/sample - loss: 263.8580 - val_loss: 172.8871 - lr: 7.1191e-04\n",
      "Epoch 690/1000\n",
      "414759/414759 [==============================] - 21s 52us/sample - loss: 263.7433 - val_loss: 172.9422 - lr: 7.1191e-04\n",
      "Epoch 691/1000\n",
      "414759/414759 [==============================] - 21s 50us/sample - loss: 263.7508 - val_loss: 172.9365 - lr: 7.1191e-04\n",
      "Epoch 692/1000\n",
      "414759/414759 [==============================] - 15s 35us/sample - loss: 263.6978 - val_loss: 172.8838 - lr: 7.1191e-04\n",
      "Epoch 693/1000\n",
      "414759/414759 [==============================] - 15s 37us/sample - loss: 263.6471 - val_loss: 172.9002 - lr: 7.1191e-04\n",
      "Epoch 694/1000\n",
      "414759/414759 [==============================] - 15s 36us/sample - loss: 263.6078 - val_loss: 172.8603 - lr: 7.1191e-04\n",
      "Epoch 695/1000\n",
      "414759/414759 [==============================] - 21s 50us/sample - loss: 263.6128 - val_loss: 172.8209 - lr: 7.1191e-04\n",
      "Epoch 696/1000\n",
      "414759/414759 [==============================] - 15s 35us/sample - loss: 263.4862 - val_loss: 172.9333 - lr: 7.1191e-04\n",
      "Epoch 697/1000\n",
      "414759/414759 [==============================] - 20s 48us/sample - loss: 263.5158 - val_loss: 172.8067 - lr: 7.1191e-04\n",
      "Epoch 698/1000\n",
      "414759/414759 [==============================] - 21s 51us/sample - loss: 263.3918 - val_loss: 172.8778 - lr: 7.1191e-04\n",
      "Epoch 699/1000\n",
      "414759/414759 [==============================] - 23s 56us/sample - loss: 263.4583 - val_loss: 172.7746 - lr: 7.1191e-04\n",
      "Epoch 700/1000\n",
      "414759/414759 [==============================] - 21s 51us/sample - loss: 263.3424 - val_loss: 172.8377 - lr: 7.1191e-04\n",
      "Epoch 701/1000\n",
      "414759/414759 [==============================] - 20s 49us/sample - loss: 263.3813 - val_loss: 172.7770 - lr: 7.1191e-04\n",
      "Epoch 702/1000\n",
      "414759/414759 [==============================] - 21s 50us/sample - loss: 263.2721 - val_loss: 172.7763 - lr: 7.1191e-04\n",
      "Epoch 703/1000\n",
      "414759/414759 [==============================] - 15s 36us/sample - loss: 263.2168 - val_loss: 172.7633 - lr: 7.1191e-04\n",
      "Epoch 704/1000\n",
      "414759/414759 [==============================] - 15s 36us/sample - loss: 263.2111 - val_loss: 172.7441 - lr: 7.1191e-04\n",
      "Epoch 705/1000\n",
      "414759/414759 [==============================] - 15s 36us/sample - loss: 263.1456 - val_loss: 172.7499 - lr: 7.1191e-04\n",
      "Epoch 706/1000\n",
      "414759/414759 [==============================] - 16s 40us/sample - loss: 263.1294 - val_loss: 172.7007 - lr: 7.1191e-04\n",
      "Epoch 707/1000\n",
      "414759/414759 [==============================] - 15s 36us/sample - loss: 263.0339 - val_loss: 172.7448 - lr: 7.1191e-04\n",
      "Epoch 708/1000\n",
      "414759/414759 [==============================] - 15s 35us/sample - loss: 263.0155 - val_loss: 172.7521 - lr: 7.1191e-04\n",
      "Epoch 709/1000\n",
      "414759/414759 [==============================] - 20s 49us/sample - loss: 262.9808 - val_loss: 172.7277 - lr: 7.1191e-04\n",
      "Epoch 710/1000\n",
      "414759/414759 [==============================] - 21s 50us/sample - loss: 262.9071 - val_loss: 172.7564 - lr: 7.1191e-04\n",
      "Epoch 711/1000\n",
      "414759/414759 [==============================] - 15s 36us/sample - loss: 262.9339 - val_loss: 172.6883 - lr: 7.1191e-04\n",
      "Epoch 712/1000\n",
      "414759/414759 [==============================] - 16s 38us/sample - loss: 262.8304 - val_loss: 172.7536 - lr: 7.1191e-04\n",
      "Epoch 713/1000\n",
      "414759/414759 [==============================] - 15s 35us/sample - loss: 262.9102 - val_loss: 172.6848 - lr: 7.1191e-04\n",
      "Epoch 714/1000\n",
      "414759/414759 [==============================] - 25s 60us/sample - loss: 262.7488 - val_loss: 172.7154 - lr: 7.1191e-04\n",
      "Epoch 715/1000\n",
      "414759/414759 [==============================] - 15s 36us/sample - loss: 262.7489 - val_loss: 172.6549 - lr: 7.1191e-04\n",
      "Epoch 716/1000\n",
      "414759/414759 [==============================] - 15s 36us/sample - loss: 262.7095 - val_loss: 172.6308 - lr: 7.1191e-04\n",
      "Epoch 717/1000\n",
      "414759/414759 [==============================] - 15s 37us/sample - loss: 262.6807 - val_loss: 172.6519 - lr: 7.1191e-04\n",
      "Epoch 718/1000\n",
      "414759/414759 [==============================] - 14s 35us/sample - loss: 262.6498 - val_loss: 172.6343 - lr: 7.1191e-04\n",
      "Epoch 719/1000\n",
      "414759/414759 [==============================] - 23s 55us/sample - loss: 262.5971 - val_loss: 172.5904 - lr: 7.1191e-04\n",
      "Epoch 720/1000\n",
      "414759/414759 [==============================] - 14s 34us/sample - loss: 262.5198 - val_loss: 172.6470 - lr: 7.1191e-04\n",
      "Epoch 721/1000\n",
      "414759/414759 [==============================] - 21s 51us/sample - loss: 262.5429 - val_loss: 172.5834 - lr: 7.1191e-04\n",
      "Epoch 722/1000\n",
      "414759/414759 [==============================] - 20s 49us/sample - loss: 262.4292 - val_loss: 172.6283 - lr: 7.1191e-04\n",
      "Epoch 723/1000\n",
      "414759/414759 [==============================] - 18s 44us/sample - loss: 262.5047 - val_loss: 172.5473 - lr: 7.1191e-04\n",
      "Epoch 724/1000\n",
      "414759/414759 [==============================] - 15s 37us/sample - loss: 262.3739 - val_loss: 172.6360 - lr: 7.1191e-04\n",
      "Epoch 725/1000\n",
      "414759/414759 [==============================] - 14s 34us/sample - loss: 262.4167 - val_loss: 172.5612 - lr: 7.1191e-04\n",
      "Epoch 726/1000\n",
      "414759/414759 [==============================] - 21s 51us/sample - loss: 262.2938 - val_loss: 172.6222 - lr: 7.1191e-04\n",
      "Epoch 727/1000\n",
      "414759/414759 [==============================] - 15s 37us/sample - loss: 262.3145 - val_loss: 172.5566 - lr: 7.1191e-04\n",
      "Epoch 728/1000\n",
      "414759/414759 [==============================] - 20s 49us/sample - loss: 262.2365 - val_loss: 172.5933 - lr: 7.1191e-04\n",
      "Epoch 729/1000\n",
      "414759/414759 [==============================] - 22s 52us/sample - loss: 262.2900 - val_loss: 172.5184 - lr: 7.1191e-04\n",
      "Epoch 730/1000\n",
      "414759/414759 [==============================] - 23s 56us/sample - loss: 262.1618 - val_loss: 172.5626 - lr: 7.1191e-04\n",
      "Epoch 731/1000\n",
      "414759/414759 [==============================] - 20s 47us/sample - loss: 262.2151 - val_loss: 172.5034 - lr: 7.1191e-04\n",
      "Epoch 732/1000\n",
      "414759/414759 [==============================] - 14s 35us/sample - loss: 262.0809 - val_loss: 172.5891 - lr: 7.1191e-04\n",
      "Epoch 733/1000\n",
      "414759/414759 [==============================] - 15s 35us/sample - loss: 262.1361 - val_loss: 172.4744 - lr: 7.1191e-04\n",
      "Epoch 734/1000\n",
      "414759/414759 [==============================] - 21s 51us/sample - loss: 261.9910 - val_loss: 172.5352 - lr: 7.1191e-04\n",
      "Epoch 735/1000\n",
      "414759/414759 [==============================] - 15s 37us/sample - loss: 262.0988 - val_loss: 172.4188 - lr: 7.1191e-04\n",
      "Epoch 736/1000\n",
      "414759/414759 [==============================] - 14s 35us/sample - loss: 261.9375 - val_loss: 172.5198 - lr: 7.1191e-04\n",
      "Epoch 737/1000\n",
      "414759/414759 [==============================] - 15s 35us/sample - loss: 261.9935 - val_loss: 172.4385 - lr: 7.1191e-04\n",
      "Epoch 738/1000\n",
      "414759/414759 [==============================] - 21s 51us/sample - loss: 261.8450 - val_loss: 172.6094 - lr: 7.1191e-04\n",
      "Epoch 739/1000\n",
      "414759/414759 [==============================] - 15s 36us/sample - loss: 261.9998 - val_loss: 172.4128 - lr: 7.1191e-04\n",
      "Epoch 740/1000\n",
      "414759/414759 [==============================] - 15s 37us/sample - loss: 261.8110 - val_loss: 172.6469 - lr: 7.1191e-04\n",
      "Epoch 741/1000\n",
      "414759/414759 [==============================] - 15s 36us/sample - loss: 261.9646 - val_loss: 172.5189 - lr: 7.1191e-04\n",
      "Epoch 742/1000\n",
      "414759/414759 [==============================] - 21s 50us/sample - loss: 261.8457 - val_loss: 172.6233 - lr: 7.1191e-04\n",
      "Epoch 743/1000\n",
      "414759/414759 [==============================] - 15s 37us/sample - loss: 261.8766 - val_loss: 172.4875 - lr: 7.1191e-04\n",
      "Epoch 744/1000\n",
      "414759/414759 [==============================] - 22s 54us/sample - loss: 261.7611 - val_loss: 172.6414 - lr: 7.1191e-04\n",
      "Epoch 745/1000\n",
      "414759/414759 [==============================] - 20s 48us/sample - loss: 261.8810 - val_loss: 172.6741 - lr: 7.1191e-04\n",
      "Epoch 746/1000\n",
      "414759/414759 [==============================] - 20s 48us/sample - loss: 261.8356 - val_loss: 172.3959 - lr: 7.1191e-04\n",
      "Epoch 747/1000\n",
      "414759/414759 [==============================] - 15s 36us/sample - loss: 261.5842 - val_loss: 172.6663 - lr: 7.1191e-04\n",
      "Epoch 748/1000\n",
      "414759/414759 [==============================] - 15s 36us/sample - loss: 261.8290 - val_loss: 172.3622 - lr: 7.1191e-04\n",
      "Epoch 749/1000\n",
      "414759/414759 [==============================] - 15s 36us/sample - loss: 261.5141 - val_loss: 172.7800 - lr: 7.1191e-04\n",
      "Epoch 750/1000\n",
      "414759/414759 [==============================] - 16s 37us/sample - loss: 261.9174 - val_loss: 172.6411 - lr: 7.1191e-04\n",
      "Epoch 751/1000\n",
      "414759/414759 [==============================] - 21s 50us/sample - loss: 261.7783 - val_loss: 172.5191 - lr: 7.1191e-04\n",
      "Epoch 752/1000\n",
      "414759/414759 [==============================] - 20s 49us/sample - loss: 261.5970 - val_loss: 172.6660 - lr: 7.1191e-04\n",
      "Epoch 753/1000\n",
      "414759/414759 [==============================] - 21s 51us/sample - loss: 261.7477 - val_loss: 172.4262 - lr: 7.1191e-04\n",
      "Epoch 754/1000\n",
      "414759/414759 [==============================] - 20s 48us/sample - loss: 261.5125 - val_loss: 172.6164 - lr: 7.1191e-04\n",
      "Epoch 755/1000\n",
      "414759/414759 [==============================] - 21s 52us/sample - loss: 261.6017 - val_loss: 172.5698 - lr: 7.1191e-04\n",
      "Epoch 756/1000\n",
      "414759/414759 [==============================] - 21s 51us/sample - loss: 261.5639 - val_loss: 172.3709 - lr: 7.1191e-04\n",
      "Epoch 757/1000\n",
      "414759/414759 [==============================] - 20s 49us/sample - loss: 261.3612 - val_loss: 172.6562 - lr: 7.1191e-04\n",
      "Epoch 758/1000\n",
      "414759/414759 [==============================] - 16s 38us/sample - loss: 261.5623 - val_loss: 172.4187 - lr: 7.1191e-04\n",
      "Epoch 759/1000\n",
      "414759/414759 [==============================] - 21s 50us/sample - loss: 261.3100 - val_loss: 172.5140 - lr: 7.1191e-04\n",
      "Epoch 760/1000\n",
      "414759/414759 [==============================] - 22s 52us/sample - loss: 261.4299 - val_loss: 172.5143 - lr: 7.1191e-04\n",
      "Epoch 761/1000\n",
      "414759/414759 [==============================] - 27s 66us/sample - loss: 261.4051 - val_loss: 172.3835 - lr: 7.1191e-04\n",
      "Epoch 762/1000\n",
      "414759/414759 [==============================] - 15s 37us/sample - loss: 261.2437 - val_loss: 172.4690 - lr: 7.1191e-04\n",
      "Epoch 763/1000\n",
      "414759/414759 [==============================] - 20s 49us/sample - loss: 261.2849 - val_loss: 172.4348 - lr: 7.1191e-04\n",
      "Epoch 764/1000\n",
      "414759/414759 [==============================] - 22s 54us/sample - loss: 261.2751 - val_loss: 172.2465 - lr: 5.3394e-04\n",
      "Epoch 765/1000\n",
      "414759/414759 [==============================] - 18s 43us/sample - loss: 260.9703 - val_loss: 172.3332 - lr: 5.3394e-04\n",
      "Epoch 766/1000\n",
      "414759/414759 [==============================] - 21s 50us/sample - loss: 261.0941 - val_loss: 172.2757 - lr: 5.3394e-04\n",
      "Epoch 767/1000\n",
      "414759/414759 [==============================] - 22s 52us/sample - loss: 260.9841 - val_loss: 172.1857 - lr: 5.3394e-04\n",
      "Epoch 768/1000\n",
      "414759/414759 [==============================] - 16s 38us/sample - loss: 260.9044 - val_loss: 172.3470 - lr: 5.3394e-04\n",
      "Epoch 769/1000\n",
      "414759/414759 [==============================] - 15s 36us/sample - loss: 260.9888 - val_loss: 172.1861 - lr: 5.3394e-04\n",
      "Epoch 770/1000\n",
      "414759/414759 [==============================] - 21s 50us/sample - loss: 260.8533 - val_loss: 172.3537 - lr: 5.3394e-04\n",
      "Epoch 771/1000\n",
      "414759/414759 [==============================] - 24s 57us/sample - loss: 261.0102 - val_loss: 172.3526 - lr: 5.3394e-04\n",
      "Epoch 772/1000\n",
      "414759/414759 [==============================] - 17s 40us/sample - loss: 261.0362 - val_loss: 172.2748 - lr: 5.3394e-04\n",
      "Epoch 773/1000\n",
      "414759/414759 [==============================] - 19s 47us/sample - loss: 260.8597 - val_loss: 172.2529 - lr: 5.3394e-04\n",
      "Epoch 774/1000\n",
      "414759/414759 [==============================] - 14s 34us/sample - loss: 260.8053 - val_loss: 172.3345 - lr: 5.3394e-04\n",
      "Epoch 775/1000\n",
      "414759/414759 [==============================] - 23s 56us/sample - loss: 260.9116 - val_loss: 172.1614 - lr: 5.3394e-04\n",
      "Epoch 776/1000\n",
      "414759/414759 [==============================] - 17s 41us/sample - loss: 260.7017 - val_loss: 172.3743 - lr: 5.3394e-04\n",
      "Epoch 777/1000\n",
      "414759/414759 [==============================] - 16s 38us/sample - loss: 260.9115 - val_loss: 172.2884 - lr: 5.3394e-04\n",
      "Epoch 778/1000\n",
      "414759/414759 [==============================] - 23s 55us/sample - loss: 260.8204 - val_loss: 172.2508 - lr: 5.3394e-04\n",
      "Epoch 779/1000\n",
      "414759/414759 [==============================] - 21s 51us/sample - loss: 260.6906 - val_loss: 172.2711 - lr: 5.3394e-04\n",
      "Epoch 780/1000\n",
      "414759/414759 [==============================] - 22s 53us/sample - loss: 260.7286 - val_loss: 172.1845 - lr: 5.3394e-04\n",
      "Epoch 781/1000\n",
      "414759/414759 [==============================] - 15s 36us/sample - loss: 260.6909 - val_loss: 172.1248 - lr: 5.3394e-04\n",
      "Epoch 782/1000\n",
      "414759/414759 [==============================] - 21s 51us/sample - loss: 260.5692 - val_loss: 172.1950 - lr: 5.3394e-04\n",
      "Epoch 783/1000\n",
      "414759/414759 [==============================] - 21s 50us/sample - loss: 260.6192 - val_loss: 172.1333 - lr: 5.3394e-04\n",
      "Epoch 784/1000\n",
      "414759/414759 [==============================] - 16s 38us/sample - loss: 260.5262 - val_loss: 172.1877 - lr: 5.3394e-04\n",
      "Epoch 785/1000\n",
      "414759/414759 [==============================] - 20s 48us/sample - loss: 260.5379 - val_loss: 172.1377 - lr: 5.3394e-04\n",
      "Epoch 786/1000\n",
      "414759/414759 [==============================] - 22s 54us/sample - loss: 260.4703 - val_loss: 172.1166 - lr: 5.3394e-04\n",
      "Epoch 787/1000\n",
      "414759/414759 [==============================] - 21s 50us/sample - loss: 260.4493 - val_loss: 172.0543 - lr: 5.3394e-04\n",
      "Epoch 788/1000\n",
      "414759/414759 [==============================] - 21s 51us/sample - loss: 260.3890 - val_loss: 172.1452 - lr: 5.3394e-04\n",
      "Epoch 789/1000\n",
      "414759/414759 [==============================] - 20s 49us/sample - loss: 260.4568 - val_loss: 172.1573 - lr: 5.3394e-04\n",
      "Epoch 790/1000\n",
      "414759/414759 [==============================] - 18s 44us/sample - loss: 260.3584 - val_loss: 172.1300 - lr: 5.3394e-04\n",
      "Epoch 791/1000\n",
      "414759/414759 [==============================] - 14s 35us/sample - loss: 260.3341 - val_loss: 172.0628 - lr: 5.3394e-04\n",
      "Epoch 792/1000\n",
      "414759/414759 [==============================] - 22s 53us/sample - loss: 260.3114 - val_loss: 172.0680 - lr: 5.3394e-04\n",
      "Epoch 793/1000\n",
      "414759/414759 [==============================] - 20s 48us/sample - loss: 260.3063 - val_loss: 172.1060 - lr: 5.3394e-04\n",
      "Epoch 794/1000\n",
      "414759/414759 [==============================] - 20s 49us/sample - loss: 260.2966 - val_loss: 172.0145 - lr: 5.3394e-04\n",
      "Epoch 795/1000\n",
      "414759/414759 [==============================] - 21s 52us/sample - loss: 260.2180 - val_loss: 172.0529 - lr: 5.3394e-04\n",
      "Epoch 796/1000\n",
      "414759/414759 [==============================] - 17s 41us/sample - loss: 260.1826 - val_loss: 172.0442 - lr: 5.3394e-04\n",
      "Epoch 797/1000\n",
      "414759/414759 [==============================] - 21s 50us/sample - loss: 260.1813 - val_loss: 172.0009 - lr: 5.3394e-04\n",
      "Epoch 798/1000\n",
      "414759/414759 [==============================] - 20s 49us/sample - loss: 260.1221 - val_loss: 172.0358 - lr: 5.3394e-04\n",
      "Epoch 799/1000\n",
      "414759/414759 [==============================] - 14s 34us/sample - loss: 260.0605 - val_loss: 172.0118 - lr: 5.3394e-04\n",
      "Epoch 800/1000\n",
      "414759/414759 [==============================] - 21s 52us/sample - loss: 260.0746 - val_loss: 171.9552 - lr: 5.3394e-04\n",
      "Epoch 801/1000\n",
      "414759/414759 [==============================] - 20s 49us/sample - loss: 259.9578 - val_loss: 172.0249 - lr: 5.3394e-04\n",
      "Epoch 802/1000\n",
      "414759/414759 [==============================] - 15s 36us/sample - loss: 259.9986 - val_loss: 171.9667 - lr: 5.3394e-04\n",
      "Epoch 803/1000\n",
      "414759/414759 [==============================] - 22s 53us/sample - loss: 259.9345 - val_loss: 171.9518 - lr: 5.3394e-04\n",
      "Epoch 804/1000\n",
      "414759/414759 [==============================] - 20s 49us/sample - loss: 259.9012 - val_loss: 171.9485 - lr: 5.3394e-04\n",
      "Epoch 805/1000\n",
      "414759/414759 [==============================] - 20s 49us/sample - loss: 259.9006 - val_loss: 171.9551 - lr: 5.3394e-04\n",
      "Epoch 806/1000\n",
      "414759/414759 [==============================] - 26s 62us/sample - loss: 259.8507 - val_loss: 171.9298 - lr: 5.3394e-04\n",
      "Epoch 807/1000\n",
      "414759/414759 [==============================] - 26s 63us/sample - loss: 259.8227 - val_loss: 171.9464 - lr: 5.3394e-04\n",
      "Epoch 808/1000\n",
      "414759/414759 [==============================] - 22s 53us/sample - loss: 259.8487 - val_loss: 171.8784 - lr: 5.3394e-04\n",
      "Epoch 809/1000\n",
      "414759/414759 [==============================] - 22s 52us/sample - loss: 259.7597 - val_loss: 171.9059 - lr: 5.3394e-04\n",
      "Epoch 810/1000\n",
      "414759/414759 [==============================] - 14s 34us/sample - loss: 259.7974 - val_loss: 171.8855 - lr: 5.3394e-04\n",
      "Epoch 811/1000\n",
      "414759/414759 [==============================] - 25s 60us/sample - loss: 259.6788 - val_loss: 171.9364 - lr: 5.3394e-04\n",
      "Epoch 812/1000\n",
      "414759/414759 [==============================] - 21s 51us/sample - loss: 259.7354 - val_loss: 171.8711 - lr: 5.3394e-04\n",
      "Epoch 813/1000\n",
      "414759/414759 [==============================] - 16s 39us/sample - loss: 259.6064 - val_loss: 171.9288 - lr: 5.3394e-04\n",
      "Epoch 814/1000\n",
      "414759/414759 [==============================] - 15s 37us/sample - loss: 259.6580 - val_loss: 171.8941 - lr: 5.3394e-04\n",
      "Epoch 815/1000\n",
      "414759/414759 [==============================] - 21s 50us/sample - loss: 259.5796 - val_loss: 171.9455 - lr: 5.3394e-04\n",
      "Epoch 816/1000\n",
      "414759/414759 [==============================] - 20s 48us/sample - loss: 259.6512 - val_loss: 171.8731 - lr: 5.3394e-04\n",
      "Epoch 817/1000\n",
      "414759/414759 [==============================] - 14s 33us/sample - loss: 259.5442 - val_loss: 171.9956 - lr: 5.3394e-04\n",
      "Epoch 818/1000\n",
      "414759/414759 [==============================] - 20s 48us/sample - loss: 259.6510 - val_loss: 171.8763 - lr: 5.3394e-04\n",
      "Epoch 819/1000\n",
      "414759/414759 [==============================] - 22s 53us/sample - loss: 259.5279 - val_loss: 172.0140 - lr: 5.3394e-04\n",
      "Epoch 820/1000\n",
      "414759/414759 [==============================] - 16s 38us/sample - loss: 259.5910 - val_loss: 171.8925 - lr: 5.3394e-04\n",
      "Epoch 821/1000\n",
      "414759/414759 [==============================] - 22s 54us/sample - loss: 259.5289 - val_loss: 171.9243 - lr: 5.3394e-04\n",
      "Epoch 822/1000\n",
      "414759/414759 [==============================] - 21s 50us/sample - loss: 259.5578 - val_loss: 171.9131 - lr: 5.3394e-04\n",
      "Epoch 823/1000\n",
      "414759/414759 [==============================] - 26s 63us/sample - loss: 259.4755 - val_loss: 171.9139 - lr: 5.3394e-04\n",
      "Epoch 824/1000\n",
      "414759/414759 [==============================] - 24s 57us/sample - loss: 259.5098 - val_loss: 171.9081 - lr: 5.3394e-04\n",
      "Epoch 825/1000\n",
      "414759/414759 [==============================] - 22s 53us/sample - loss: 259.4772 - val_loss: 171.8395 - lr: 5.3394e-04\n",
      "Epoch 826/1000\n",
      "414759/414759 [==============================] - 15s 37us/sample - loss: 259.3670 - val_loss: 171.8424 - lr: 5.3394e-04\n",
      "Epoch 827/1000\n",
      "414759/414759 [==============================] - 15s 36us/sample - loss: 259.3507 - val_loss: 171.9133 - lr: 5.3394e-04\n",
      "Epoch 828/1000\n",
      "414759/414759 [==============================] - 23s 57us/sample - loss: 259.3098 - val_loss: 171.9040 - lr: 5.3394e-04\n",
      "Epoch 829/1000\n",
      "414759/414759 [==============================] - 14s 34us/sample - loss: 259.3310 - val_loss: 171.7249 - lr: 5.3394e-04\n",
      "Epoch 830/1000\n",
      "414759/414759 [==============================] - 14s 35us/sample - loss: 259.1640 - val_loss: 171.9692 - lr: 5.3394e-04\n",
      "Epoch 831/1000\n",
      "414759/414759 [==============================] - 15s 36us/sample - loss: 259.3656 - val_loss: 171.7483 - lr: 5.3394e-04\n",
      "Epoch 832/1000\n",
      "414759/414759 [==============================] - 23s 55us/sample - loss: 259.1558 - val_loss: 171.9701 - lr: 5.3394e-04\n",
      "Epoch 833/1000\n",
      "414759/414759 [==============================] - 23s 56us/sample - loss: 259.4078 - val_loss: 171.9196 - lr: 5.3394e-04\n",
      "Epoch 834/1000\n",
      "414759/414759 [==============================] - 20s 48us/sample - loss: 259.3391 - val_loss: 171.8243 - lr: 5.3394e-04\n",
      "Epoch 835/1000\n",
      "414759/414759 [==============================] - 15s 36us/sample - loss: 259.1749 - val_loss: 172.0137 - lr: 5.3394e-04\n",
      "Epoch 836/1000\n",
      "414759/414759 [==============================] - 26s 61us/sample - loss: 259.3816 - val_loss: 171.7467 - lr: 5.3394e-04\n",
      "Epoch 837/1000\n",
      "414759/414759 [==============================] - 16s 39us/sample - loss: 259.0924 - val_loss: 172.1930 - lr: 5.3394e-04\n",
      "Epoch 838/1000\n",
      "414759/414759 [==============================] - 23s 55us/sample - loss: 259.4994 - val_loss: 172.2183 - lr: 5.3394e-04\n",
      "Epoch 839/1000\n",
      "414759/414759 [==============================] - 14s 34us/sample - loss: 259.5198 - val_loss: 171.8557 - lr: 5.3394e-04\n",
      "Epoch 840/1000\n",
      "414759/414759 [==============================] - 20s 49us/sample - loss: 259.1085 - val_loss: 171.9172 - lr: 5.3394e-04\n",
      "Epoch 841/1000\n",
      "414759/414759 [==============================] - 15s 37us/sample - loss: 259.1980 - val_loss: 171.9515 - lr: 5.3394e-04\n",
      "Epoch 842/1000\n",
      "414759/414759 [==============================] - 15s 36us/sample - loss: 259.2060 - val_loss: 171.7313 - lr: 5.3394e-04\n",
      "Epoch 843/1000\n",
      "414759/414759 [==============================] - 20s 49us/sample - loss: 258.8887 - val_loss: 172.0828 - lr: 5.3394e-04\n",
      "Epoch 844/1000\n",
      "414759/414759 [==============================] - 16s 38us/sample - loss: 259.3163 - val_loss: 171.8912 - lr: 5.3394e-04\n",
      "Epoch 845/1000\n",
      "414759/414759 [==============================] - 21s 50us/sample - loss: 259.0912 - val_loss: 171.7616 - lr: 4.0045e-04\n",
      "Epoch 846/1000\n",
      "414759/414759 [==============================] - 17s 40us/sample - loss: 258.8970 - val_loss: 171.7825 - lr: 4.0045e-04\n",
      "Epoch 847/1000\n",
      "414759/414759 [==============================] - 21s 50us/sample - loss: 258.9446 - val_loss: 171.6954 - lr: 4.0045e-04\n",
      "Epoch 848/1000\n",
      "414759/414759 [==============================] - 14s 34us/sample - loss: 258.8000 - val_loss: 171.7564 - lr: 4.0045e-04\n",
      "Epoch 849/1000\n",
      "414759/414759 [==============================] - 15s 36us/sample - loss: 258.8000 - val_loss: 171.6669 - lr: 4.0045e-04\n",
      "Epoch 850/1000\n",
      "414759/414759 [==============================] - 21s 50us/sample - loss: 258.7955 - val_loss: 171.6440 - lr: 4.0045e-04\n",
      "Epoch 851/1000\n",
      "414759/414759 [==============================] - 19s 47us/sample - loss: 258.7271 - val_loss: 171.7341 - lr: 4.0045e-04\n",
      "Epoch 852/1000\n",
      "414759/414759 [==============================] - 20s 47us/sample - loss: 258.7878 - val_loss: 171.5782 - lr: 4.0045e-04\n",
      "Epoch 853/1000\n",
      "414759/414759 [==============================] - 22s 53us/sample - loss: 258.6390 - val_loss: 171.7861 - lr: 4.0045e-04\n",
      "Epoch 854/1000\n",
      "414759/414759 [==============================] - 21s 50us/sample - loss: 258.9095 - val_loss: 171.7699 - lr: 4.0045e-04\n",
      "Epoch 855/1000\n",
      "414759/414759 [==============================] - 24s 59us/sample - loss: 258.8358 - val_loss: 171.6395 - lr: 4.0045e-04\n",
      "Epoch 856/1000\n",
      "414759/414759 [==============================] - 21s 51us/sample - loss: 258.6740 - val_loss: 171.7008 - lr: 4.0045e-04\n",
      "Epoch 857/1000\n",
      "414759/414759 [==============================] - 20s 49us/sample - loss: 258.6862 - val_loss: 171.6618 - lr: 4.0045e-04\n",
      "Epoch 858/1000\n",
      "414759/414759 [==============================] - 20s 48us/sample - loss: 258.6185 - val_loss: 171.5903 - lr: 4.0045e-04\n",
      "Epoch 859/1000\n",
      "414759/414759 [==============================] - 20s 49us/sample - loss: 258.5686 - val_loss: 171.7262 - lr: 4.0045e-04\n",
      "Epoch 860/1000\n",
      "414759/414759 [==============================] - 20s 48us/sample - loss: 258.6090 - val_loss: 171.5557 - lr: 4.0045e-04\n",
      "Epoch 861/1000\n",
      "414759/414759 [==============================] - 23s 55us/sample - loss: 258.5167 - val_loss: 171.7422 - lr: 4.0045e-04\n",
      "Epoch 862/1000\n",
      "414759/414759 [==============================] - 21s 51us/sample - loss: 258.7065 - val_loss: 171.6854 - lr: 4.0045e-04\n",
      "Epoch 863/1000\n",
      "414759/414759 [==============================] - 15s 36us/sample - loss: 258.6098 - val_loss: 171.6295 - lr: 4.0045e-04\n",
      "Epoch 864/1000\n",
      "414759/414759 [==============================] - 21s 51us/sample - loss: 258.5114 - val_loss: 171.6350 - lr: 4.0045e-04\n",
      "Epoch 865/1000\n",
      "414759/414759 [==============================] - 16s 39us/sample - loss: 258.4976 - val_loss: 171.5686 - lr: 4.0045e-04\n",
      "Epoch 866/1000\n",
      "414759/414759 [==============================] - 22s 53us/sample - loss: 258.4077 - val_loss: 171.5401 - lr: 4.0045e-04\n",
      "Epoch 867/1000\n",
      "414759/414759 [==============================] - 14s 35us/sample - loss: 258.3740 - val_loss: 171.5495 - lr: 4.0045e-04\n",
      "Epoch 868/1000\n",
      "414759/414759 [==============================] - 16s 39us/sample - loss: 258.3960 - val_loss: 171.5063 - lr: 4.0045e-04\n",
      "Epoch 869/1000\n",
      "414759/414759 [==============================] - 21s 51us/sample - loss: 258.3367 - val_loss: 171.6389 - lr: 4.0045e-04\n",
      "Epoch 870/1000\n",
      "414759/414759 [==============================] - 14s 35us/sample - loss: 258.3969 - val_loss: 171.5803 - lr: 4.0045e-04\n",
      "Epoch 871/1000\n",
      "414759/414759 [==============================] - 23s 55us/sample - loss: 258.3535 - val_loss: 171.5790 - lr: 4.0045e-04\n",
      "Epoch 872/1000\n",
      "414759/414759 [==============================] - 20s 49us/sample - loss: 258.3869 - val_loss: 171.5852 - lr: 4.0045e-04\n",
      "Epoch 873/1000\n",
      "414759/414759 [==============================] - 14s 35us/sample - loss: 258.3601 - val_loss: 171.5461 - lr: 4.0045e-04\n",
      "Epoch 874/1000\n",
      "414759/414759 [==============================] - 22s 53us/sample - loss: 258.2699 - val_loss: 171.5346 - lr: 4.0045e-04\n",
      "Epoch 875/1000\n",
      "414759/414759 [==============================] - 22s 53us/sample - loss: 258.2115 - val_loss: 171.4649 - lr: 4.0045e-04\n",
      "Epoch 876/1000\n",
      "414759/414759 [==============================] - 14s 34us/sample - loss: 258.2284 - val_loss: 171.4473 - lr: 4.0045e-04\n",
      "Epoch 877/1000\n",
      "414759/414759 [==============================] - 19s 45us/sample - loss: 258.2444 - val_loss: 171.5107 - lr: 4.0045e-04\n",
      "Epoch 878/1000\n",
      "414759/414759 [==============================] - 14s 34us/sample - loss: 258.1721 - val_loss: 171.4997 - lr: 4.0045e-04\n",
      "Epoch 879/1000\n",
      "414759/414759 [==============================] - 21s 50us/sample - loss: 258.1206 - val_loss: 171.4972 - lr: 4.0045e-04\n",
      "Epoch 880/1000\n",
      "414759/414759 [==============================] - 14s 34us/sample - loss: 258.1449 - val_loss: 171.4588 - lr: 4.0045e-04\n",
      "Epoch 881/1000\n",
      "414759/414759 [==============================] - 23s 55us/sample - loss: 258.1177 - val_loss: 171.4515 - lr: 4.0045e-04\n",
      "Epoch 882/1000\n",
      "414759/414759 [==============================] - 14s 34us/sample - loss: 258.0874 - val_loss: 171.4019 - lr: 4.0045e-04\n",
      "Epoch 883/1000\n",
      "414759/414759 [==============================] - 21s 50us/sample - loss: 258.0608 - val_loss: 171.4887 - lr: 4.0045e-04\n",
      "Epoch 884/1000\n",
      "414759/414759 [==============================] - 20s 49us/sample - loss: 258.0648 - val_loss: 171.4247 - lr: 4.0045e-04\n",
      "Epoch 885/1000\n",
      "414759/414759 [==============================] - 19s 46us/sample - loss: 258.0163 - val_loss: 171.4825 - lr: 4.0045e-04\n",
      "Epoch 886/1000\n",
      "414759/414759 [==============================] - 14s 35us/sample - loss: 258.0265 - val_loss: 171.4359 - lr: 4.0045e-04\n",
      "Epoch 887/1000\n",
      "414759/414759 [==============================] - 20s 48us/sample - loss: 257.9746 - val_loss: 171.4587 - lr: 4.0045e-04\n",
      "Epoch 888/1000\n",
      "414759/414759 [==============================] - 28s 67us/sample - loss: 257.9626 - val_loss: 171.4462 - lr: 4.0045e-04\n",
      "Epoch 889/1000\n",
      "414759/414759 [==============================] - 15s 35us/sample - loss: 257.9611 - val_loss: 171.4321 - lr: 4.0045e-04\n",
      "Epoch 890/1000\n",
      "414759/414759 [==============================] - 21s 51us/sample - loss: 257.9509 - val_loss: 171.3942 - lr: 4.0045e-04\n",
      "Epoch 891/1000\n",
      "414759/414759 [==============================] - 15s 35us/sample - loss: 257.9254 - val_loss: 171.4114 - lr: 4.0045e-04\n",
      "Epoch 892/1000\n",
      "414759/414759 [==============================] - 20s 49us/sample - loss: 257.8772 - val_loss: 171.3829 - lr: 4.0045e-04\n",
      "Epoch 893/1000\n",
      "414759/414759 [==============================] - 16s 37us/sample - loss: 257.8561 - val_loss: 171.4261 - lr: 4.0045e-04\n",
      "Epoch 894/1000\n",
      "414759/414759 [==============================] - 21s 51us/sample - loss: 257.9289 - val_loss: 171.4302 - lr: 4.0045e-04\n",
      "Epoch 895/1000\n",
      "414759/414759 [==============================] - 16s 39us/sample - loss: 257.8566 - val_loss: 171.3728 - lr: 4.0045e-04\n",
      "Epoch 896/1000\n",
      "414759/414759 [==============================] - 23s 55us/sample - loss: 257.8047 - val_loss: 171.4206 - lr: 4.0045e-04\n",
      "Epoch 897/1000\n",
      "414759/414759 [==============================] - 15s 36us/sample - loss: 257.7767 - val_loss: 171.3374 - lr: 4.0045e-04\n",
      "Epoch 898/1000\n",
      "414759/414759 [==============================] - 20s 49us/sample - loss: 257.7095 - val_loss: 171.3773 - lr: 4.0045e-04\n",
      "Epoch 899/1000\n",
      "414759/414759 [==============================] - 21s 51us/sample - loss: 257.7318 - val_loss: 171.3377 - lr: 4.0045e-04\n",
      "Epoch 900/1000\n",
      "414759/414759 [==============================] - 20s 49us/sample - loss: 257.6951 - val_loss: 171.3514 - lr: 4.0045e-04\n",
      "Epoch 901/1000\n",
      "414759/414759 [==============================] - 25s 59us/sample - loss: 257.7029 - val_loss: 171.3026 - lr: 4.0045e-04\n",
      "Epoch 902/1000\n",
      "414759/414759 [==============================] - 21s 51us/sample - loss: 257.6212 - val_loss: 171.3272 - lr: 4.0045e-04\n",
      "Epoch 903/1000\n",
      "414759/414759 [==============================] - 15s 37us/sample - loss: 257.6409 - val_loss: 171.3268 - lr: 4.0045e-04\n",
      "Epoch 904/1000\n",
      "414759/414759 [==============================] - 20s 49us/sample - loss: 257.6007 - val_loss: 171.3276 - lr: 4.0045e-04\n",
      "Epoch 905/1000\n",
      "414759/414759 [==============================] - 19s 45us/sample - loss: 257.5765 - val_loss: 171.3230 - lr: 4.0045e-04\n",
      "Epoch 906/1000\n",
      "414759/414759 [==============================] - 14s 34us/sample - loss: 257.5136 - val_loss: 171.3575 - lr: 4.0045e-04\n",
      "Epoch 907/1000\n",
      "414759/414759 [==============================] - 22s 53us/sample - loss: 257.5266 - val_loss: 171.2932 - lr: 4.0045e-04\n",
      "Epoch 908/1000\n",
      "414759/414759 [==============================] - 26s 62us/sample - loss: 257.5258 - val_loss: 171.3315 - lr: 4.0045e-04\n",
      "Epoch 909/1000\n",
      "414759/414759 [==============================] - 21s 51us/sample - loss: 257.5147 - val_loss: 171.3119 - lr: 4.0045e-04\n",
      "Epoch 910/1000\n",
      "414759/414759 [==============================] - 15s 35us/sample - loss: 257.4737 - val_loss: 171.2729 - lr: 4.0045e-04\n",
      "Epoch 911/1000\n",
      "414759/414759 [==============================] - 14s 33us/sample - loss: 257.4144 - val_loss: 171.2906 - lr: 4.0045e-04\n",
      "Epoch 912/1000\n",
      "414759/414759 [==============================] - 25s 61us/sample - loss: 257.4307 - val_loss: 171.2945 - lr: 4.0045e-04\n",
      "Epoch 913/1000\n",
      "414759/414759 [==============================] - 14s 34us/sample - loss: 257.4507 - val_loss: 171.2418 - lr: 4.0045e-04\n",
      "Epoch 914/1000\n",
      "414759/414759 [==============================] - 16s 39us/sample - loss: 257.3842 - val_loss: 171.2987 - lr: 4.0045e-04\n",
      "Epoch 915/1000\n",
      "414759/414759 [==============================] - 14s 34us/sample - loss: 257.4305 - val_loss: 171.2539 - lr: 4.0045e-04\n",
      "Epoch 916/1000\n",
      "414759/414759 [==============================] - 23s 56us/sample - loss: 257.3445 - val_loss: 171.2630 - lr: 4.0045e-04\n",
      "Epoch 917/1000\n",
      "414759/414759 [==============================] - 16s 39us/sample - loss: 257.3416 - val_loss: 171.2688 - lr: 4.0045e-04\n",
      "Epoch 918/1000\n",
      "414759/414759 [==============================] - 21s 50us/sample - loss: 257.2586 - val_loss: 171.2757 - lr: 4.0045e-04\n",
      "Epoch 919/1000\n",
      "414759/414759 [==============================] - 21s 51us/sample - loss: 257.3287 - val_loss: 171.2488 - lr: 4.0045e-04\n",
      "Epoch 920/1000\n",
      "414759/414759 [==============================] - 27s 65us/sample - loss: 257.2487 - val_loss: 171.2595 - lr: 4.0045e-04\n",
      "Epoch 921/1000\n",
      "414759/414759 [==============================] - 26s 63us/sample - loss: 257.2508 - val_loss: 171.2052 - lr: 4.0045e-04\n",
      "Epoch 922/1000\n",
      "414759/414759 [==============================] - 22s 53us/sample - loss: 257.1964 - val_loss: 171.2766 - lr: 4.0045e-04\n",
      "Epoch 923/1000\n",
      "414759/414759 [==============================] - 20s 47us/sample - loss: 257.2631 - val_loss: 171.2330 - lr: 4.0045e-04\n",
      "Epoch 924/1000\n",
      "414759/414759 [==============================] - 21s 51us/sample - loss: 257.2015 - val_loss: 171.2530 - lr: 4.0045e-04\n",
      "Epoch 925/1000\n",
      "414759/414759 [==============================] - 16s 39us/sample - loss: 257.1959 - val_loss: 171.2330 - lr: 4.0045e-04\n",
      "Epoch 926/1000\n",
      "414759/414759 [==============================] - 20s 48us/sample - loss: 257.1767 - val_loss: 171.1843 - lr: 4.0045e-04\n",
      "Epoch 927/1000\n",
      "414759/414759 [==============================] - 20s 49us/sample - loss: 257.0907 - val_loss: 171.2233 - lr: 4.0045e-04\n",
      "Epoch 928/1000\n",
      "414759/414759 [==============================] - 22s 54us/sample - loss: 257.0873 - val_loss: 171.1819 - lr: 4.0045e-04\n",
      "Epoch 929/1000\n",
      "414759/414759 [==============================] - 16s 39us/sample - loss: 257.0570 - val_loss: 171.2024 - lr: 4.0045e-04\n",
      "Epoch 930/1000\n",
      "414759/414759 [==============================] - 16s 38us/sample - loss: 257.0215 - val_loss: 171.1545 - lr: 4.0045e-04\n",
      "Epoch 931/1000\n",
      "414759/414759 [==============================] - 15s 36us/sample - loss: 256.9950 - val_loss: 171.1539 - lr: 4.0045e-04\n",
      "Epoch 932/1000\n",
      "414759/414759 [==============================] - 15s 37us/sample - loss: 256.9971 - val_loss: 171.1480 - lr: 4.0045e-04\n",
      "Epoch 933/1000\n",
      "414759/414759 [==============================] - 15s 37us/sample - loss: 256.9784 - val_loss: 171.1534 - lr: 4.0045e-04\n",
      "Epoch 934/1000\n",
      "414759/414759 [==============================] - 20s 48us/sample - loss: 256.9728 - val_loss: 171.1550 - lr: 4.0045e-04\n",
      "Epoch 935/1000\n",
      "414759/414759 [==============================] - 22s 52us/sample - loss: 256.9501 - val_loss: 171.1210 - lr: 4.0045e-04\n",
      "Epoch 936/1000\n",
      "414759/414759 [==============================] - 20s 49us/sample - loss: 256.9391 - val_loss: 171.1280 - lr: 4.0045e-04\n",
      "Epoch 937/1000\n",
      "414759/414759 [==============================] - 16s 39us/sample - loss: 256.8698 - val_loss: 171.1254 - lr: 4.0045e-04\n",
      "Epoch 938/1000\n",
      "414759/414759 [==============================] - 23s 55us/sample - loss: 256.8746 - val_loss: 171.1628 - lr: 4.0045e-04\n",
      "Epoch 939/1000\n",
      "414759/414759 [==============================] - 14s 34us/sample - loss: 256.8931 - val_loss: 171.0872 - lr: 4.0045e-04\n",
      "Epoch 940/1000\n",
      "414759/414759 [==============================] - 16s 40us/sample - loss: 256.7881 - val_loss: 171.1200 - lr: 4.0045e-04\n",
      "Epoch 941/1000\n",
      "414759/414759 [==============================] - 15s 36us/sample - loss: 256.8099 - val_loss: 171.0661 - lr: 4.0045e-04\n",
      "Epoch 942/1000\n",
      "414759/414759 [==============================] - 16s 38us/sample - loss: 256.7946 - val_loss: 171.0920 - lr: 4.0045e-04\n",
      "Epoch 943/1000\n",
      "414759/414759 [==============================] - 14s 34us/sample - loss: 256.7419 - val_loss: 171.0845 - lr: 4.0045e-04\n",
      "Epoch 944/1000\n",
      "414759/414759 [==============================] - 22s 54us/sample - loss: 256.7679 - val_loss: 171.0464 - lr: 4.0045e-04\n",
      "Epoch 945/1000\n",
      "414759/414759 [==============================] - 16s 38us/sample - loss: 256.7334 - val_loss: 171.0735 - lr: 4.0045e-04\n",
      "Epoch 946/1000\n",
      "414759/414759 [==============================] - 20s 49us/sample - loss: 256.7140 - val_loss: 171.0554 - lr: 4.0045e-04\n",
      "Epoch 947/1000\n",
      "414759/414759 [==============================] - 21s 51us/sample - loss: 256.6583 - val_loss: 171.0789 - lr: 4.0045e-04\n",
      "Epoch 948/1000\n",
      "414759/414759 [==============================] - 15s 35us/sample - loss: 256.6531 - val_loss: 171.0229 - lr: 4.0045e-04\n",
      "Epoch 949/1000\n",
      "414759/414759 [==============================] - 21s 50us/sample - loss: 256.6088 - val_loss: 171.0632 - lr: 4.0045e-04\n",
      "Epoch 950/1000\n",
      "414759/414759 [==============================] - 15s 35us/sample - loss: 256.6262 - val_loss: 170.9896 - lr: 4.0045e-04\n",
      "Epoch 951/1000\n",
      "414759/414759 [==============================] - 21s 50us/sample - loss: 256.5625 - val_loss: 171.0564 - lr: 4.0045e-04\n",
      "Epoch 952/1000\n",
      "414759/414759 [==============================] - 14s 35us/sample - loss: 256.5848 - val_loss: 171.0381 - lr: 4.0045e-04\n",
      "Epoch 953/1000\n",
      "414759/414759 [==============================] - 20s 49us/sample - loss: 256.5522 - val_loss: 171.0582 - lr: 4.0045e-04\n",
      "Epoch 954/1000\n",
      "414759/414759 [==============================] - 15s 36us/sample - loss: 256.5809 - val_loss: 171.0089 - lr: 4.0045e-04\n",
      "Epoch 955/1000\n",
      "414759/414759 [==============================] - 14s 35us/sample - loss: 256.5022 - val_loss: 171.0310 - lr: 4.0045e-04\n",
      "Epoch 956/1000\n",
      "414759/414759 [==============================] - 22s 54us/sample - loss: 256.5306 - val_loss: 170.9485 - lr: 4.0045e-04\n",
      "Epoch 957/1000\n",
      "414759/414759 [==============================] - 15s 36us/sample - loss: 256.4606 - val_loss: 171.0203 - lr: 4.0045e-04\n",
      "Epoch 958/1000\n",
      "414759/414759 [==============================] - 20s 49us/sample - loss: 256.4476 - val_loss: 170.9552 - lr: 4.0045e-04\n",
      "Epoch 959/1000\n",
      "414759/414759 [==============================] - 24s 59us/sample - loss: 256.4112 - val_loss: 171.0261 - lr: 4.0045e-04\n",
      "Epoch 960/1000\n",
      "414759/414759 [==============================] - 19s 45us/sample - loss: 256.4417 - val_loss: 170.9884 - lr: 4.0045e-04\n",
      "Epoch 961/1000\n",
      "414759/414759 [==============================] - 14s 33us/sample - loss: 256.3811 - val_loss: 170.9973 - lr: 4.0045e-04\n",
      "Epoch 962/1000\n",
      "414759/414759 [==============================] - 21s 50us/sample - loss: 256.3538 - val_loss: 170.9540 - lr: 4.0045e-04\n",
      "Epoch 963/1000\n",
      "414759/414759 [==============================] - 20s 48us/sample - loss: 256.3179 - val_loss: 170.9132 - lr: 4.0045e-04\n",
      "Epoch 964/1000\n",
      "414759/414759 [==============================] - 23s 55us/sample - loss: 256.3133 - val_loss: 170.9474 - lr: 4.0045e-04\n",
      "Epoch 965/1000\n",
      "414759/414759 [==============================] - 20s 49us/sample - loss: 256.3108 - val_loss: 170.9684 - lr: 4.0045e-04\n",
      "Epoch 966/1000\n",
      "414759/414759 [==============================] - 21s 50us/sample - loss: 256.2733 - val_loss: 170.9507 - lr: 4.0045e-04\n",
      "Epoch 967/1000\n",
      "414759/414759 [==============================] - 21s 50us/sample - loss: 256.2653 - val_loss: 170.9103 - lr: 4.0045e-04\n",
      "Epoch 968/1000\n",
      "414759/414759 [==============================] - 15s 37us/sample - loss: 256.2131 - val_loss: 170.9392 - lr: 4.0045e-04\n",
      "Epoch 969/1000\n",
      "414759/414759 [==============================] - 16s 38us/sample - loss: 256.2293 - val_loss: 170.9247 - lr: 4.0045e-04\n",
      "Epoch 970/1000\n",
      "414759/414759 [==============================] - 21s 50us/sample - loss: 256.1948 - val_loss: 170.9649 - lr: 4.0045e-04\n",
      "Epoch 971/1000\n",
      "414759/414759 [==============================] - 16s 38us/sample - loss: 256.2362 - val_loss: 170.9067 - lr: 4.0045e-04\n",
      "Epoch 972/1000\n",
      "414759/414759 [==============================] - 14s 35us/sample - loss: 256.1706 - val_loss: 170.9245 - lr: 4.0045e-04\n",
      "Epoch 973/1000\n",
      "414759/414759 [==============================] - 21s 50us/sample - loss: 256.1407 - val_loss: 170.8930 - lr: 4.0045e-04\n",
      "Epoch 974/1000\n",
      "414759/414759 [==============================] - 15s 36us/sample - loss: 256.1009 - val_loss: 170.9180 - lr: 4.0045e-04\n",
      "Epoch 975/1000\n",
      "414759/414759 [==============================] - 14s 34us/sample - loss: 256.0870 - val_loss: 170.8976 - lr: 4.0045e-04\n",
      "Epoch 976/1000\n",
      "414759/414759 [==============================] - 21s 50us/sample - loss: 256.0622 - val_loss: 170.8784 - lr: 4.0045e-04\n",
      "Epoch 977/1000\n",
      "414759/414759 [==============================] - 20s 48us/sample - loss: 256.0904 - val_loss: 170.8553 - lr: 4.0045e-04\n",
      "Epoch 978/1000\n",
      "414759/414759 [==============================] - 15s 37us/sample - loss: 256.0255 - val_loss: 170.8638 - lr: 4.0045e-04\n",
      "Epoch 979/1000\n",
      "414759/414759 [==============================] - 15s 35us/sample - loss: 256.0569 - val_loss: 170.8632 - lr: 4.0045e-04\n",
      "Epoch 980/1000\n",
      "414759/414759 [==============================] - 21s 51us/sample - loss: 255.9916 - val_loss: 170.8901 - lr: 4.0045e-04\n",
      "Epoch 981/1000\n",
      "414759/414759 [==============================] - 15s 37us/sample - loss: 255.9921 - val_loss: 170.8667 - lr: 4.0045e-04\n",
      "Epoch 982/1000\n",
      "414759/414759 [==============================] - 20s 49us/sample - loss: 255.9527 - val_loss: 170.8436 - lr: 4.0045e-04\n",
      "Epoch 983/1000\n",
      "414759/414759 [==============================] - 16s 38us/sample - loss: 255.9527 - val_loss: 170.8533 - lr: 4.0045e-04\n",
      "Epoch 984/1000\n",
      "414759/414759 [==============================] - 15s 37us/sample - loss: 255.9126 - val_loss: 170.8751 - lr: 4.0045e-04\n",
      "Epoch 985/1000\n",
      "414759/414759 [==============================] - 19s 46us/sample - loss: 255.9293 - val_loss: 170.8316 - lr: 4.0045e-04\n",
      "Epoch 986/1000\n",
      "414759/414759 [==============================] - 19s 45us/sample - loss: 255.8935 - val_loss: 170.8576 - lr: 4.0045e-04\n",
      "Epoch 987/1000\n",
      "414759/414759 [==============================] - 20s 49us/sample - loss: 255.8991 - val_loss: 170.8525 - lr: 4.0045e-04\n",
      "Epoch 988/1000\n",
      "414759/414759 [==============================] - 21s 51us/sample - loss: 255.8727 - val_loss: 170.8297 - lr: 4.0045e-04\n",
      "Epoch 989/1000\n",
      "414759/414759 [==============================] - 15s 36us/sample - loss: 255.8427 - val_loss: 170.8184 - lr: 4.0045e-04\n",
      "Epoch 990/1000\n",
      "414759/414759 [==============================] - 16s 37us/sample - loss: 255.7999 - val_loss: 170.8548 - lr: 4.0045e-04\n",
      "Epoch 991/1000\n",
      "414759/414759 [==============================] - 16s 38us/sample - loss: 255.7925 - val_loss: 170.8029 - lr: 4.0045e-04\n",
      "Epoch 992/1000\n",
      "414759/414759 [==============================] - 21s 50us/sample - loss: 255.7744 - val_loss: 170.8133 - lr: 4.0045e-04\n",
      "Epoch 993/1000\n",
      "414759/414759 [==============================] - 15s 36us/sample - loss: 255.7497 - val_loss: 170.8077 - lr: 4.0045e-04\n",
      "Epoch 994/1000\n",
      "414759/414759 [==============================] - 14s 35us/sample - loss: 255.6994 - val_loss: 170.7754 - lr: 4.0045e-04\n",
      "Epoch 995/1000\n",
      "414759/414759 [==============================] - 21s 51us/sample - loss: 255.7213 - val_loss: 170.7539 - lr: 4.0045e-04\n",
      "Epoch 996/1000\n",
      "414759/414759 [==============================] - 15s 36us/sample - loss: 255.6614 - val_loss: 170.8056 - lr: 4.0045e-04\n",
      "Epoch 997/1000\n",
      "414759/414759 [==============================] - 19s 46us/sample - loss: 255.6855 - val_loss: 170.7633 - lr: 4.0045e-04\n",
      "Epoch 998/1000\n",
      "414759/414759 [==============================] - 15s 36us/sample - loss: 255.6383 - val_loss: 170.7902 - lr: 4.0045e-04\n",
      "Epoch 999/1000\n",
      "414759/414759 [==============================] - 22s 53us/sample - loss: 255.6657 - val_loss: 170.7559 - lr: 4.0045e-04\n",
      "Epoch 1000/1000\n",
      "414759/414759 [==============================] - 14s 35us/sample - loss: 255.6366 - val_loss: 170.7829 - lr: 4.0045e-04\n",
      "\n",
      "training finished in 1000 epochs (reach max pre-specified epoches), transform data to adjust the platform effect...\n",
      "\n",
      "\n",
      "re-run DE on CVAE transformed scRNA-seq data!\n",
      "Differential analysis across cell-types on scRNA-seq data...\n",
      "finally selected 1828 cell-type marker genes\n",
      "\n",
      "\n",
      "platform effect adjustment by CVAE finished. Elapsed time: 336.47 minutes.\n",
      "\n",
      "\n",
      "use the marker genes derived from CVAE transformed scRNA-seq for downstream regression!\n",
      "\n",
      "gene filtering before modeling...\n",
      "40 genes with nUMIs<5 in all spatial spots and need to be excluded\n",
      "finally use 1788 genes for modeling\n",
      "\n",
      "spot filtering before modeling...\n",
      "all spots passed filtering\n",
      "\n",
      "\n",
      "######### Start GLRM modeling... #########\n",
      "\n",
      "GLRM settings:\n",
      "use SciPy minimize method:  L-BFGS-B\n",
      "global optimization turned off, local minimum will be used in GLRM\n",
      "use hybrid version of GLRM\n",
      "Numba detected total 64 available CPU cores. Use 64 CPU cores\n",
      "use 2001 points to calculate the heavy-tail density\n",
      "use weight threshold for Adaptive Lasso:  0.001\n",
      "total 425 unique nUMIs, min: 0.0, max: 1007.0\n",
      "\n",
      "Build graph: \n",
      " Graph with 3532 nodes and 10189 edges\n",
      "\n",
      "estimation of gene-specific platform effect gamma_g is skipped as already using CVAE to adjust platform effect\n",
      "\n",
      "\n",
      "Start GLRM fitting...\n",
      "\n",
      "first estimate MLE theta and corresponding e^alpha and sigma^2...\n",
      "\n",
      "GLRM model initialization...\n",
      "calculate MLE theta and sigma^2 iteratively...\n",
      "  iter | time_opt | time_sig | sigma2\n",
      "     0 | 13185.191 | 2841.395 |  0.496\n",
      "     1 | 10359.626 | 2141.069 |  0.316\n",
      "     2 | 9257.900 | 2533.157 |  0.253\n",
      "     3 | 7973.026 | 2119.780 |  0.231\n",
      "     4 | 6633.855 | 2160.195 |  0.223\n",
      "     5 | 5377.168 | 2168.924 |  0.220\n",
      "     6 | 4209.072 | 1795.447 |  0.219\n",
      "MLE theta and sigma^2 calculation finished. Elapsed time: 1212.60 minutes.\n",
      "MLE theta estimation finished. Elapsed time: 1212.60 minutes.\n",
      "\n",
      "calculate weights of Adaptive Lasso...\n",
      "\n",
      "Stage 1: variable selection using Adaptive Lasso starts with the MLE theta and e^alpha, using already estimated sigma^2 and gamma_g...\n",
      "specified hyper-parameter for Adaptive Lasso is: 0.72\n",
      "\n",
      "start ADMM iteration...\n",
      "  iter |  res_pri_n | res_dual_n |    eps_pri |   eps_dual |        rho |    new_rho | time_opt | time_reg | time_lap | tilde_RMSE |   hat_RMSE\n",
      "     0 |     20.524 |     20.527 |      0.449 |      0.449 |       1.00 |       1.00 | 3092.705 |    0.005 |    0.013 |   0.093520 |   0.046760\n",
      "     1 |     20.511 |      0.045 |      0.449 |      0.470 |       1.00 |       2.00 | 2913.504 |    0.004 |    0.013 |   0.093469 |   0.046734\n",
      "     2 |     19.580 |      5.945 |      0.448 |      0.509 |       2.00 |       2.00 | 3746.434 |    0.006 |    0.013 |   0.090121 |   0.045061\n",
      "     3 |     18.209 |      9.377 |      0.447 |      0.544 |       2.00 |       4.00 | 3189.692 |    0.004 |    0.012 |   0.083152 |   0.041576\n",
      "     4 |     16.631 |     17.223 |      0.446 |      0.607 |       4.00 |       4.00 | 4320.666 |    0.004 |    0.012 |   0.075792 |   0.037896\n",
      "     5 |     14.924 |     20.963 |      0.450 |      0.661 |       4.00 |       4.00 | 4115.356 |    0.004 |    0.011 |   0.067655 |   0.033827\n",
      "     6 |     13.736 |     19.214 |      0.448 |      0.707 |       4.00 |       8.00 | 4131.957 |    0.011 |    0.016 |   0.062133 |   0.031066\n",
      "     7 |     12.402 |     27.604 |      0.456 |      0.792 |       8.00 |       8.00 | 4825.934 |    0.006 |    0.015 |   0.055940 |   0.027970\n",
      "     8 |     10.969 |     33.058 |      0.462 |      0.866 |       8.00 |       8.00 | 4583.706 |    0.006 |    0.013 |   0.049230 |   0.024615\n",
      "     9 |      9.970 |     30.096 |      0.459 |      0.928 |       8.00 |      16.00 | 4476.027 |    0.005 |    0.012 |   0.044705 |   0.022353\n",
      "    10 |      8.855 |     42.390 |      0.471 |      1.041 |      16.00 |      16.00 | 5229.442 |    0.009 |    0.017 |   0.039676 |   0.019838\n",
      "    11 |      7.692 |     50.111 |      0.479 |      1.138 |      16.00 |      16.00 | 5052.216 |    0.004 |    0.013 |   0.034357 |   0.017178\n",
      "    12 |      6.931 |     44.547 |      0.473 |      1.217 |      16.00 |      32.00 | 4966.142 |    0.005 |    0.012 |   0.030955 |   0.015477\n",
      "    13 |      6.086 |     60.381 |      0.489 |      1.363 |      32.00 |      32.00 | 5562.227 |    0.004 |    0.012 |   0.027176 |   0.013588\n",
      "    14 |      5.227 |     70.679 |      0.499 |      1.487 |      32.00 |      32.00 | 5281.843 |    0.010 |    0.016 |   0.023255 |   0.011627\n",
      "    15 |      4.677 |     62.135 |      0.491 |      1.588 |      32.00 |      64.00 | 5165.023 |    0.005 |    0.012 |   0.020798 |   0.010399\n",
      "    16 |      4.064 |     82.203 |      0.511 |      1.774 |      64.00 |      64.00 | 5532.584 |    0.006 |    0.013 |   0.018080 |   0.009040\n",
      "    17 |      3.466 |     94.698 |      0.523 |      1.931 |      64.00 |      64.00 | 5340.810 |    0.005 |    0.012 |   0.015378 |   0.007689\n",
      "    18 |      3.074 |     83.223 |      0.512 |      2.058 |      64.00 |     128.00 | 5155.602 |    0.005 |    0.012 |   0.013619 |   0.006809\n",
      "    19 |      2.653 |    107.445 |      0.536 |      2.292 |     128.00 |     128.00 | 5370.390 |    0.006 |    0.017 |   0.011726 |   0.005863\n",
      "    20 |      2.257 |    122.064 |      0.551 |      2.491 |     128.00 |     128.00 | 5166.620 |    0.005 |    0.011 |   0.009932 |   0.004966\n",
      "    21 |      1.991 |    107.081 |      0.536 |      2.653 |     128.00 |     256.00 | 4969.446 |    0.005 |    0.011 |   0.008754 |   0.004377\n",
      "    22 |      1.695 |    139.618 |      0.568 |      2.946 |     256.00 |     256.00 | 5159.351 |    0.004 |    0.011 |   0.007429 |   0.003714\n",
      "    23 |      1.423 |    157.548 |      0.586 |      3.190 |     256.00 |     256.00 | 4915.173 |    0.007 |    0.013 |   0.006193 |   0.003096\n",
      "    24 |      1.243 |    138.488 |      0.567 |      3.386 |     256.00 |     512.00 | 4727.916 |    0.005 |    0.012 |   0.005399 |   0.002699\n",
      "    25 |      1.048 |    174.964 |      0.604 |      3.736 |     512.00 |     512.00 | 4828.151 |    0.004 |    0.012 |   0.004527 |   0.002264\n",
      "    26 |      0.876 |    193.454 |      0.622 |      4.028 |     512.00 |     512.00 | 4600.045 |    0.005 |    0.011 |   0.003745 |   0.001873\n",
      "    27 |      0.766 |    168.328 |      0.597 |      4.263 |     512.00 |    1024.00 | 4471.914 |    0.007 |    0.013 |   0.003255 |   0.001628\n",
      "    28 |      0.643 |    209.984 |      0.639 |      4.689 |    1024.00 |    1024.00 | 4421.692 |    0.004 |    0.011 |   0.002713 |   0.001356\n",
      "    29 |      0.534 |    236.790 |      0.665 |      5.044 |    1024.00 |    1024.00 | 4270.578 |    0.005 |    0.011 |   0.002223 |   0.001112\n",
      "    30 |      0.459 |    211.904 |      0.640 |      5.325 |    1024.00 |    2048.00 | 4114.076 |    0.007 |    0.013 |   0.001897 |   0.000949\n",
      "    31 |      0.379 |    267.343 |      0.696 |      5.816 |    2048.00 |    2048.00 | 4145.745 |    0.006 |    0.012 |   0.001550 |   0.000775\n",
      "    32 |      0.313 |    291.929 |      0.720 |      6.215 |    2048.00 |    2048.00 | 3931.264 |    0.005 |    0.011 |   0.001253 |   0.000626\n",
      "    33 |      0.268 |    258.784 |      0.687 |      6.529 |    2048.00 |    4096.00 | 3829.700 |    0.005 |    0.011 |   0.001060 |   0.000530\n",
      "    34 |      0.219 |    317.787 |      0.746 |      7.072 |    4096.00 |    4096.00 | 3830.746 |    0.004 |    0.011 |   0.000849 |   0.000425\n",
      "    35 |      0.179 |    339.015 |      0.768 |      7.514 |    4096.00 |    4096.00 | 3630.312 |    0.005 |    0.011 |   0.000674 |   0.000337\n",
      "    36 |      0.152 |    302.802 |      0.731 |      7.858 |    4096.00 |    8192.00 | 3451.483 |    0.005 |    0.011 |   0.000564 |   0.000282\n",
      "    37 |      0.124 |    366.859 |      0.795 |      8.452 |    8192.00 |    8192.00 | 3502.357 |    0.007 |    0.014 |   0.000447 |   0.000224\n",
      "    38 |      0.102 |    381.399 |      0.810 |      8.937 |    8192.00 |    8192.00 | 3302.241 |    0.005 |    0.011 |   0.000352 |   0.000176\n",
      "    39 |      0.087 |    335.377 |      0.764 |      9.320 |    8192.00 |   16384.00 | 3132.357 |    0.005 |    0.011 |   0.000294 |   0.000147\n",
      "    40 |      0.071 |    406.707 |      0.835 |      9.992 |   16384.00 |   16384.00 | 3053.390 |    0.010 |    0.013 |   0.000233 |   0.000117\n",
      "    41 |      0.058 |    437.475 |      0.866 |     10.543 |   16384.00 |   16384.00 | 2855.289 |    0.005 |    0.013 |   0.000182 |   0.000091\n",
      "    42 |      0.049 |    385.087 |      0.814 |     10.979 |   16384.00 |   32768.00 | 2695.070 |    0.014 |    0.012 |   0.000149 |   0.000075\n",
      "    43 |      0.039 |    471.155 |      0.900 |     11.715 |   32768.00 |   32768.00 | 2636.467 |    0.006 |    0.012 |   0.000115 |   0.000058\n",
      "    44 |      0.032 |    497.302 |      0.926 |     12.295 |   32768.00 |   32768.00 | 2508.298 |    0.008 |    0.011 |   0.000087 |   0.000043\n",
      "    45 |      0.027 |    440.811 |      0.869 |     12.739 |   32768.00 |   65536.00 | 2355.793 |    0.006 |    0.011 |   0.000071 |   0.000035\n",
      "    46 |      0.021 |    518.736 |      0.947 |     13.488 |   65536.00 |          / | 2210.976 |    0.008 |    0.011 |   0.000054 |   0.000027\n",
      "early stop!\n",
      "Terminated (optimal) in 47 iterations.\n",
      "One optimization by ADMM finished. Elapsed time: 3246.21 minutes.\n",
      "\n",
      "Stage 1 variable selection finished. Elapsed time: 3246.21 minutes.\n",
      "\n",
      "Stage 2: final theta estimation with Graph Laplacian Constrain using already estimated sigma^2 and gamma_g\n",
      "specified hyper-parameter for Graph Laplacian Constrain is: 13.895\n",
      "\n",
      "start ADMM iteration...\n",
      "  iter |  res_pri_n | res_dual_n |    eps_pri |   eps_dual |        rho |    new_rho | time_opt | time_reg | time_lap | tilde_RMSE |   hat_RMSE\n",
      "     0 |     25.517 |     21.530 |      0.454 |      0.454 |       1.00 |       1.00 | 8667.823 |    0.009 |    0.030 |   0.064289 |   0.069695\n",
      "     1 |     22.267 |     13.947 |      0.451 |      0.472 |       1.00 |       1.00 | 2608.652 |    0.007 |    0.030 |   0.082363 |   0.065451\n",
      "     2 |     20.036 |     16.535 |      0.449 |      0.491 |       1.00 |       1.00 | 2870.567 |    0.013 |    0.032 |   0.068015 |   0.060539\n",
      "     3 |     18.506 |     18.994 |      0.448 |      0.509 |       1.00 |       2.00 | 2681.456 |    0.007 |    0.033 |   0.058510 |   0.056814\n",
      "     4 |     18.183 |     31.452 |      0.460 |      0.545 |       2.00 |       2.00 | 3562.765 |    0.007 |    0.032 |   0.055471 |   0.056040\n",
      "     5 |     17.093 |     34.809 |      0.463 |      0.578 |       2.00 |       2.00 | 3411.765 |    0.010 |    0.032 |   0.055841 |   0.052463\n",
      "     6 |     15.866 |     38.957 |      0.468 |      0.609 |       2.00 |       4.00 | 3386.111 |    0.006 |    0.032 |   0.049243 |   0.049120\n",
      "     7 |     15.626 |     61.894 |      0.490 |      0.671 |       4.00 |       4.00 | 3997.284 |    0.007 |    0.031 |   0.047468 |   0.047964\n",
      "     8 |     14.443 |     66.374 |      0.495 |      0.727 |       4.00 |       4.00 | 3866.138 |    0.013 |    0.030 |   0.047652 |   0.044133\n",
      "     9 |     13.034 |     73.143 |      0.502 |      0.778 |       4.00 |       8.00 | 3801.876 |    0.007 |    0.031 |   0.040491 |   0.040347\n",
      "    10 |     12.449 |    112.775 |      0.541 |      0.876 |       8.00 |       8.00 | 4281.676 |    0.007 |    0.030 |   0.038348 |   0.038129\n",
      "    11 |     11.043 |    119.992 |      0.549 |      0.961 |       8.00 |       8.00 | 4185.179 |    0.006 |    0.031 |   0.036798 |   0.033621\n",
      "    12 |      9.473 |    130.385 |      0.559 |      1.036 |       8.00 |      16.00 | 4033.079 |    0.006 |    0.030 |   0.029289 |   0.029302\n",
      "    13 |      8.609 |    192.404 |      0.621 |      1.169 |      16.00 |      16.00 | 4532.460 |    0.007 |    0.031 |   0.026952 |   0.026236\n",
      "    14 |      7.109 |    202.769 |      0.631 |      1.277 |      16.00 |      16.00 | 4280.829 |    0.009 |    0.032 |   0.024048 |   0.021484\n",
      "    15 |      5.563 |    216.020 |      0.645 |      1.364 |      16.00 |      32.00 | 4074.473 |    0.008 |    0.031 |   0.016907 |   0.017161\n",
      "    16 |      4.721 |    297.990 |      0.727 |      1.507 |      32.00 |      32.00 | 4580.493 |    0.006 |    0.028 |   0.014809 |   0.014227\n",
      "    17 |      3.478 |    309.260 |      0.738 |      1.609 |      32.00 |      32.00 | 4158.218 |    0.006 |    0.027 |   0.012127 |   0.010327\n",
      "    18 |      2.289 |    321.602 |      0.750 |      1.678 |      32.00 |      64.00 | 3802.601 |    0.006 |    0.028 |   0.006596 |   0.007001\n",
      "    19 |      1.823 |    406.428 |      0.835 |      1.782 |      64.00 |      64.00 | 4138.414 |    0.007 |    0.024 |   0.005250 |   0.005369\n",
      "    20 |      1.167 |    414.298 |      0.843 |      1.843 |      64.00 |      64.00 | 3718.702 |    0.007 |    0.025 |   0.004382 |   0.003295\n",
      "    21 |      0.556 |    421.966 |      0.851 |      1.872 |      64.00 |     128.00 | 2568.345 |    0.007 |    0.025 |   0.001427 |   0.001608\n",
      "    22 |      0.495 |    490.187 |      0.919 |      1.914 |     128.00 |     128.00 | 2987.843 |    0.006 |    0.021 |   0.000776 |   0.001330\n",
      "    23 |      0.307 |    493.490 |      0.922 |      1.934 |     128.00 |     128.00 | 2974.679 |    0.006 |    0.022 |   0.001300 |   0.000758\n",
      "    24 |      0.108 |    497.064 |      0.926 |      1.938 |     128.00 |     256.00 | 1958.339 |    0.007 |    0.022 |   0.000389 |   0.000255\n",
      "    25 |      0.146 |    543.846 |      0.972 |      1.949 |     256.00 |     256.00 | 1894.182 |    0.013 |    0.019 |   0.000128 |   0.000322\n",
      "    26 |      0.086 |    544.739 |      0.973 |      1.957 |     256.00 |     256.00 | 2409.979 |    0.006 |    0.022 |   0.000378 |   0.000199\n",
      "    27 |      0.024 |    545.962 |      0.975 |      1.959 |     256.00 |     512.00 | 1481.370 |    0.011 |    0.019 |   0.000100 |   0.000062\n",
      "    28 |      0.042 |    574.359 |      1.003 |      1.967 |     512.00 |     512.00 | 1430.306 |    0.005 |    0.021 |   0.000021 |   0.000088\n",
      "    29 |      0.024 |    574.529 |      1.003 |      1.972 |     512.00 |     512.00 | 1915.503 |    0.006 |    0.021 |   0.000108 |   0.000055\n",
      "    30 |      0.008 |    574.863 |      1.003 |      1.974 |     512.00 |          / | 1396.070 |    0.005 |    0.018 |   0.000033 |   0.000018\n",
      "early stop!\n",
      "Terminated (optimal) in 31 iterations.\n",
      "One optimization by ADMM finished. Elapsed time: 1761.00 minutes.\n",
      "\n",
      "\n",
      "stage 2 finished. Elapsed time: 1761.00 minutes.\n",
      "\n",
      "GLRM fitting finished. Elapsed time: 6219.81 minutes.\n",
      "\n",
      "\n",
      "Post-processing estimated cell-type proportion theta...\n",
      "hard thresholding small theta values with threshold 0\n",
      "\n",
      "\n",
      "cell type deconvolution finished. Estimate results saved in /home/exouser/Spatial/celltype_proportions.csv. Elapsed time: 109.27 hours.\n",
      "\n",
      "\n",
      "######### No imputation #########\n",
      "\n",
      "\n",
      "whole pipeline finished. Total elapsed time: 109.27 hours.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args='runDeconvolution -q IPF_spatial_spot_nUMI.csv                           -r IPF_ref_scRNA_cell_nUMI.csv                           -c IPF_ref_scRNA_cell_celltype.csv                           -a IPF_spatial_spot_adjacency_matrix.csv                           -m IPF_selected_2534_celltype_markers.csv                           --n_hv_gene 2000                           --n_marker_per_cmp 20                           --pseudo_spot_max_cell 10                           --lambda_r 0.72                           --lambda_g 13.895                           --seed 1                           -n 64\\n', returncode=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "cmd = '''runDeconvolution -q IPF_spatial_spot_nUMI.csv \\\n",
    "                          -r IPF_ref_scRNA_cell_nUMI.csv \\\n",
    "                          -c IPF_ref_scRNA_cell_celltype.csv \\\n",
    "                          -a IPF_spatial_spot_adjacency_matrix.csv \\\n",
    "                          -m IPF_selected_2534_celltype_markers.csv \\\n",
    "                          --n_hv_gene 2000 \\\n",
    "                          --n_marker_per_cmp 20 \\\n",
    "                          --pseudo_spot_max_cell 10 \\\n",
    "                          --lambda_r 0.72 \\\n",
    "                          --lambda_g 13.895 \\\n",
    "                          --seed 1 \\\n",
    "                          -n 64\n",
    "'''\n",
    "subprocess.run(cmd, check=True, text=True, shell=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
